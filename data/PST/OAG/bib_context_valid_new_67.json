{
    "5f4e198191e011084df59c0b": [
        {
            "pid": "5dbebb7447c8f766462c220c",
            "content": "size) is usually a pre-specified parameter. Current works <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> did not efficiently handle the change of discriminative attr y a graph auto-encoder, but this method neglects linkage between paper and author and coauthorship. <ref type=\"bibr\" target=\"#b6\">[7]</ref> addresses the pairwise classification problem by extracting"
        },
        {
            "pid": "5b67b45517c44aac1c86078b",
            "content": "ef>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> did not efficiently ised <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based one of author mentions belonging to the same author and are essential in the name disambiguation task. <ref type=\"bibr\" target=\"#b5\">[6]</ref> first learns representation for every name mention in a pair ative attribute to separate papers into small blocks and we use the same trainset and testset as in <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>In Semantic Scholar, the selected meta-paths of our m block as sequence s \u2208 S; 3 Construct meta-path based view {G p1</figDesc><table /><note>\u2022 Aminer-AND<ref type=\"bibr\" target=\"#b5\">[6]</ref>: This dataset contains 70,285 records of 12,798 unique autho"
        },
        {
            "pid": null,
            "content": "to effective representation ability. While most GNN works <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> focus on transductive setting, there have been some recent"
        },
        {
            "pid": "5a260c3517c44a4ba8a2529f",
            "content": "biguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the number of authors (i.e., cluster size) is usually a pre ef type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works exploit graph topological features in the ce, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type=\"bibr\" target=\"#b4\">[5]</ref> leverages only relational data in the form of anonymized gra"
        },
        {
            "pid": null,
            "content": "Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type=\"bibr\" target=\"#b3\">[4]</ref>, i.e., papers of an author are regarded as belonging to diff"
        },
        {
            "pid": "5c04967517c44a2c74708cec",
            "content": "s by aggregating their features. Both DeepGL and GraphSage are designed for homogeneous graphs. LAN <ref type=\"bibr\" target=\"#b14\">[15]</ref> aggregates neighbors with both rule-based and network-base"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t ef type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type= by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.\u2022 GHOST<ref type=\"bibr\" target=\"#b1\">[2]</ref>: GHOST use affinity propagation algorithm for clustering on"
        },
        {
            "pid": "5c04967517c44a2c74708cec",
            "content": "s by aggregating their features. Both DeepGL and GraphSage are designed for homogeneous graphs. LAN <ref type=\"bibr\" target=\"#b14\">[15]</ref> aggregates neighbors with both rule-based and network-base"
        },
        {
            "pid": null,
            "content": "evel attention and Pseudo-Siamese structure, we also test three variants of MA-PairRNN.</p><p>\u2022 MLP <ref type=\"bibr\" target=\"#b24\">[25]</ref>: It's s multilayer perceptron that directly projecting inp"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t ef type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type= by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.\u2022 GHOST<ref type=\"bibr\" target=\"#b1\">[2]</ref>: GHOST use affinity propagation algorithm for clustering on"
        },
        {
            "pid": "53e9b253b7602d9703cf4028",
            "content": "perceptron that directly projecting input features into a low dimensional vector.</p><p>\u2022 Deepwalk <ref type=\"bibr\" target=\"#b25\">[26]</ref>: Deepwalk captures contextual information of neighborhood"
        },
        {
            "pid": null,
            "content": "Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type=\"bibr\" target=\"#b3\">[4]</ref>, i.e., papers of an author are regarded as belonging to diff"
        },
        {
            "pid": "5a73cbcc17c44a0b3035f2ff",
            "content": ""
        },
        {
            "pid": "5f043eac9e795e1b1125eccf",
            "content": "owledge graphs.</p><p>Heterogeneous information networks <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref> have been studied in recent years. Meta-path is designed t"
        },
        {
            "pid": "5c04967517c44a2c74708cec",
            "content": "s by aggregating their features. Both DeepGL and GraphSage are designed for homogeneous graphs. LAN <ref type=\"bibr\" target=\"#b14\">[15]</ref> aggregates neighbors with both rule-based and network-base"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "a set of base graph features by relational functions that can generalize across networks. GraphSage <ref type=\"bibr\" target=\"#b13\">[14]</ref> samples a fixed number of neighbors and generate node embe s potential to add new nodes dynamically.</p><p>For each meta-path based view, similar to GraphSage <ref type=\"bibr\" target=\"#b13\">[14]</ref>, node representations are generated by aggregating feature neighborhood via uniform random walks for node embedding in homogeneous network.</p><p>\u2022 GraphSage <ref type=\"bibr\" target=\"#b13\">[14]</ref>: GraphSage samples node neighborhoods to generate node emb"
        },
        {
            "pid": "53e9b457b7602d9703f54cf4",
            "content": "> \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>I. INTRODUCTION</head><p>Namesake problem <ref type=\"bibr\" target=\"#b0\">[1]</ref> poses a huge challenge on many applications, e.g., informati 0\"><head>A. Name Disambiguation</head><p>Name disambiguation methods can be divided into supervised <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref"
        },
        {
            "pid": "53e9aa56b7602d97033c5d27",
            "content": "ef>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>"
        },
        {
            "pid": null,
            "content": "blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t ef type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type= by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.\u2022 GHOST<ref type=\"bibr\" target=\"#b1\">[2]</ref>: GHOST use affinity propagation algorithm for clustering on"
        }
    ],
    "5f44e5bd91e011872f85ed90": [
        {
            "pid": "5aed14d617c44a4438158f74",
            "content": "were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD <ref type=\"bibr\" target=\"#b3\">[4]</ref> focuses on the lip region, we found that lip landmarks can b"
        },
        {
            "pid": "573696026e3b12023e515eec",
            "content": ", we feed color images. Secondly, our model is significantly deeper, with residual skip connections <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Thirdly, inspired by this public implementation 2 , we us"
        },
        {
            "pid": null,
            "content": "syncing talking face videos to match a given input audio stream has received considerable attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "599c7b46601a182cd2721db4",
            "content": "=\"bibr\" target=\"#b9\">[10]</ref> (56 words), TIMIT <ref type=\"bibr\" target=\"#b13\">[14]</ref> and LRW <ref type=\"bibr\" target=\"#b7\">[8]</ref> (1000 words) which significantly hampers a model from learni there is a need for a metric that is designed specifically for measuring lip-sync errors.</p><p>LRW <ref type=\"bibr\" target=\"#b7\">[8]</ref> LRS2  <ref type=\"table\">1</ref>: We propose two new metrics test sets, one each using the test set videos of LRS2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, LRW <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and LRS3 <ref type=\"bibr\" target=\"#b2\">[3]</ref> respective"
        },
        {
            "pid": "599c7b46601a182cd2721db4",
            "content": "=\"bibr\" target=\"#b9\">[10]</ref> (56 words), TIMIT <ref type=\"bibr\" target=\"#b13\">[14]</ref> and LRW <ref type=\"bibr\" target=\"#b7\">[8]</ref> (1000 words) which significantly hampers a model from learni there is a need for a metric that is designed specifically for measuring lip-sync errors.</p><p>LRW <ref type=\"bibr\" target=\"#b7\">[8]</ref> LRS2  <ref type=\"table\">1</ref>: We propose two new metrics test sets, one each using the test set videos of LRS2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, LRW <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and LRS3 <ref type=\"bibr\" target=\"#b2\">[3]</ref> respective"
        },
        {
            "pid": null,
            "content": "syncing talking face videos to match a given input audio stream has received considerable attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "53e9ba23b7602d970462d60d",
            "content": "LRS2 train split (\u2248 29 hours) with a batch size of 64, with T v = 5 frames using the Adam optimizer <ref type=\"bibr\" target=\"#b11\">[12]</ref> with an initial learning rate of 1e \u22123 . Our expert lip-sy ain set <ref type=\"bibr\" target=\"#b0\">[1]</ref>, with a batch size of 80. We use the Adam optimizer <ref type=\"bibr\" target=\"#b11\">[12]</ref> with an initial learning rate of 1e \u22124 and betas \u03b2 1 = 0.5"
        },
        {
            "pid": "55465f0c0cf2939c2feed7f2",
            "content": "with a limited set of words such as GRID <ref type=\"bibr\" target=\"#b9\">[10]</ref> (56 words), TIMIT <ref type=\"bibr\" target=\"#b13\">[14]</ref> and LRW <ref type=\"bibr\" target=\"#b7\">[8]</ref> (1000 word"
        },
        {
            "pid": "5d04e8ffda56295d08dd1a2f",
            "content": "given input audio stream has received considerable attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar speech representations to lip landmarks using several hours of a single speaker. More recent works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> in this line direc large amount of data of a particular speaker, typically a few hours. A recent work along this line <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposes to seamlessly edit videos of individual speakers"
        },
        {
            "pid": null,
            "content": "volutional blocks. Each block consists of a convolutional layer followed by a Leaky ReLU activation <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The discriminator is trained to maximize the objective fu"
        },
        {
            "pid": "599e92739c05cae4992b4ee0",
            "content": "\">23]</ref> in the research community.</p><p>Initial works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> using deep learning in this space learned a mapping from sp of talking face videos was achieved by a few recent works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> on videos of Barack Obama. They learn a mapping between the"
        }
    ],
    "5f92ba1691e011edb3573ba0": [
        {
            "pid": "599c7987601a182cd2648373",
            "content": "0\"><head n=\"1\">INTRODUCTION</head><p>Self-attention-based architectures, in particular Transformers <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref>, have become the model of choice in natu ww.tei-c.org/ns/1.0\"><head n=\"3\">METHOD</head><p>In model design we follow the original Transformer <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref> as closely as possible. An advantage of sulting sequence of embedding vectors serves as input to the encoder.</p><p>The Transformer encoder <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref> consists of alternating layers of multih xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">RELATED WORK</head><p>Transformers were proposed by <ref type=\"bibr\" target=\"#b43\">Vaswani et al. (2017)</ref> for machine translation, and have since b i-c.org/ns/1.0\"><head>APPENDIX A MULTIHEAD SELF-ATTENTION</head><p>Standard qkv self-attention (SA, <ref type=\"bibr\" target=\"#b43\">Vaswani et al. (2017)</ref>) is a popular building block for neural a"
        },
        {
            "pid": null,
            "content": "2020)</ref>, CIFAR-10/100 <ref type=\"bibr\" target=\"#b23\">(Krizhevsky, 2009)</ref>, Oxford-IIIT Pets <ref type=\"bibr\" target=\"#b31\">(Parkhi et al., 2012)</ref>, and Oxford Flowers-102 <ref type=\"bibr\""
        },
        {
            "pid": null,
            "content": "CNN-like architectures with self-attention <ref type=\"bibr\" target=\"#b47\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Carion et al., 2020)</ref>, some replacing the convolutions entirely < ing self-attention, e.g. for object detection <ref type=\"bibr\" target=\"#b18\">(Hu et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Carion et al., 2020)</ref>, video processing <ref type=\"bibr\" target=\" other computer vision tasks, such as detection and segmentation. Our results, coupled with those in <ref type=\"bibr\" target=\"#b6\">Carion et al. (2020)</ref>, indicate the promise of this approach. Ano"
        },
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "training. Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training <ref type=\"bibr\" target=\"#b8\">(Chen et al., 2020b;</ref><ref type=\"bibr\" target=\"#b16\">He et al., 20"
        },
        {
            "pid": null,
            "content": "works (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification <ref type=\"bibr\" target=\"#b3\">(Bello et al., 2019)</ref> or by further processing the output of a CN"
        },
        {
            "pid": null,
            "content": "works (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification <ref type=\"bibr\" target=\"#b3\">(Bello et al., 2019)</ref> or by further processing the output of a CN"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "onvolutional architectures remain dominant <ref type=\"bibr\" target=\"#b25\">(LeCun et al., 1989;</ref><ref type=\"bibr\" target=\"#b24\">Krizhevsky et al., 2012;</ref><ref type=\"bibr\" target=\"#b15\">He et al"
        },
        {
            "pid": "5ede0553e06a4c1b26a841d6",
            "content": "ation of Transformers with global self-attention to full-sized images. Closest to our model is iGPT <ref type=\"bibr\" target=\"#b7\">(Chen et al., 2020a)</ref>, which applies Transformers to image pixels"
        },
        {
            "pid": "5f645c489e795e0286c904b1",
            "content": "br\" target=\"#b29\">(Mahajan et al., 2018;</ref><ref type=\"bibr\" target=\"#b51\">Xie et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Kolesnikov et al., 2020)</ref>.</p><p>Inspired by the Transformer sca e at higher resolution than pre-training <ref type=\"bibr\" target=\"#b40\">(Touvron et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Kolesnikov et al., 2020)</ref>. When feeding images of higher resolut bibr\" target=\"#b38\">Sun et al. (2017)</ref> study how CNN performance scales with dataset size, and <ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b14\">Djolon s. We de-duplicate the pre-training datasets w.r.t. the test sets of the downstream tasks following <ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref>. We transfer the models trained on the r\" target=\"#b30\">(Nilsback &amp; Zisserman, 2008)</ref>. For these datasets, pre-processing follows <ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref>.</p><p>We also evaluate on the 19-task ther scaling of ViT would likely lead to improved performance. for ResNets we also run the setup of <ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref> and select the best results across thi re robust than simply re-initializing the very last layer.</p><p>For VTAB we follow the protocol in <ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref>, and use the same hyperparameter setti ith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used in <ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref>, except that we do not use task-specif and 152x2 -pre-trained on JFT with SGD and Adam. For SGD, we use the hyperparameters recommended by<ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref>. Results are presented</note></figure> r used to pre-train ResNets on JFT. Note that the absolute numbers are lower than those reported by <ref type=\"bibr\" target=\"#b22\">Kolesnikov et al. (2020)</ref>, since we pre-train only for 7 epochs, type=\"bibr\" target=\"#b37\">(Salimans &amp; Kingma, 2016)</ref>. These modifications improve transfer <ref type=\"bibr\" target=\"#b22\">(Kolesnikov et al., 2020)</ref>, and we denote the modified model \"Re /16 -to state-of-the-art CNNs from the literature. The first comparison point is Big Transfer (BiT) <ref type=\"bibr\" target=\"#b22\">(Kolesnikov et al., 2020)</ref>, which performs supervised transfer l run at 384 resolution (running fine-tuning at different resolution than training is common practice <ref type=\"bibr\" target=\"#b22\">(Kolesnikov et al., 2020)</ref>).</p><p>When transferring ViT models"
        },
        {
            "pid": "5dcd263a3a55ac58039516c5",
            "content": "ave exploration of contrastive pre-training <ref type=\"bibr\" target=\"#b8\">(Chen et al., 2020b;</ref><ref type=\"bibr\" target=\"#b16\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2"
        }
    ],
    "5fdb279d91e0118a02c4f4ef": [
        {
            "pid": "62376b385aee126c0f09faaf",
            "content": "ing new samples, etc. Two of the most popular methods are Generative Adversarial Networks (GANs) by <ref type=\"bibr\" target=\"#b2\">Goodfellow et al. (2014)</ref> and Variational 2.z combines with x for"
        },
        {
            "pid": null,
            "content": ">, Random Forests by <ref type=\"bibr\" target=\"#b1\">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=\"bibr\" target=\"#b5\">Ho et al. (2015)</ref>, and Deep Learning by <ref type=\"bibr\" target=\""
        },
        {
            "pid": "5fae6688d4150a363cdec517",
            "content": ""
        },
        {
            "pid": null,
            "content": "iques include Linear Regression by <ref type=\"bibr\" target=\"#b6\">Karlic et al. (2010)</ref>, SVM by <ref type=\"bibr\" target=\"#b0\">Cheng et al. (2011)</ref>, Random Forests by <ref type=\"bibr\" target=\" model the same problem as a regression task that includes SVR (Support Vector Regression) model by <ref type=\"bibr\" target=\"#b0\">Cheng et al. (2011)</ref> and DeepDIFF (Attention Based). These models"
        },
        {
            "pid": "5a260c0c17c44a4ba8a1e0e6",
            "content": "tably DeepChrome by <ref type=\"bibr\" target=\"#b12\">Singh et al. (2016)</ref> and AttentiveChrome by <ref type=\"bibr\" target=\"#b13\">Singh et al. (2017)</ref> that employ deep learning to learn complex , Rule-Based Learning by <ref type=\"bibr\" target=\"#b5\">Ho et al. (2015)</ref>, and Deep Learning by <ref type=\"bibr\" target=\"#b13\">Singh et al. (2017</ref><ref type=\"bibr\" target=\"#b12\">Singh et al. ( n Reduction</head><p>Following the work from <ref type=\"bibr\" target=\"#b12\">Singh et al. (2016</ref><ref type=\"bibr\" target=\"#b13\">Singh et al. ( , 2017))</ref>, we use five core Histone Modification"
        },
        {
            "pid": "5e09a858df1a9c0c41688641",
            "content": "g and thus may converge to a better model. Beside, multiple embedding modules were also proposed by <ref type=\"bibr\" target=\"#b4\">Guo et al. (2020b)</ref> to learn DNA representations which was shown"
        },
        {
            "pid": null,
            "content": "al. (2010)</ref>, SVM by <ref type=\"bibr\" target=\"#b0\">Cheng et al. (2011)</ref>, Random Forests by <ref type=\"bibr\" target=\"#b1\">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "iques include Linear Regression by <ref type=\"bibr\" target=\"#b6\">Karlic et al. (2010)</ref>, SVM by <ref type=\"bibr\" target=\"#b0\">Cheng et al. (2011)</ref>, Random Forests by <ref type=\"bibr\" target=\" model the same problem as a regression task that includes SVR (Support Vector Regression) model by <ref type=\"bibr\" target=\"#b0\">Cheng et al. (2011)</ref> and DeepDIFF (Attention Based). These models"
        },
        {
            "pid": null,
            "content": "al. (2010)</ref>, SVM by <ref type=\"bibr\" target=\"#b0\">Cheng et al. (2011)</ref>, Random Forests by <ref type=\"bibr\" target=\"#b1\">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=\"bibr\" targ"
        },
        {
            "pid": "5a260c0c17c44a4ba8a1e0e6",
            "content": "tably DeepChrome by <ref type=\"bibr\" target=\"#b12\">Singh et al. (2016)</ref> and AttentiveChrome by <ref type=\"bibr\" target=\"#b13\">Singh et al. (2017)</ref> that employ deep learning to learn complex , Rule-Based Learning by <ref type=\"bibr\" target=\"#b5\">Ho et al. (2015)</ref>, and Deep Learning by <ref type=\"bibr\" target=\"#b13\">Singh et al. (2017</ref><ref type=\"bibr\" target=\"#b12\">Singh et al. ( n Reduction</head><p>Following the work from <ref type=\"bibr\" target=\"#b12\">Singh et al. (2016</ref><ref type=\"bibr\" target=\"#b13\">Singh et al. ( , 2017))</ref>, we use five core Histone Modification"
        },
        {
            "pid": "5fae6688d4150a363cdec517",
            "content": ""
        }
    ],
    "5f7d8a8391e011346ad27d2b": [
        {
            "pid": "5a260c8417c44a4ba8a31511",
            "content": "Mix, a data augmentation method for generating sub-sequences along with their labels based on mixup <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Under the active sequence labeling framew ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Sequence Mixup in the Embedding Space</head><p>Mixup <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref> is a data augmentation method that impleme lation-based Regularizations Mixup implements interpolation in the input space to regularize models <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type=\"b r limit rather than a too narrow score range setting.</p><p>For the mixing coefficient \u03bb, we follow <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref> to sample it from Beta(\u03b1, \u03b1) and explore \u03b1"
        },
        {
            "pid": null,
            "content": "equences that pass the sequence quality screening. For screening, we utilize a language model GPT-2 <ref type=\"bibr\" target=\"#b28\">(Radford et al., 2019)</ref> to score sequence x by computing its per"
        },
        {
            "pid": null,
            "content": "\"bibr\" target=\"#b35\">Silfverberg et al. (2017)</ref> employ heuristic rules based on specific task, <ref type=\"bibr\" target=\"#b16\">Hu et al. (2017)</ref> propose to augment text data in an encoder-dec"
        },
        {
            "pid": "53e9b14cb7602d9703bd1674",
            "content": "rmalized Token Entropy (NTE) Another uncertainty measure for the query policy is normalized entropy <ref type=\"bibr\" target=\"#b32\">(Settles and Craven, 2008)</ref>, defined as:</p><formula xml:id=\"for"
        },
        {
            "pid": "5bdc315017c44a1f58a05cc6",
            "content": "rce sequence labeling, several approaches have been applied including using semi-supervised methods <ref type=\"bibr\" target=\"#b6\">(Clark et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2"
        },
        {
            "pid": "5d9ed47b47c8f76646fb2892",
            "content": "recognition (NER) <ref type=\"bibr\" target=\"#b22\">(Lample et al., 2016)</ref>, and event extraction <ref type=\"bibr\" target=\"#b44\">(Yang et al., 2019)</ref>. Recently, neural sequential models <ref ty"
        },
        {
            "pid": null,
            "content": "nsertion, swap, and deletion <ref type=\"bibr\" target=\"#b41\">(Wei and Zou, 2019)</ref>, paraphrasing <ref type=\"bibr\" target=\"#b5\">(Cho et al., 2019)</ref> or back translation <ref type=\"bibr\" target=\""
        },
        {
            "pid": null,
            "content": "also perform statistical significance tests for the above results. We use Wilcoxon Signed Rank Test <ref type=\"bibr\" target=\"#b42\">(Wilcoxon, 1992)</ref>, a non-parametric alternative to the paired t-"
        },
        {
            "pid": "5b1642d68fbcbf6e5a9b7f1a",
            "content": "so infeasible to apply heuristic data augmentation methods such as context-based words substitution <ref type=\"bibr\" target=\"#b20\">(Kobayashi, 2018)</ref>, synonym replacement, random insertion, swap, ncoder-decoder manner. Very recently, <ref type=\"bibr\" target=\"#b1\">(Anaby-Tavor et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Kobayashi, 2018)</ref> harness the power of pre-trained language mode"
        },
        {
            "pid": "5a73cb7117c44a0b3035994d",
            "content": "proaches including synonym replancement, random insertion, swap and deletion for text augmentation, <ref type=\"bibr\" target=\"#b18\">Kafle et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b35\">Silfverberg"
        },
        {
            "pid": "5a9cb66717c44a376ffb8ac1",
            "content": "r\" target=\"#b17\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b22\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Akbik et al.,"
        }
    ],
    "5f7ee07491e011a5faf0feb2": [
        {
            "pid": "53e9ba9bb7602d97046c645c",
            "content": "ods include AdaBoost <ref type=\"bibr\" target=\"#b10\">(Freund and Schapire, 1997)</ref> and RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref>, which target at classification and rankin ng model that makes more accurate predictions should receive a higher weight. Inspired by RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref>, we reduce the ranking combination problem = {q1 = (The Tale of Genji, country, ?t) q2 = (The Tale of Genji, genre, ?t)} Similar to RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref> Ranking loss. The overall objective of KEn <p>Let the set of all the critical entity pairs from all the validation queries of an entity as P . <ref type=\"bibr\" target=\"#b9\">Freund et al. (2004)</ref> have proved that, when using RankBoost, thi"
        },
        {
            "pid": "58d82fced649053542fd6ed1",
            "content": "erent KGs. Recent works on multilingual KG embeddings provide support for automated entity matching <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017</ref><ref type=\"bibr\" target=\"#b4\">(Chen et al., , extended embedding models to bridge multiple KGs, typically for KGs of multiple languages. MTransE <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref> jointly learns a transformation across two s The embedding learning process jointly trains the knowledge model and the alignment model following <ref type=\"bibr\" target=\"#b5\">Chen et al. (2017)</ref>, while self-learning is added to improve the G i and G j . \u03bb is a positive hyperparameter that weights the two model components.</p><p>Following <ref type=\"bibr\" target=\"#b5\">Chen et al. (2017)</ref>, instead of directly optimizing J in Eq. ( <r"
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b4\">(Chen et al., , 2018b;;</ref><ref type=\"bibr\" target=\"#b26\">Sun et al., 2018</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2020a))</ref>. However, the performance of the state-of r, the performance of the state-of-the-art (SOTA) entity matching methods is still far from perfect <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020a)</ref>, which may cause erroneous knowledge transf pe=\"bibr\" target=\"#b18\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Sun et al., 2019a</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2020a) )</ref> and degree centrality measures <ref type"
        },
        {
            "pid": "5cede105da562983788e48c5",
            "content": "robust against the sparsity of data <ref type=\"bibr\" target=\"#b11\">(Hao et al., 2019)</ref>. RotatE <ref type=\"bibr\" target=\"#b30\">(Sun et al., 2019b)</ref> employs a complex embedding space and model ple scoring techniques: TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and RotatE <ref type=\"bibr\" target=\"#b30\">(Sun et al., 2019b)</ref>. TransE models relations as translations be es the single-embedding TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and RotatE <ref type=\"bibr\" target=\"#b30\">(Sun et al., 2019b)</ref>, we also include DistMult <ref type=\"bibr\""
        },
        {
            "pid": "58437789ac44360f10843622",
            "content": "2015)</ref>, Wikidata <ref type=\"bibr\" target=\"#b33\">(Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014)</ref> and YAGO <ref type=\"bibr\" target=\"#b22\">(Rebele et al., 2016)</ref>. In contrast, KGs often consist of numero"
        },
        {
            "pid": "5bdc315017c44a1f58a05d46",
            "content": "t al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Yang et al., 2019)</ref>, neighborhood information <ref type=\"bibr\" target=\"#b36\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al.,"
        },
        {
            "pid": "5db929df47c8f766461fbb83",
            "content": "bibr\" target=\"#b36\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al., 2015;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Sun et al., 2019"
        },
        {
            "pid": "5d3ed25a275ded87f97dea4f",
            "content": "odels achieve satisfactory performance on KG completion and are robust against the sparsity of data <ref type=\"bibr\" target=\"#b11\">(Hao et al., 2019)</ref>. RotatE <ref type=\"bibr\" target=\"#b30\">(Sun"
        },
        {
            "pid": "573698636e3b12023e72a2d5",
            "content": "stMult <ref type=\"bibr\" target=\"#b38\">(Yang et al., 2015)</ref>, as well as neural models like HolE <ref type=\"bibr\" target=\"#b19\">(Nickel et al., 2016)</ref> and ConvE <ref type=\"bibr\" target=\"#b8\">( >(Yang et al., 2015)</ref>, TransD <ref type=\"bibr\" target=\"#b13\">(Ji et al., 2015)</ref>, and HolE <ref type=\"bibr\" target=\"#b19\">(Nickel et al., 2016)</ref>. After extensive hyperparameter tuning, t"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "58437789ac44360f10843622",
            "content": "2015)</ref>, Wikidata <ref type=\"bibr\" target=\"#b33\">(Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014)</ref> and YAGO <ref type=\"bibr\" target=\"#b22\">(Rebele et al., 2016)</ref>. In contrast, KGs often consist of numero"
        },
        {
            "pid": "555048d645ce0a409eb71e05",
            "content": "tive models including translational models <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b35\">Wang et al., 2014)</ref> and bilinear models <ref type=\"bibr\" target= e premise that the candidate space has excluded the triples that have been seen in the training set <ref type=\"bibr\" target=\"#b35\">(Wang et al., 2014)</ref>. Competitive methods. We compare six varian"
        },
        {
            "pid": "5bdc315017c44a1f58a05d46",
            "content": "t al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Yang et al., 2019)</ref>, neighborhood information <ref type=\"bibr\" target=\"#b36\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al.,"
        },
        {
            "pid": null,
            "content": "ains RotatE on 5 KGs and uses the representative non-embedding symbolic entity alignment tool PARIS <ref type=\"bibr\" target=\"#b24\">(Suchanek et al., 2011)</ref> for entity matching. PARIS delivered en"
        },
        {
            "pid": null,
            "content": "\"#b16\">(Koncel-Kedziorski et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2018a;</ref><ref type=\"bibr\" target=\"#b1\">Bordes et al., 2014)</ref>. Recently, extensive efforts have been inve"
        },
        {
            "pid": "5de0ceaadf1a9c0c415b0c12",
            "content": "e=\"bibr\" target=\"#b38\">Yang et al., 2015;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Sun et al., 2019a</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2"
        },
        {
            "pid": null,
            "content": "from latent representations of observed facts. Representative models including translational models <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b35\">Wang et al., weighting (KEnS m ): MRR is a widely-used metric for evaluating the ranking performance of a model <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al., , after applying a relation-specific translation vector r. The representative models include TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and its extensions TransD <ref type=\"bibr\" true triple (h, r, t).</p><p>We here consider two representative triple scoring techniques: TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and RotatE <ref type=\"bibr\" target=\"#b30\"> higher. Although another common metric, Mean Reciprocal Rank (MRR), has been used in previous works <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref>, it is not applicable to the evaluation of ce techniques introduced in in Section 3. For baseline methods, besides the single-embedding TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and RotatE <ref type=\"bibr\" target=\"#b30\">"
        },
        {
            "pid": "59ae3be32bbe271c4c71b8ba",
            "content": "ell as neural models like HolE <ref type=\"bibr\" target=\"#b19\">(Nickel et al., 2016)</ref> and ConvE <ref type=\"bibr\" target=\"#b8\">(Dettmers et al., 2018)</ref>. Due to the large body of work in this l"
        },
        {
            "pid": "5f0bbded9e795e8e5a5296e0",
            "content": "er is to extend the ensemble transfer mechanism to population sparse domain knowledge in biological <ref type=\"bibr\" target=\"#b12\">(Hao et al., 2020)</ref> and medical knowledge bases <ref type=\"bibr\""
        },
        {
            "pid": "5f0bbded9e795e8e5a5296e0",
            "content": "er is to extend the ensemble transfer mechanism to population sparse domain knowledge in biological <ref type=\"bibr\" target=\"#b12\">(Hao et al., 2020)</ref> and medical knowledge bases <ref type=\"bibr\""
        },
        {
            "pid": null,
            "content": "br\">), attributes (Trsedya et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Sun et al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Yang et al., 2019)</ref>, neighborhood information <ref type=\"bibr\" t"
        },
        {
            "pid": null,
            "content": "ootstrapping approach to iteratively propose new alignment labels to enhance the performance. MuGNN <ref type=\"bibr\" target=\"#b2\">(Cao et al., 2019)</ref> encodes KGs via multi-channel Graph Neural Ne"
        }
    ],
    "5f0d85c69fced0a24be4f019": [
        {
            "pid": "58d83045d649053542fe853e",
            "content": "l-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib a prefetching (VLDP) <ref type=\"bibr\" target=\"#b44\">[45]</ref> and signature path prefetching (SPP) <ref type=\"bibr\" target=\"#b32\">[33]</ref> are well known delta prefetchers. VLDP stores the history ing proposals <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> have also been coded and evaluated with ChampSim, helping improve performance for server workloads like CloudSuite <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b52\">[53]</ref>, <ref type=\"bib"
        },
        {
            "pid": "53e9a42bb7602d9702d44c0b",
            "content": "he limits of data prefetching. Apart from these spatial prefetchers, there are temporal prefetchers <ref type=\"bibr\" target=\"#b53\">[54]</ref>, <ref type=\"bibr\" target=\"#b54\">[55]</ref>, <ref type=\"bib"
        },
        {
            "pid": "53e9a237b7602d9702b3c2d9",
            "content": "ses. Hardware prefetchers such as next-line (NL) and stride based on instruction pointer (IPstride) <ref type=\"bibr\" target=\"#b17\">[18]</ref> are some of the simple, efficient, and light-weight data p"
        },
        {
            "pid": "5b67b47917c44aac1c8635be",
            "content": "rformance drop of 3%. TSKID goes down by 6% with HAWKEYE <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, which is mostly attributed to the inaccurate prefetch req"
        },
        {
            "pid": "5d04eeba8607575390f83f47",
            "content": "\">[55]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref>, <ref type=\"bibr\" target=\"#b57\">[58]</ref> that target irr der of accesses. Usually, temporal prefetchers demand hundreds of KBs. Recently, Managed ISB (MISB) <ref type=\"bibr\" target=\"#b58\">[59]</ref> and Triage <ref type=\"bibr\" target=\"#b57\">[58]</ref> have \">[29]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref>. However, if a hardware vendor does not wish to communicat \">[33]</ref>, <ref type=\"bibr\" target=\"#b52\">[53]</ref>, <ref type=\"bibr\" target=\"#b57\">[58]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref> and additional prefetchers <ref type=\"bibr\" target=\"#b11\"> \">[24]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bibr\" target=\"#b57\">[58]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref> can be used on top of IPCP to improve the performance. As"
        },
        {
            "pid": "53e9a32eb7602d9702c392a4",
            "content": ""
        },
        {
            "pid": "53e9a806b7602d9703145b0f",
            "content": "\">[16]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>- <ref type=\"bib"
        },
        {
            "pid": "53e9a775b7602d97030b16b6",
            "content": "\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>- <ref type=\"bibr\" target=\"#b40\">[41]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref> that control the prefetch degree and prefetch distance bas"
        },
        {
            "pid": null,
            "content": "atial prefetchers demand less storage (closer to tens of KBs, except spatial memory streaming (SMS) <ref type=\"bibr\" target=\"#b46\">[47]</ref> and Bingo <ref type=\"bibr\" target=\"#b10\">[11]</ref>) as co et=\"#b12\">[13]</ref> are designed specifically for L2's access patterns.</p><p>Prefetchers like SMS <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref> and Bingo <ref ressiveness based on the success probability of future deltas.</p><p>Spatial Memory Streaming (SMS) <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref> is a spatial pr ge, SPP performs better than VLDP and DSPatch, at the L1. Similarly, Bingo performs better than SMS <ref type=\"bibr\" target=\"#b46\">[47]</ref> with relatively less storage demand. Note that Bingo deman"
        },
        {
            "pid": "573696026e3b12023e515eec",
            "content": "rks. We also use a set of Convolutional Neural Networks (CNNs) and a Recurrent Neural Network (RNN) <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib"
        },
        {
            "pid": "53e99b1bb7602d97023b2390",
            "content": "l prefetchers <ref type=\"bibr\" target=\"#b53\">[54]</ref>, <ref type=\"bibr\" target=\"#b54\">[55]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bib rs like temporal streaming <ref type=\"bibr\" target=\"#b54\">[55]</ref>, Irregular Stream Buffer (ISB) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and Domino <ref type=\"bibr\" target=\"#b11\">[12]</ref> trac \">[25]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref>. However, if a ibr\" target=\"#b58\">[59]</ref> and additional prefetchers <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bib"
        }
    ],
    "5f0bde8e9e795ea206ff8ef5": [
        {
            "pid": "599c7965601a182cd2638d24",
            "content": "simply regularize model predictions to be invariant to small noise applied to either input examples <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Sajjadi et r\" target=\"#b1\">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018;</ref><ref type=\"bibr\">2016)</ref> defines the n type=\"bibr\" target=\"#b34\">(Sajjadi et al., 2016;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016;</ref><ref type=\"bibr\" target=\"#b23\">Miyato et al., 2018)</ref>. But different from existing work, we focu has been shown to be beneficial <ref type=\"bibr\" target=\"#b15\">(Grandvalet &amp; Bengio, 2005;</ref><ref type=\"bibr\" target=\"#b23\">Miyato et al., 2018)</ref>, we sharpen predictions when computing the current parameters \u03b8 indicating that the gradient is not propagated through \u03b8, as suggested by VAT <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref>. We set \u03bb to 1 for most of our experiment cally, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref>, an algorithm that generates adversarial =\"#b41\">(Tarvainen &amp; Valpola, 2017)</ref> Conv-Large 3.1M 12.31 \u00b1 0.28 3.95 \u00b1 0.19 VAT + EntMin <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 \u00b1 0.05 3.86 \u00b1 0.11"
        },
        {
            "pid": "53e9aeb7b7602d97038dc893",
            "content": "bserve that advanced data augmentation methods, specifically those work best in supervised learning <ref type=\"bibr\" target=\"#b39\">(Simard et al., 1998;</ref><ref type=\"bibr\">Krizhevsky et al., 2012;<"
        },
        {
            "pid": "53e9a806b7602d970314bc39",
            "content": "Db, Yelp-2, Yelp-5, Amazon-2 and Amazon-5 sentiment classification and DBPedia topic classification <ref type=\"bibr\" target=\"#b19\">(Maas et al., 2011;</ref><ref type=\"bibr\">Zhang et al., 2015)</ref>.<"
        },
        {
            "pid": "599c7983601a182cd2646c4f",
            "content": "ss <ref type=\"bibr\" target=\"#b36\">(Salimans et al., 2016)</ref> works very well in practice. Later, <ref type=\"bibr\" target=\"#b11\">Dai et al. (2017)</ref> shows that this can be seen as an instantiati"
        },
        {
            "pid": "573696c56e3b12023e5c58be",
            "content": "ining set and the unlabeled set for IMDb and external data for Yelp-2, Yelp-5, Amazon-2 and Amazon-5<ref type=\"bibr\" target=\"#b20\">(McAuley et al., 2015)</ref> 5 . Note that for Yelp and Amazon based"
        },
        {
            "pid": null,
            "content": "ing. Later, the pre-training of word embeddings was simplified and substantially scaled in Word2Vec <ref type=\"bibr\" target=\"#b21\">(Mikolov et al., 2013)</ref> and Glove <ref type=\"bibr\" target=\"#b29\""
        },
        {
            "pid": "5550456245ce0a409eb55cee",
            "content": "stantially scaled in Word2Vec <ref type=\"bibr\" target=\"#b21\">(Mikolov et al., 2013)</ref> and Glove <ref type=\"bibr\" target=\"#b29\">(Pennington et al., 2014)</ref>  <ref type=\"formula\">2018</ref>) have"
        },
        {
            "pid": "53e9aeb7b7602d97038dc893",
            "content": "bserve that advanced data augmentation methods, specifically those work best in supervised learning <ref type=\"bibr\" target=\"#b39\">(Simard et al., 1998;</ref><ref type=\"bibr\">Krizhevsky et al., 2012;<"
        },
        {
            "pid": "5cede0e5da562983788c40d8",
            "content": "hown to work well for representation learning <ref type=\"bibr\" target=\"#b47\">(Hu et al., 2017;</ref><ref type=\"bibr\" target=\"#b48\">Ye et al., 2019)</ref>. Invariant representation learning <ref type=\""
        },
        {
            "pid": "53e9be09b7602d9704abe386",
            "content": ""
        },
        {
            "pid": "53e9a806b7602d970314bc39",
            "content": "Db, Yelp-2, Yelp-5, Amazon-2 and Amazon-5 sentiment classification and DBPedia topic classification <ref type=\"bibr\" target=\"#b19\">(Maas et al., 2011;</ref><ref type=\"bibr\">Zhang et al., 2015)</ref>.<"
        }
    ],
    "5fe1ccc591e0119a161edd6a": [
        {
            "pid": "573696116e3b12023e52463f",
            "content": ""
        },
        {
            "pid": "5b67b4b117c44aac1c86693f",
            "content": ""
        },
        {
            "pid": "5a4aef9e17c44a2190f7a8e2",
            "content": ". Hence MSA can be generalised to other choices of kernels and normalisation that are equally valid <ref type=\"bibr\" target=\"#b51\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Tsai et al.,"
        },
        {
            "pid": null,
            "content": "olecular Geometry</head><p>We apply the LieTransformer to the QM9 molecule property prediction task <ref type=\"bibr\" target=\"#b56\">(Wu et al., 2018)</ref>. This dataset consists of 133,885 small inorg"
        },
        {
            "pid": "5ce2cf99ced107d4c631b9ee",
            "content": "eTransformer-SE2) and depth.</p><p>Model architecture. The architecture used for the SetTransformer <ref type=\"bibr\" target=\"#b33\">(Lee et al., 2019)</ref> consists of 4 layers in the encoder, 4 layer"
        },
        {
            "pid": null,
            "content": "h perception tasks. This has led to the success of CNNs in multiple domains such as computer vision <ref type=\"bibr\" target=\"#b32\">(Krizhevsky et al., 2012)</ref> and audio <ref type=\"bibr\" target=\"#b"
        },
        {
            "pid": null,
            "content": "h mild assumptions) are necessarily convolutions <ref type=\"bibr\">(Kondor &amp; Trivedi, 2018;</ref><ref type=\"bibr\" target=\"#b8\">Cohen et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bekkers, 2020)< equivariant if and only if they are convolutions <ref type=\"bibr\">(Kondor &amp; Trivedi, 2018;</ref><ref type=\"bibr\" target=\"#b8\">Cohen et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bekkers, 2020)<"
        },
        {
            "pid": null,
            "content": "bled this work, including Pytorch <ref type=\"bibr\" target=\"#b41\">(Paszke et al., 2017)</ref>, NumPy <ref type=\"bibr\" target=\"#b36\">(Oliphant, 2006;</ref><ref type=\"bibr\" target=\"#b49\">Walt et al., 201"
        },
        {
            "pid": "5ac1829d17c44a1fda91824e",
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        }
    ],
    "5f88146591e0118ce8f040a7": [
        {
            "pid": "599c7983601a182cd2646713",
            "content": "es has improved both the rate of learning and final performance. Similar to our findings about MoCo,<ref type=\"bibr\" target=\"#b32\">Wu et al. (2017)</ref> find that mining the very hardest negatives hu"
        },
        {
            "pid": "58437725ac44360f1082fa21",
            "content": "e=\"bibr\" target=\"#b31\">(Weinberger &amp; Saul, 2009)</ref>. In this paradigm, selecting the hardest <ref type=\"bibr\" target=\"#b5\">(Bucher et al., 2016)</ref> or harder <ref type=\"bibr\" target=\"#b23\">("
        },
        {
            "pid": "5aed14e217c44a4438159ac5",
            "content": "s for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type=\"bibr\" target=\"#b15\">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=\"bibr\" target=\"#b22\">("
        },
        {
            "pid": null,
            "content": "paradigm, selecting the hardest <ref type=\"bibr\" target=\"#b5\">(Bucher et al., 2016)</ref> or harder <ref type=\"bibr\" target=\"#b23\">(Schroff et al., 2015)</ref>  </p></div> <div xmlns=\"http://www.tei-c"
        },
        {
            "pid": "5550478845ce0a409eb67b76",
            "content": ""
        },
        {
            "pid": null,
            "content": "ve curriculum is beyond the scope of this work, curricula have shown utility in many other contexts <ref type=\"bibr\" target=\"#b3\">(Bengio et al., 2009)</ref>.</p></div> <div xmlns=\"http://www.tei-c.or"
        },
        {
            "pid": "53e9a645b7602d9702f7362e",
            "content": ""
        },
        {
            "pid": null,
            "content": "ve curriculum is beyond the scope of this work, curricula have shown utility in many other contexts <ref type=\"bibr\" target=\"#b3\">(Bengio et al., 2009)</ref>.</p></div> <div xmlns=\"http://www.tei-c.or"
        },
        {
            "pid": "53e99924b7602d970215faff",
            "content": ""
        },
        {
            "pid": "5eede0b091e0116a23aafc15",
            "content": ""
        },
        {
            "pid": "5eede0b091e0116a23aafc15",
            "content": ""
        },
        {
            "pid": "5550478845ce0a409eb67b76",
            "content": ""
        },
        {
            "pid": "5da6ec7147c8f766460bb40e",
            "content": ""
        },
        {
            "pid": "5da6ec7147c8f766460bb40e",
            "content": ""
        },
        {
            "pid": "53e9a645b7602d9702f7362e",
            "content": ""
        },
        {
            "pid": "53e9b844b7602d970440513c",
            "content": "rget=\"#b11\">(Chen et al., 2020c)</ref> and the downstream task of linear classification on ImageNet <ref type=\"bibr\" target=\"#b13\">(Deng et al., 2009)</ref>. We make the following contributions (see F e query). To evaluate semantic similarity we used the ImageNet class hierarchy derived from WordNet <ref type=\"bibr\" target=\"#b13\">(Deng et al., 2009)</ref>. For each negative, we computed the tree de"
        },
        {
            "pid": null,
            "content": "b7\">(Caron et al., 2018)</ref>, SwAV <ref type=\"bibr\" target=\"#b8\">(Caron et al., 2020)</ref>, SeLa <ref type=\"bibr\" target=\"#b2\">(Asano et al., 2020)</ref>, PCL <ref type=\"bibr\" target=\"#b20\">(Li et"
        },
        {
            "pid": null,
            "content": "et al., 2013)</ref>. Work in object detection has also benefited from efforts to find hard examples <ref type=\"bibr\" target=\"#b25\">(Sung, 1996;</ref><ref type=\"bibr\">Can\u00e9vet &amp; Fleuret, 2015;</ref>"
        },
        {
            "pid": "5eede0b091e0116a23aafc15",
            "content": ""
        },
        {
            "pid": "53e9a16ab7602d9702a5dfae",
            "content": "n active learning, for example, it is common to favor examples on which the model is most uncertain <ref type=\"bibr\" target=\"#b14\">(Fu et al., 2013)</ref>. Work in object detection has also benefited"
        },
        {
            "pid": "53e9a645b7602d9702f7362e",
            "content": ""
        },
        {
            "pid": null,
            "content": "ve curriculum is beyond the scope of this work, curricula have shown utility in many other contexts <ref type=\"bibr\" target=\"#b3\">(Bengio et al., 2009)</ref>.</p></div> <div xmlns=\"http://www.tei-c.or"
        }
    ],
    "5f86cae991e011dbc7eba2fa": [
        {
            "pid": "57a4e91aac44365e35c97b34",
            "content": "o gather to the requesting core, only the requested elements from the data array.</p><p>Song et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> propose a graph processor based on sparse matrix algebra,"
        },
        {
            "pid": "5c0495e417c44a2c74704e5d",
            "content": "even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For PIUMA, the application code does not need to change f"
        },
        {
            "pid": null,
            "content": "a graph. Determining the relationships between entities in a graph is the basis of graph analytics <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Graph analytics poses important challenges on existing proc"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf317b4",
            "content": "progress. Nevertheless, GPUs usually perform better on graph algorithms than CPUs for small graphs <ref type=\"bibr\" target=\"#b4\">[5]</ref>, because they have more threads, which hides memory latency,"
        },
        {
            "pid": null,
            "content": "e overhead of moving the thread is compensated by the amount of locally consumed data. Young et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> report that migrating a thread involves moving 200 bytes, w ead migration. Therefore, optimizing data locality is crucial for obtaining good performance on Emu <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which is often hard to obtain for graph analysis applicati"
        },
        {
            "pid": "53e9bbe5b7602d9704839b20",
            "content": "#fig_2\">3</ref>. PIUMA multi-threaded cores (MTC) are round-robin multi-threaded in-order pipelines <ref type=\"bibr\" target=\"#b5\">[6]</ref>. At any moment, each thread can only have one in-flight inst"
        },
        {
            "pid": null,
            "content": "e overhead of moving the thread is compensated by the amount of locally consumed data. Young et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> report that migrating a thread involves moving 200 bytes, w ead migration. Therefore, optimizing data locality is crucial for obtaining good performance on Emu <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which is often hard to obtain for graph analysis applicati"
        },
        {
            "pid": null,
            "content": "are important contributors to PUMA's performance and energy efficiency.</p><p>The Emu architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> is a recently proposed architecture for big data analysis, i"
        },
        {
            "pid": "5c0495e417c44a2c74704e5d",
            "content": "even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For PIUMA, the application code does not need to change f"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf317b4",
            "content": "progress. Nevertheless, GPUs usually perform better on graph algorithms than CPUs for small graphs <ref type=\"bibr\" target=\"#b4\">[5]</ref>, because they have more threads, which hides memory latency,"
        },
        {
            "pid": "58d83045d649053542fe8543",
            "content": "A, such as the absence of caches, and fine-grained communication and memory accesses. Graphicionado <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a graph analysis accelerator, implementing a vertex-cen"
        }
    ],
    "5f896fa591e01149071e45df": [
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "similar from every other image in the dataset <ref type=\"bibr\" target=\"#b50\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020)</ref>. We propose to train feature representations \" target=\"#b26\">Misra &amp; Maaten, 2020;</ref><ref type=\"bibr\" target=\"#b15\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020)</ref> which aims to learn representations by consid main. We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR <ref type=\"bibr\" target=\"#b1\">(Chen et al., 2020)</ref>. The SimCLR loss encourages two augmentation seline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR <ref type=\"bibr\" target=\"#b1\">(Chen et al., 2020)</ref>, and then uses the resulting representation"
        },
        {
            "pid": "53e9b844b7602d970440513c",
            "content": "teacher quality, we experiment with a larger network and transfer from the full ILSVRC 2012 dataset <ref type=\"bibr\" target=\"#b4\">(Deng et al., 2009)</ref> to BSCD-FSL.</p><p>In particular, we used th"
        },
        {
            "pid": null,
            "content": "et al., 2019a)</ref> has shown that existing stateof-the-art few-shot learners fail to generalize. <ref type=\"bibr\" target=\"#b40\">Tseng et al. (2020)</ref> attempt to address this problem by simulati </ref>, which includes most stateof-the-art approaches as well as a cross-domain few-shot technique <ref type=\"bibr\" target=\"#b40\">Tseng et al. (2020)</ref>. The top performing among these is naive Tr 6\">(Finn et al., 2017)</ref>, MetaOpt: <ref type=\"bibr\" target=\"#b21\">(Lee et al., 2019)</ref> FWT: <ref type=\"bibr\" target=\"#b40\">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type=\"bibr\" ta 6\">(Finn et al., 2017)</ref>, MetaOpt: <ref type=\"bibr\" target=\"#b21\">(Lee et al., 2019)</ref> FWT: <ref type=\"bibr\" target=\"#b40\">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": ""
        },
        {
            "pid": "5aed14e217c44a4438159ac5",
            "content": ""
        },
        {
            "pid": "5a9cb66717c44a376ffb88a8",
            "content": ""
        },
        {
            "pid": "5de8d54c3a55ac9c4229187d",
            "content": ""
        },
        {
            "pid": "5aed14d117c44a4438158aa4",
            "content": ". Semi-supervised few-shot learning (SS-FSL) <ref type=\"bibr\" target=\"#b32\">(Ren et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Yu et al., 2020;"
        },
        {
            "pid": "5b1643ba8fbcbf6e5a9bc797",
            "content": ""
        },
        {
            "pid": null,
            "content": "r\" target=\"#b35\">Snell et al., 2017;</ref><ref type=\"bibr\" target=\"#b43\">Vinyals et al., 2016;</ref><ref type=\"bibr\" target=\"#b38\">Sung et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Hou et al., 20"
        }
    ],
    "5f842b5891e01129be18ffbd": [
        {
            "pid": "5f7af09591e011983cc81efc",
            "content": "vie reviews (MR) <ref type=\"bibr\" target=\"#b36\">(Pang &amp; Lee, 2005)</ref>.</p><p>Comparison with <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref>: <ref type=\"bibr\" target=\"#b23\">Kalant , 2005)</ref>.</p><p>Comparison with <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref>: <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref> also consider ways to sample negatives"
        },
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "bibr\" target=\"#b34\">(Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b47\">Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a)</ref>. In computer vision, unsupervised contrastiv s to nearby locations, and negative pairs farther apart; other objectives have also been considered <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref>. The success of the associated methods depe the hard sampling method on vision tasks using the STL10, CIFAR100 and CIFAR10 data. We use SimCLR <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref> as the baseline method, and all models are ted by, The vision experiments in the main body of the paper are all based off the SimCLR framework <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref>. They use a relatively small batch size (up \"bibr\" target=\"#b54\">Xu et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b48\">Tian et al., 2 practice, an empirical approximation of it <ref type=\"bibr\" target=\"#b47\">(Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref t s that preserve semantic content, e.g., jittering, random cropping, separating color channels, etc. <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5bbacb2c17c44aecc4eab82b",
            "content": "=\"#b34\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b37\">Purushwalkam &amp; Gupta, 2020;</ref><ref type=\"bibr\" target=\"#b40\">Sermanet et al., 2018)</ref>.</p><p>Surprisingly, the choice of negat"
        },
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "bibr\" target=\"#b34\">(Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b47\">Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a)</ref>. In computer vision, unsupervised contrastiv s to nearby locations, and negative pairs farther apart; other objectives have also been considered <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref>. The success of the associated methods depe the hard sampling method on vision tasks using the STL10, CIFAR100 and CIFAR10 data. We use SimCLR <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref> as the baseline method, and all models are ted by, The vision experiments in the main body of the paper are all based off the SimCLR framework <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref>. They use a relatively small batch size (up \"bibr\" target=\"#b54\">Xu et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b48\">Tian et al., 2 practice, an empirical approximation of it <ref type=\"bibr\" target=\"#b47\">(Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref t s that preserve semantic content, e.g., jittering, random cropping, separating color channels, etc. <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref"
        },
        {
            "pid": "573696086e3b12023e51c1cc",
            "content": "hod to correct its mistakes more quickly <ref type=\"bibr\" target=\"#b38\">(Schroff et al., 2015;</ref><ref type=\"bibr\" target=\"#b41\">Song et al., 2016)</ref>. For representation learning, informative ne"
        },
        {
            "pid": "573696086e3b12023e51c1cc",
            "content": "hod to correct its mistakes more quickly <ref type=\"bibr\" target=\"#b38\">(Schroff et al., 2015;</ref><ref type=\"bibr\" target=\"#b41\">Song et al., 2016)</ref>. For representation learning, informative ne"
        },
        {
            "pid": null,
            "content": "0\"><head n=\"1\">INTRODUCTION</head><p>Owing to their empirical success, contrastive learning methods <ref type=\"bibr\" target=\"#b7\">(Chopra et al., 2005;</ref><ref type=\"bibr\" target=\"#b15\">Hadsell et a"
        },
        {
            "pid": null,
            "content": "0\"><head n=\"1\">INTRODUCTION</head><p>Owing to their empirical success, contrastive learning methods <ref type=\"bibr\" target=\"#b7\">(Chopra et al., 2005;</ref><ref type=\"bibr\" target=\"#b15\">Hadsell et a"
        },
        {
            "pid": "5dcd263a3a55ac58039516c5",
            "content": "ect detection and segmentation tasks <ref type=\"bibr\" target=\"#b30\">(Misra &amp; Maaten, 2020;</ref><ref type=\"bibr\" target=\"#b19\">He et al., 2020)</ref>.</p><p>Contrastive learning relies on two key </ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref type=\"bibr\" target=\"#b19\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Chen et al., 2020 very large, we also run experiments using MoCo-v2 with standard negative memory bank size N = 65536 <ref type=\"bibr\" target=\"#b19\">(He et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Chen et al., 202"
        },
        {
            "pid": null,
            "content": "target=\"#b2\">(Blum &amp; Mitchell, 1998;</ref><ref type=\"bibr\" target=\"#b54\">Xu et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al.,"
        }
    ],
    "5feb068b91e011f5d3420813": [
        {
            "pid": "5d89e9483a55acd95282ff4c",
            "content": "been proposed to this end, which compare networks' internal layers in addition to final predictions <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Sun et al. 202 e teacher layers into m buckets with approximately the same sizes and pick only one layer from each <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\">Sun et al. 2019)</ref>. Ther PKD is not the only model that utilizes internal layers' information. Other models such as TinyBERT <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019)</ref> and MobileBERT <ref type=\"bibr\" target=\"#b21"
        },
        {
            "pid": "5e8da0c991e011f2de5839a7",
            "content": "rnal layers in addition to final predictions <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Sun et al. 2020</ref><ref type=\"bibr\">Sun et al. , 2019))</ref>, but ther models such as TinyBERT <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019)</ref> and MobileBERT <ref type=\"bibr\" target=\"#b21\">(Sun et al. 2020</ref>) also found it crucial for training competitiv"
        },
        {
            "pid": "5db9295f47c8f766461f51cd",
            "content": ""
        },
        {
            "pid": "53e9b8b4b7602d9704494109",
            "content": ""
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "et al. ( <ref type=\"formula\">2019</ref>) squeezed multiple translation engines into one transformer <ref type=\"bibr\" target=\"#b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille"
        },
        {
            "pid": null,
            "content": "y> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Introduction</head><p>Knowledge distillation (KD) <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar w.tei-c.org/ns/1.0\"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "nguage understanding (NLU) models such as <ref type=\"bibr\">ELMO (Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> for this model. Therefore, our in-house ver -tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, so we skip that part and refer the reader _0\">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or"
        },
        {
            "pid": null,
            "content": "layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins"
        },
        {
            "pid": null,
            "content": "the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type=\"bibr\">Sun et al. (2019)</ref> and Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>. For our experiments, the batch size is set to 32 and the"
        },
        {
            "pid": null,
            "content": "the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type=\"bibr\">Sun et al. (2019)</ref> and Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>. For our experiments, the batch size is set to 32 and the"
        },
        {
            "pid": null,
            "content": "layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins"
        },
        {
            "pid": "56d81308dabfae2eee5eff4b",
            "content": ""
        },
        {
            "pid": "5d245ba4da56295a28fc1342",
            "content": "b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t"
        },
        {
            "pid": "5d245ba4da56295a28fc1342",
            "content": "b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t"
        },
        {
            "pid": "5d245ba4da56295a28fc1342",
            "content": "b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "et al. ( <ref type=\"formula\">2019</ref>) squeezed multiple translation engines into one transformer <ref type=\"bibr\" target=\"#b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "et al. ( <ref type=\"formula\">2019</ref>) squeezed multiple translation engines into one transformer <ref type=\"bibr\" target=\"#b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille"
        },
        {
            "pid": null,
            "content": "layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins"
        },
        {
            "pid": null,
            "content": "y> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Introduction</head><p>Knowledge distillation (KD) <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar w.tei-c.org/ns/1.0\"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": "onsider it as a complementary and generic add-on to enrich the training process of any neural model <ref type=\"bibr\" target=\"#b11\">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is"
        },
        {
            "pid": null,
            "content": "the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type=\"bibr\">Sun et al. (2019)</ref> and Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>. For our experiments, the batch size is set to 32 and the"
        },
        {
            "pid": null,
            "content": "layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins"
        },
        {
            "pid": "5d245ba4da56295a28fc1342",
            "content": "b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t"
        },
        {
            "pid": null,
            "content": "layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins"
        },
        {
            "pid": "5d245ba4da56295a28fc1342",
            "content": "b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "nguage understanding (NLU) models such as <ref type=\"bibr\">ELMO (Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> for this model. Therefore, our in-house ver -tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, so we skip that part and refer the reader _0\">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "nguage understanding (NLU) models such as <ref type=\"bibr\">ELMO (Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> for this model. Therefore, our in-house ver -tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, so we skip that part and refer the reader _0\">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or"
        },
        {
            "pid": "573697556e3b12023e63e99f",
            "content": ""
        },
        {
            "pid": "5d245ba4da56295a28fc1342",
            "content": "b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t"
        },
        {
            "pid": "5d245ba4da56295a28fc1342",
            "content": "b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t"
        },
        {
            "pid": null,
            "content": "the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type=\"bibr\">Sun et al. (2019)</ref> and Wu et al. <ref type=\"bibr\" target=\"#b1\">(2020)</ref>. For our experiments, the batch size is set to 32 and the"
        }
    ],
    "5f7d9bfd91e011346ad27f0c": [
        {
            "pid": "5da056bb47c8f76646056c7b",
            "content": ""
        },
        {
            "pid": null,
            "content": "at times replacing the state of the art in tasks such as forecasting, regression and classification <ref type=\"bibr\" target=\"#b6\">(De Brouwer et al., 2019;</ref><ref type=\"bibr\">Tan et al., 2020a;</re"
        },
        {
            "pid": null,
            "content": "arning for time series is far from established: in fact, non-deep learning methods such as TS-CHIEF <ref type=\"bibr\" target=\"#b28\">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type=\"bibr\" target=\"#b19\" Regression and classification of time series: Currently, non-deep learning methods such as TS-CHIEF <ref type=\"bibr\" target=\"#b28\">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type=\"bibr\" target=\"#b19\""
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": ""
        },
        {
            "pid": null,
            "content": "on from a limited number of time steps stored inside their hidden state (vanishing gradient problem <ref type=\"bibr\" target=\"#b13\">(Hochreiter, 1998;</ref><ref type=\"bibr\" target=\"#b25\">Pascanu et al."
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "ass of deep learning models, which were first proposed for the task of natural language translation <ref type=\"bibr\" target=\"#b31\">(Vaswani et al., 2017)</ref> but have since come to monopolize the st the core of our method lies a transformer encoder, as described in the original transformer work by <ref type=\"bibr\" target=\"#b31\">Vaswani et al. (2017)</ref>; however, we do not use the decoder part pos .</formula><p>Instead of deterministic, sinusoidal encodings, which were originally proposed by <ref type=\"bibr\" target=\"#b31\">Vaswani et al. (2017)</ref>, we use fully learnable positional encodi block, leading to significant performance gains over batch normalization, as originally proposed by <ref type=\"bibr\" target=\"#b31\">Vaswani et al. (2017)</ref>. However, here we instead use batch norma"
        },
        {
            "pid": null,
            "content": "arning for time series is far from established: in fact, non-deep learning methods such as TS-CHIEF <ref type=\"bibr\" target=\"#b28\">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type=\"bibr\" target=\"#b19\" Regression and classification of time series: Currently, non-deep learning methods such as TS-CHIEF <ref type=\"bibr\" target=\"#b28\">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type=\"bibr\" target=\"#b19\""
        },
        {
            "pid": "599c7a3c601a182cd269f1ae",
            "content": "e series regression and classification dataset benchmarks <ref type=\"bibr\">(Tan et al., 2020a;</ref><ref type=\"bibr\" target=\"#b0\">Bagnall et al., 2017)</ref>, matching or even outperforming sophistica classification based on evaluations on public benchmarks <ref type=\"bibr\">(Tan et al., 2020a;</ref><ref type=\"bibr\" target=\"#b0\">Bagnall et al., 2017)</ref>, followed by CNN-based deep architectures nsupervised learning for univariate and multivariate classification datasets of the UEA/UCR archive <ref type=\"bibr\" target=\"#b0\">(Bagnall et al., 2017)</ref>.</p><p>Transformer models for time series"
        },
        {
            "pid": null,
            "content": "arning for time series is far from established: in fact, non-deep learning methods such as TS-CHIEF <ref type=\"bibr\" target=\"#b28\">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type=\"bibr\" target=\"#b19\" Regression and classification of time series: Currently, non-deep learning methods such as TS-CHIEF <ref type=\"bibr\" target=\"#b28\">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type=\"bibr\" target=\"#b19\""
        },
        {
            "pid": "58d82fc8d649053542fd592c",
            "content": "of time series under increasing \"missingness\" of values. For the purpose of time series clustering, <ref type=\"bibr\" target=\"#b17\">Lei et al. (2017)</ref> also follow a method which aims at preserving"
        },
        {
            "pid": "53e9aa10b7602d970337f51e",
            "content": "eir hidden state (vanishing gradient problem <ref type=\"bibr\" target=\"#b13\">(Hochreiter, 1998;</ref><ref type=\"bibr\" target=\"#b25\">Pascanu et al., 2013)</ref>), and thus the context used for represent"
        }
    ],
    "5f03f3b611dc83056223206d": [
        {
            "pid": "5db9293347c8f766461f059b",
            "content": "cal applications pertaining to fairness, privacy, and safety <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. For example, we can train a GNN model to predict the effec on interpreting GNNs at the model-level. The existing study <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> only provides example-level explanations for graph models. tudies focusing on the interpretability of deep graph models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type have no baseline to compare with. Note that existing studies <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> only focus on interpreting GNNs at example-level while igno [4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type=\"bibr\" target=\"#b39\">[40]</ref> proposes to explain deep graph models at the example-level"
        },
        {
            "pid": "5a9cb66717c44a376ffb868e",
            "content": "\"bibr\" target=\"#b37\">[38]</ref>, ORGAN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Junction Tree VAE <ref type=\"bibr\" target=\"#b16\">[17]</ref>, DGMG <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Graph"
        },
        {
            "pid": null,
            "content": "hat kind of interpretations are provided, existing techniques can be categorized into example-level <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, and occlusion-based methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target"
        },
        {
            "pid": null,
            "content": "get=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> or model-level <ref cclusion-based methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Instead of providing input-dependent explanations, model-l"
        },
        {
            "pid": null,
            "content": "get=\"#b34\">[35]</ref> to train the generator. According to <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, the loss function for the action a t at step t can be math rmula_8\">10</ref>) is the final graph reward for G t +1 which can be obtained by performing Rollout <ref type=\"bibr\" target=\"#b41\">[42]</ref> m times on the intermediate graph G t +1 . Each time, a fi"
        },
        {
            "pid": "5ac1829d17c44a1fda917eb3",
            "content": "f type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, and link prediction <ref type=\"bibr\" target=\"#b45\">[46]</ref>. In addition, extensive efforts have been made towards dif"
        },
        {
            "pid": null,
            "content": "he graphs are labeled into two different classes according to their mutagenic effect on a bacterium <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Each node is labeled based on its type of atom and there ar this is consistent with the chemical fact that carbon rings and NO 2 chemical groups are mutagenic <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Such observations indicate that the trained GNN classifier"
        },
        {
            "pid": null,
            "content": "the generator is trained based on the feedback from the trained graph models using policy gradient <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We also incorporate several graph rules to encourage the ossible actions. With the reward function, the generator \u0434 \u03b8 (\u2022) can be trained via policy gradient <ref type=\"bibr\" target=\"#b34\">[35]</ref>.</p><p>Reward: The reward for step t, denoted as R t , is i and be valid to graph rules. Since such guidance is not differentiable, we employ policy gradient <ref type=\"bibr\" target=\"#b34\">[35]</ref> to train the generator. According to <ref type=\"bibr\" targ"
        },
        {
            "pid": "5ac1829d17c44a1fda917eb3",
            "content": "f type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, and link prediction <ref type=\"bibr\" target=\"#b45\">[46]</ref>. In addition, extensive efforts have been made towards dif"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": "type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, graph classification <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, and link predicti on networks (GATs) <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and graph isomorphism networks (GINs) <ref type=\"bibr\" target=\"#b38\">[39]</ref>, they share a similar feature learning strategy. For each"
        },
        {
            "pid": "5e5e18d793d709897ce34a43",
            "content": "ef><ref type=\"bibr\" target=\"#b18\">19]</ref>, graph pooling <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, and graph attention <ref type=\"bibr\" target=\"#b9\">[10,</re"
        }
    ],
    "5f7ef3f59fced0a24bebb79d": [
        {
            "pid": "555046eb45ce0a409eb62bcb",
            "content": "user packets are not detoured in transmission, numerous network attack surfaces are opened up today <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b3\">[4]</ref>. For example, an at d path validation that fill the void, such as ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref> and OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>. However, for the targeting environment which is adversarial performance of the Click router as the baseline, and compare our PSVM with the-state-of-the-art OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>.  Method and parameter setup. For fairness, we use the same ave been proposed in ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the Origin and Path Trace (OPT) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=\"bibr\" tar"
        },
        {
            "pid": "5a73cbaa17c44a0b3035d124",
            "content": "t deal of attention has been paid to source authentication <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, the verification of a packet's actual path has been neglect"
        },
        {
            "pid": "5a73cbaa17c44a0b3035d124",
            "content": "t deal of attention has been paid to source authentication <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, the verification of a packet's actual path has been neglect"
        },
        {
            "pid": "53e9a627b7602d9702f5663a",
            "content": "Furthermore, we suppose that node N i and its CGA would use secret methods (such as Diffie-Hellman <ref type=\"bibr\" target=\"#b18\">[19]</ref>) to share the master key (Key N i ), which may be replaced"
        },
        {
            "pid": "53e99fd0b7602d97028ab790",
            "content": "ches for IP path tracking fail to solve the above problems, being unable to identify path deviation <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, there a"
        },
        {
            "pid": "558a31a9e4b037c08755b24f",
            "content": "he above problems, being unable to identify path deviation <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, there are several proposals addressing both sour"
        },
        {
            "pid": "573697486e3b12023e6349a0",
            "content": "sion, numerous network attack surfaces are opened up today <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b3\">[4]</ref>. For example, an attacker may try to flood arbitrary packets unctions from routing nodes, and its design is inspired by <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, some of which we nd Path Trace (OPT) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and PPV <ref type=\"bibr\" target=\"#b27\">[29]</ref>. In ICING"
        },
        {
            "pid": "57d063feac44367354297eb6",
            "content": ""
        },
        {
            "pid": null,
            "content": ">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and PPV <ref type=\"bibr\" target=\"#b27\">[29]</ref>. In ICING, it requires each router to verify the optimized"
        },
        {
            "pid": null,
            "content": "br\" target=\"#b12\">[13]</ref> (especially Pathlet <ref type=\"bibr\" target=\"#b16\">[17]</ref> or SCION <ref type=\"bibr\" target=\"#b17\">[18]</ref> where the path is assigned by end-nodes).</p><p>Moreover,"
        },
        {
            "pid": null,
            "content": "pology analysis <ref type=\"bibr\" target=\"#b13\">[14]</ref>, obtained from some BGP related protocols <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, or employing t"
        }
    ],
    "5f7fdd328de39f0828397ac2": [
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "w method, hence we fail to answer the question.</p><p>Pretrained language models, pioneered by BERT <ref type=\"bibr\" target=\"#b11\">[12]</ref>, have emerged as silver bullets for many NLP tasks, such a ficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, even if the embeddings for larger positions are"
        },
        {
            "pid": "53e997bab7602d9701fa1393",
            "content": "</p><p>The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a human cognitive system storing information for logical re ttentional system capable of selecting and operating control processes and strategies\", as Baddeley <ref type=\"bibr\" target=\"#b1\">[2]</ref> pointed out in his 1992 classic. Later research detailed tha"
        },
        {
            "pid": null,
            "content": "graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we can handle long texts with CogLTX, the prob"
        },
        {
            "pid": null,
            "content": "a human cognitive system storing information for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "5e8d8e6b9fced0a24b5d64aa",
            "content": "ideas were investigated on document-level in DrQA <ref type=\"bibr\" target=\"#b5\">[6]</ref> and ORQA <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and there are also previous works to extract important se"
        },
        {
            "pid": "57a4e91dac44365e35c98845",
            "content": "et=\"#b11\">[12]</ref>, have emerged as silver bullets for many NLP tasks, such as question answering <ref type=\"bibr\" target=\"#b37\">[38]</ref> and text classification <ref type=\"bibr\" target=\"#b21\">[22 BERT (usually 512 tokens). This situation may be rare for normalized benchmarks, for example SQuAD <ref type=\"bibr\" target=\"#b37\">[38]</ref> and GLUE <ref type=\"bibr\" target=\"#b46\">[47]</ref>, but ve"
        },
        {
            "pid": null,
            "content": "1</ref>, the sliding window method suffers from the lack of long-distance attention. Previous works <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> tried to aggregate"
        },
        {
            "pid": "5bdc315017c44a1f58a05c8a",
            "content": "8]</ref> and GLUE <ref type=\"bibr\" target=\"#b46\">[47]</ref>, but very common for more complex tasks <ref type=\"bibr\" target=\"#b52\">[53]</ref> or real-world textual data.</p><p>A straightforward soluti the-art results on four tasks, including NewsQA <ref type=\"bibr\" target=\"#b43\">[44]</ref>, HotpotQA <ref type=\"bibr\" target=\"#b52\">[53]</ref>, 20NewsGroups <ref type=\"bibr\" target=\"#b21\">[22]</ref> an >2</ref>(a)) in nature suggest the answer block as relevant. Even multi-hop datasets, e.g. HotpotQA <ref type=\"bibr\" target=\"#b52\">[53]</ref>, usually annotate supporting sentences. In these cases, th m can be elegantly solved by concatenating all the paragraphs as the input of BERTs.</p><p>HotpotQA <ref type=\"bibr\" target=\"#b52\">[53]</ref> is a multi-hop QA dataset of 112,779 questions, whose dist"
        },
        {
            "pid": "599c7953601a182cd263079b",
            "content": "les attentions between faraway sentences. Similar ideas were investigated on document-level in DrQA <ref type=\"bibr\" target=\"#b5\">[6]</ref> and ORQA <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and ther"
        },
        {
            "pid": null,
            "content": "key-blocks assumption is strongly related to latent variable models, which are usually solved by EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> or variational bayes <ref type=\"bibr\" target=\"#b18\">[19]</ e viewed as a generalization of (conditional) latent variable model p(y|x; \u03b8) \u221d p(z|x)p(y|z; \u03b8). EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> infers the distribution of z as posterior p(z|y, x; \u03b8) in"
        },
        {
            "pid": null,
            "content": "1</ref>, the sliding window method suffers from the lack of long-distance attention. Previous works <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> tried to aggregate"
        },
        {
            "pid": null,
            "content": "te enough without interaction and comparison between blocks, similar to the motivation of reranking <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>MemRecall in nature enables multi-step reasoning by r .  In the first epoch, the judge is nearly untrained and selects some blocks at random. Among them, <ref type=\"bibr\" target=\"#b6\">(7)</ref> contributes most to the correct classification, thus is mark ype=\"formula\" target=\"#formula_0\">1</ref>) is marked as \"relevant\" at once. Then in the next epoch, <ref type=\"bibr\" target=\"#b6\">(7)</ref> becomes not essential for classification and is marked as \"i an).In the first epoch, the judge is nearly untrained and selects some blocks at random. Among them,<ref type=\"bibr\" target=\"#b6\">(7)</ref> contributes most to the correct classification, thus is mark (1) with strong evidence \"prayers\" and (1) is marked as \"relevant\" at once. Then in the next epoch,<ref type=\"bibr\" target=\"#b6\">(7)</ref> becomes not essential for classification and is marked as \"i"
        },
        {
            "pid": null,
            "content": "rking memory decay over time <ref type=\"bibr\" target=\"#b4\">[5]</ref>, unless are kept via rehearsal <ref type=\"bibr\" target=\"#b2\">[3]</ref>, i.e. paying attention to and refreshing the information in y decay to 1/10 of the max learning rates. The common hyperparameters are batch size = 32, strides= <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, t up = 0.2 and t down"
        },
        {
            "pid": null,
            "content": "n his 1992 classic. Later research detailed that the contents in the working memory decay over time <ref type=\"bibr\" target=\"#b4\">[5]</ref>, unless are kept via rehearsal <ref type=\"bibr\" target=\"#b2\" es. The common hyperparameters are batch size = 32, strides= <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, t up = 0.2 and t down = \u22120.05.</p><p>In this section, we sep"
        },
        {
            "pid": null,
            "content": "or p(z|y, x; \u03b8) in E-step, while variational bayes methods <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> use an estimationfriendly q(z|y, x). However, in CogLTX z h"
        },
        {
            "pid": null,
            "content": "rking memory decay over time <ref type=\"bibr\" target=\"#b4\">[5]</ref>, unless are kept via rehearsal <ref type=\"bibr\" target=\"#b2\">[3]</ref>, i.e. paying attention to and refreshing the information in y decay to 1/10 of the max learning rates. The common hyperparameters are batch size = 32, strides= <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, t up = 0.2 and t down"
        },
        {
            "pid": null,
            "content": "ghest scores are inserted into z as much as len(z + ) \u2264 L. The superiority over vector space models <ref type=\"bibr\" target=\"#b39\">[40]</ref> lies in that x i fully interacts with current z + via tran"
        },
        {
            "pid": null,
            "content": "epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as the answer sentence fails to be directly retrieved by re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we ca"
        },
        {
            "pid": null,
            "content": "or p(z|y, x; \u03b8) in E-step, while variational bayes methods <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> use an estimationfriendly q(z|y, x). However, in CogLTX z h"
        },
        {
            "pid": null,
            "content": "hich is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p></div> <div xmlns="
        },
        {
            "pid": null,
            "content": "ion for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> already showed that t"
        },
        {
            "pid": null,
            "content": "n his 1992 classic. Later research detailed that the contents in the working memory decay over time <ref type=\"bibr\" target=\"#b4\">[5]</ref>, unless are kept via rehearsal <ref type=\"bibr\" target=\"#b2\" es. The common hyperparameters are batch size = 32, strides= <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, t up = 0.2 and t down = \u22120.05.</p><p>In this section, we sep"
        }
    ],
    "5f75aa6a9fced0a24b64599c": [
        {
            "pid": "53e9b8f5b7602d97044d854f",
            "content": "nches remain the single most important category of mispredictions to tackle. Prior works like EXACT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, SLB <ref type=\"bibr\" target=\"#b17\">[18]</ref> have proposed ei-c.org/ns/1.0\"><head n=\"2.2\">Existing Techniques to Handle Data Dependent Branches</head><p>EXACT <ref type=\"bibr\" target=\"#b8\">[9]</ref> is a branch prediction technique proposed to lower mispredic pairs that need to get tracked quickly increase the storage requirement over 10KB. As mentioned in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, EXACT predictor does not win over an similarly sized TAGE p leading to the branch. To enable this, we employ a scheme similar to EXACT predictor ID generation <ref type=\"bibr\" target=\"#b8\">[9]</ref> where the ARF is extended to track the load addresses. Inste hat wrote to the same address and use the store data value for override. EXACT's active update unit <ref type=\"bibr\" target=\"#b8\">[9]</ref> and SLB <ref type=\"bibr\" target=\"#b17\">[18]</ref> use this o en. But, when data has high entropy, it affects their accuracy. Among recent works, EXACT predictor <ref type=\"bibr\" target=\"#b8\">[9]</ref> proposes associating data dependent branches with their corr BT. We assume that opcode and other instruction metadata is available with the ROB entry similar to <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>.</p><p>1) 3D-Branch"
        },
        {
            "pid": "5c7958b34895d9cbc63d780d",
            "content": "rred to as local history) to provide a prediction. TAgged GEometric history length predictor (TAGE) <ref type=\"bibr\" target=\"#b34\">[35]</ref> is one of the most successful branch predictor proposals. t=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. The TAGE branch predictor, proposed in <ref type=\"bibr\" target=\"#b34\">[35]</ref>, has proven itself to be very efficient and is part of Cha sed tagged prediction by partial matching (PPM) predictors <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. The TAGE branch predictor, proposed in <ref type=\"bibr\" ta"
        },
        {
            "pid": "5aed14d617c44a44381593e0",
            "content": "s direction. Branch predictor proposals in literature track specific branches in the global history <ref type=\"bibr\" target=\"#b26\">[27]</ref> or only use the specific branch's prior history (referred"
        },
        {
            "pid": "53e9b145b7602d9703bcb487",
            "content": "div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"8\">RELATED WORK</head><p>Prior works such as <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "53e9a57cb7602d9702ea4df4",
            "content": "r is evaluated across 104 workloads spanning multiple application suites. Simpoint-like methodology <ref type=\"bibr\" target=\"#b20\">[21]</ref> is used to identify the representative phases while runnin"
        },
        {
            "pid": null,
            "content": "iding a misprediction.</p><p>In our studies, we morph a highly accurate value predictor called EVES <ref type=\"bibr\" target=\"#b33\">[34]</ref> into a load address predictor (LAP). It is indexed using t ponent to predict the load address. We disable the stride component to simplify LAP. As observed by <ref type=\"bibr\" target=\"#b33\">[34]</ref>, this predictor achieves high accuracy predictions with li eder loads.</p><p>3) Load Address Prediction: As mentioned earlier, we repurpose the EVES predictor <ref type=\"bibr\" target=\"#b33\">[34]</ref> to predict the load addresses. We integrated the 384-entry"
        },
        {
            "pid": "53e999cab7602d970220f327",
            "content": "odal predictors to long branch history based tagged prediction by partial matching (PPM) predictors <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. The TAGE branch p"
        },
        {
            "pid": "558c6c66e4b02b9f07a703d0",
            "content": "oduces the value eventually feeding into the data-dependent branches. Control Flow Decoupling (CFD) <ref type=\"bibr\" target=\"#b35\">[36]</ref>, is another approach to hoist the load out the loop to cre"
        },
        {
            "pid": null,
            "content": "ry of mispredictions to tackle. Prior works like EXACT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, SLB <ref type=\"bibr\" target=\"#b17\">[18]</ref> have proposed different techniques to lower the mispredict tion from the load value allows us to have higher accuracy.</p><p>Store-Load-Branch (SLB) predictor <ref type=\"bibr\" target=\"#b17\">[18]</ref> tracks the stores that feed the value to the data-dependen data value for override. EXACT's active update unit <ref type=\"bibr\" target=\"#b8\">[9]</ref> and SLB <ref type=\"bibr\" target=\"#b17\">[18]</ref> use this observation, but the hardware requirements were p ion and accomplishes this at much less storage. We also discussed Store-load-branch (SLB) predictor <ref type=\"bibr\" target=\"#b17\">[18]</ref> which requires compiler support to identify the store that"
        },
        {
            "pid": null,
            "content": "n extended ARF, we require 14 bytes per entry. Since x64 ISA <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, only has 16 general purpose architectural registers and on"
        },
        {
            "pid": "53e9bb9ab7602d97047e2965",
            "content": "get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> on enhancing branch predictors based on the correlation wit"
        }
    ],
    "5f857a2791e011ff32809698": [
        {
            "pid": "58437771ac44360f1083f46e",
            "content": "tative set of latency-critical applications, we use the benchmarks of the TailBench benchmark suite <ref type=\"bibr\" target=\"#b18\">[20]</ref>. This suite includes eight representative applications of . However, we found that these values are in line with the results of this application presented in <ref type=\"bibr\" target=\"#b18\">[20]</ref>.</p><p>Once we have defined the QoS requirements for each saturation QPS (i.e., point at which the LQoS is achieved). This issue has been also pointed out in <ref type=\"bibr\" target=\"#b18\">[20]</ref>, where authors realized of this unexpected behavior and de"
        },
        {
            "pid": "5c96086e3cb210d2716c49c1",
            "content": "those works that do not consider virtualization. With the aim of improving system fairness, CoPart <ref type=\"bibr\" target=\"#b2\">[3]</ref> leverages Intel's CAT and MBA technologies and characterizes"
        },
        {
            "pid": "53e9b86db7602d9704437300",
            "content": "iques. In the experimental evaluation, mostly high performance benchmarks are employed. Finally, in <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the Quasar cluster management system is presented. The main"
        },
        {
            "pid": null,
            "content": "Hyper-Threading</head><p>The processor of the experimental platform implements the Hyper-Threading <ref type=\"bibr\" target=\"#b16\">[18]</ref> technology, that is, Intel's implementation of the simulta"
        },
        {
            "pid": null,
            "content": "nsider just LXC virtualization <ref type=\"bibr\" target=\"#b5\">[6]</ref> (i.e., Linux containers). In <ref type=\"bibr\" target=\"#b6\">[7]</ref>, D. Lo et al. aim to increase server utilization by tolerati"
        },
        {
            "pid": null,
            "content": "ref> technology, that is, Intel's implementation of the simultaneous multi-threading (SMT) paradigm <ref type=\"bibr\" target=\"#b17\">[19]</ref>.</p><p>The key characteristic of the SMT architecture is i"
        },
        {
            "pid": "5ca72ae8181a2f3597ce6e25",
            "content": "then limits the amount of resources assigned to the BE workloads. Pursuing the same target, PARTIES <ref type=\"bibr\" target=\"#b7\">[8]</ref> monitors tail latency applications, memory capacity and netw"
        },
        {
            "pid": "55503ed645ce0a409eb2ad68",
            "content": "ds. However, authors do not consider reducing unpredictability through resource partitioning. AVMMC <ref type=\"bibr\" target=\"#b9\">[10]</ref>, on the other hand, determines dynamically the best VM to c"
        },
        {
            "pid": null,
            "content": "Hyper-Threading</head><p>The processor of the experimental platform implements the Hyper-Threading <ref type=\"bibr\" target=\"#b16\">[18]</ref> technology, that is, Intel's implementation of the simulta"
        },
        {
            "pid": "5c96086e3cb210d2716c49c1",
            "content": "those works that do not consider virtualization. With the aim of improving system fairness, CoPart <ref type=\"bibr\" target=\"#b2\">[3]</ref> leverages Intel's CAT and MBA technologies and characterizes"
        },
        {
            "pid": null,
            "content": "ref> technology, that is, Intel's implementation of the simultaneous multi-threading (SMT) paradigm <ref type=\"bibr\" target=\"#b17\">[19]</ref>.</p><p>The key characteristic of the SMT architecture is i"
        }
    ],
    "5f7fdd328de39f0828397afd": [
        {
            "pid": "5f02f17c91e011ee5e0258c8",
            "content": "preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo <ref type=\"bibr\" target=\"#b3\">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood cency matrix \u00c3 and feature matrix X in the precomputation phase, which requires O(LmF ) time. PPRGo <ref type=\"bibr\" target=\"#b3\">[4]</ref> calculates approximate the Personalized PageRank (PPR) matri in APPNP and PPRGo <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>; 2) w = 0 for = 0, . . . , L \u2212 1 and w L = 1, in which case P br\" target=\"#b36\">[37]</ref>, SGC and PPRGo (linear model) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initia"
        },
        {
            "pid": "58437722ac44360f1082efeb",
            "content": "r\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\"#b14\">[15]</ref> adopts a message-passing approach and gathers information vertices and edges, respectively. For ease of presentation, we assume that G is a self-looped graph <ref type=\"bibr\" target=\"#b14\">[15]</ref>, with a self-loop attached to each node in V . Let n = |V and d(u) = |N (u)| is the degree of u. We use d = m n to denote the average degree of G. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we define the normalized adjacency matrix of G as \u00c3 = D \u2212 ork Friendster.</p><p>Baselines and detailed setup. We adopt three state-of-the-art GNN methods GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, GDC <ref t e node classification task on the three small standard graphs Cora, Citeseer, and Pubmed. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we apply the standard fixed training/validation/testing s 0 and 1, the convolution matrix D r\u22121 AD \u2212r represents the symmetric normalization adjacency matrix <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5cf48a3eda56291d582a1174",
            "content": "guarantees the connectivity of the sampled adjacency matrix. 2) Graph Sampling methods: Cluster-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> builds a complete GCN from clusters in each mini-batch. Grap samples s l is usually much larger than the average degree d. Furthermore, it has been observed in <ref type=\"bibr\" target=\"#b7\">[8]</ref> that the overlapping nodes in different batches will lead to g of each batch and perform forward propagation on the same subgraph across all layers. Cluster-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> uses graph clustering techniques <ref type=\"bibr\" target=\"#b interaction network Yelp <ref type=\"bibr\" target=\"#b36\">[37]</ref>, a co-purchasing networks Amazon <ref type=\"bibr\" target=\"#b7\">[8]</ref> and a large social network Friendster <ref type=\"bibr\" targe =\"#b15\">16]</ref>, the transition probability matrix AD \u22121 <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, and the reverse tran e use \"fixed-partition\" splits for each dataset, following <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> (see the supplementary materials for further details). The cr"
        },
        {
            "pid": null,
            "content": "use seven open graph datasets with different size: three citation networks Cora, Citeser and Pubmed <ref type=\"bibr\" target=\"#b24\">[25]</ref>, a Protein-Protein interaction network PPI <ref type=\"bibr"
        },
        {
            "pid": null,
            "content": "use seven open graph datasets with different size: three citation networks Cora, Citeser and Pubmed <ref type=\"bibr\" target=\"#b24\">[25]</ref>, a Protein-Protein interaction network PPI <ref type=\"bibr"
        },
        {
            "pid": "5b67b47917c44aac1c8637c6",
            "content": "get=\"#b0\">[1]</ref> mixes higher-order information to learn a wider class of representations. JKNet <ref type=\"bibr\" target=\"#b31\">[32]</ref> explores the relationship between node influence and rando /ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, and the reverse transition probability matrix D \u22121 A <ref type=\"bibr\" target=\"#b31\">[32]</ref>, respectively. We can also manipulate the weights w to sim"
        },
        {
            "pid": "5b67b45517c44aac1c860876",
            "content": "type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, recommendation systems <ref type=\"bibr\" target=\"#b35\">[36]</ref>, and computer vision <ref type=\"bibr\" target=\"#b38\">[39,</"
        },
        {
            "pid": "573696026e3b12023e515eec",
            "content": "t=\"#b3\">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initial residual connection <ref type=\"bibr\" target=\"#b11\">[12]</ref> across the hidden layers to facilitate training. For simpl"
        },
        {
            "pid": "5cf48a3eda56291d582a1174",
            "content": "guarantees the connectivity of the sampled adjacency matrix. 2) Graph Sampling methods: Cluster-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> builds a complete GCN from clusters in each mini-batch. Grap samples s l is usually much larger than the average degree d. Furthermore, it has been observed in <ref type=\"bibr\" target=\"#b7\">[8]</ref> that the overlapping nodes in different batches will lead to g of each batch and perform forward propagation on the same subgraph across all layers. Cluster-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> uses graph clustering techniques <ref type=\"bibr\" target=\"#b interaction network Yelp <ref type=\"bibr\" target=\"#b36\">[37]</ref>, a co-purchasing networks Amazon <ref type=\"bibr\" target=\"#b7\">[8]</ref> and a large social network Friendster <ref type=\"bibr\" targe =\"#b15\">16]</ref>, the transition probability matrix AD \u22121 <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, and the reverse tran e use \"fixed-partition\" splits for each dataset, following <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> (see the supplementary materials for further details). The cr"
        },
        {
            "pid": "5db9292e47c8f766461eff27",
            "content": "ach node's degree and keeps a constant sample size in all layers to achieve scales linearly. LADIES <ref type=\"bibr\" target=\"#b39\">[40]</ref> further proposes a layer-dependent sampler to constrain ne rs from worse time and space complexity. FastGCN <ref type=\"bibr\" target=\"#b4\">[5]</ref> and LADIES <ref type=\"bibr\" target=\"#b39\">[40]</ref> restrict the same sample size across all layers to limit t lso use one state-ofthe-art scalable GNN from each of the three categories: LADIES (layer sampling) <ref type=\"bibr\" target=\"#b39\">[40]</ref>, GraphSAINT (graph sampling) <ref type=\"bibr\" target=\"#b36"
        },
        {
            "pid": "5d9edc8347c8f76646042a37",
            "content": "duces a normalization technique to eliminate biases of mini-batch estimation. 3) Linear Models: SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> computes the product of the feature matrix and the k-th po final feature propagation matrix and result in an optimal training time complexity of O(nF 2 ). SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> repeatedly perform multiplication of normalized adjacency . To achieve high scalability, we borrow the idea of decoupling prediction and propagation from SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> and APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In pa . , L \u2212 1 and w L = 1, in which case P degenerates to the L-th transition probability matrix in SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p><formula xml:id=\"formula_3\">D \u22121/2 AD \u22121/2</formula></ l datasets. For the scalable GNNs, SGC is outperformed by the vanilla GCN due to the simplification <ref type=\"bibr\" target=\"#b29\">[30]</ref>. On the other hand, the results of LADIES and GraphSAINT a \u2212r represents the symmetric normalization adjacency matrix <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, the transition pro GraphSAINT (graph sampling) <ref type=\"bibr\" target=\"#b36\">[37]</ref>, SGC and PPRGo (linear model) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p><p>We implement"
        }
    ],
    "5fd3404791e01161cf73952c": [
        {
            "pid": "5c8dd94c4895d9cbc6a7d918",
            "content": "for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. To remedy such a prob e roughly divided into two branches: random walk-based and reconstruction-based methods. The former <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target ures with the assumption that nodes within the sliding window or neighborhoods are closely relevant <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" targ <ref type=\"bibr\" target=\"#b44\">[44]</ref>, PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref>, BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref> and FOBE <ref type=\"bibr\" target=\"#b32\">[32]</ref> are speci iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1.1\">Data Preprocessing.</head><p>As used in BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref>, we select 60% edges for training and remaining edges for te [36]</ref>. \u2022 Bipartite graph embedding: PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref> and BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref>.</p><p>PinSage integrates random walk into GNN architectures"
        },
        {
            "pid": "5e3a92413a55ac054d0cdbf7",
            "content": "plit graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI <ref type=\"bibr\" target=\"#b26\">[26]</ref> proposes a new approach  The yellow dotted lines (Eq.( <re"
        },
        {
            "pid": "5a260c8117c44a4ba8a30adf",
            "content": "aph embedding paradigm <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b5\">5,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. Although they work pretty well in the settings of homogene"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "Because it follows a standard minmax game originated from the generative adversarial network (GAN) <ref type=\"bibr\" target=\"#b10\">[10]</ref>, and the \"GAN\" distance and Jensen-Shannon divergence are"
        },
        {
            "pid": "5a260c3517c44a4ba8a25252",
            "content": "ed methods. The former <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target=\"#b44\">44]</ref> relies on designing the heuristics of random walks to gener graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE <ref type=\"bibr\" target=\"#b44\">[44]</ref>, PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref>, BiNE <"
        },
        {
            "pid": "5f8ebbb99fced0a24b4e19be",
            "content": "The negative sampling used in Eq.( <ref type=\"formula\" target=\"#formula_17\">14</ref>) is similar to <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b21\">21]</ref>: \ud835\udc38 \u2032 (\ud835\udc62,\ud835\udc63) is comp n rate \ud835\udefd in Eq.( <ref type=\"formula\" target=\"#formula_10\">9</ref>) and the harmonic factor \ud835\udf06 in Eq. <ref type=\"bibr\" target=\"#b12\">(12)</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_5\">4</"
        },
        {
            "pid": "5cf48a48da56291d582ab75a",
            "content": "s within a sliding window <ref type=\"bibr\" target=\"#b45\">[45]</ref>. The reconstruction-based works <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" ta \"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> and collaborative filtering <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b37\">37]</ref> are also connected ed on local subgraphs for the task of inductive matrix completion. \u2022 Collaborative filtering: NeuMF <ref type=\"bibr\" target=\"#b15\">[15]</ref> and NGCF <ref type=\"bibr\" target=\"#b37\">[37]</ref>. NeuMF"
        },
        {
            "pid": "5cf48a48da56291d582ab75a",
            "content": "s within a sliding window <ref type=\"bibr\" target=\"#b45\">[45]</ref>. The reconstruction-based works <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" ta \"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> and collaborative filtering <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b37\">37]</ref> are also connected ed on local subgraphs for the task of inductive matrix completion. \u2022 Collaborative filtering: NeuMF <ref type=\"bibr\" target=\"#b15\">[15]</ref> and NGCF <ref type=\"bibr\" target=\"#b37\">[37]</ref>. NeuMF"
        },
        {
            "pid": null,
            "content": "Because it follows a standard minmax game originated from the generative adversarial network (GAN) <ref type=\"bibr\" target=\"#b10\">[10]</ref>, and the \"GAN\" distance and Jensen-Shannon divergence are"
        },
        {
            "pid": null,
            "content": "ced by a random node from the same node set. BiGI is an end-to-end model which is optimized by Adam <ref type=\"bibr\" target=\"#b17\">[17]</ref>. The whole architecture of BiGI is shown in Figure <ref ty"
        },
        {
            "pid": "5a260c8117c44a4ba8a30adf",
            "content": "aph embedding paradigm <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b5\">5,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. Although they work pretty well in the settings of homogene"
        },
        {
            "pid": null,
            "content": "target=\"#b10\">[10]</ref>, and the \"GAN\" distance and Jensen-Shannon divergence are closely related <ref type=\"bibr\" target=\"#b24\">[24]</ref>.</p><p>From Eq.( <ref type=\"formula\" target=\"#formula_4\">5"
        },
        {
            "pid": "5a260c3517c44a4ba8a25252",
            "content": "ed methods. The former <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target=\"#b44\">44]</ref> relies on designing the heuristics of random walks to gener graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE <ref type=\"bibr\" target=\"#b44\">[44]</ref>, PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref>, BiNE <"
        },
        {
            "pid": null,
            "content": ">40,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> are closely related with collaborative filtering <ref type=\"bibr\" target=\"#b28\">[28]</ref>. They attempt to reconstruct the adjacency matrix by learn"
        },
        {
            "pid": "5736977f6e3b12023e66632b",
            "content": "oneering homogeneous graph methods include DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec <ref type=\"bibr\" target=\"#b11\">[11]</ref> and VG into:</p><p>\u2022 Homogeneous graph embedding: DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec <ref type=\"bibr\" target=\"#b11\">[11]</ref> and VG"
        },
        {
            "pid": "53e9b253b7602d9703cf4028",
            "content": "ually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec following strong baselines which can be divided into:</p><p>\u2022 Homogeneous graph embedding: DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec"
        },
        {
            "pid": "5e807d589fced0a24b30b594",
            "content": "rget=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b22\">22,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> to learn node representations via aggregating features of n"
        },
        {
            "pid": "53e9b253b7602d9703cf4028",
            "content": "ually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec following strong baselines which can be divided into:</p><p>\u2022 Homogeneous graph embedding: DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec"
        },
        {
            "pid": null,
            "content": "s coming from the joint distribution of two random variables or the product of their marginals. DIM <ref type=\"bibr\" target=\"#b16\">[16]</ref> introduces the structural information into input patches a"
        },
        {
            "pid": null,
            "content": "ced by a random node from the same node set. BiGI is an end-to-end model which is optimized by Adam <ref type=\"bibr\" target=\"#b17\">[17]</ref>. The whole architecture of BiGI is shown in Figure <ref ty"
        },
        {
            "pid": "5b67b45517c44aac1c860876",
            "content": "get=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b34\">34,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> are closely related icular, some works <ref type=\"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> train graph neural tite graph are hard to be preserved by them. IGE <ref type=\"bibr\" target=\"#b44\">[44]</ref>, PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref>, BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref> and FOBE <re ax objective in DGI <ref type=\"bibr\" target=\"#b36\">[36]</ref>. \u2022 Bipartite graph embedding: PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref> and BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref>.</p><p>Pi th our infomax objective. Compared with other GNN encoders <ref type=\"bibr\" target=\"#b37\">[37,</ref><ref type=\"bibr\" target=\"#b40\">40]</ref> for bipartite graphs, it achieves promising performances em"
        }
    ],
    "5f64211c9e795e0286c313a2": [
        {
            "pid": "599c7963601a182cd26379b2",
            "content": "two well-accepted propositions-deep neural networks learn meaningful patterns before fitting noise <ref type=\"bibr\" target=\"#b2\">[3]</ref> and minimum entropy regularisation principle [10]-we propose meaningful patterns before fitting noise, even when severe label noise exists in human annotations <ref type=\"bibr\" target=\"#b2\">[3]</ref>. (2) As a learner attains confident knowledge as time progre e meaningful patterns before fitting noise, even when severe label noise exists in human annotations<ref type=\"bibr\" target=\"#b2\">[3]</ref>. (2) As a learner attains confident knowledge as time progre"
        },
        {
            "pid": "53e9be09b7602d9704abe386",
            "content": "mum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>Secondly, note imum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios<ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.Secondly, note that O n machine learning <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target"
        },
        {
            "pid": null,
            "content": "ef type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. In another self KD <ref type=\"bibr\" target=\"#b56\">[57]</ref>, the deepest classifier provides knowledge for shallower c t, self KD methods <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> maximise the consistency of intraclass images' predictions"
        },
        {
            "pid": "5b67b47917c44aac1c86372e",
            "content": ") Co-training strategies, which train two or more learners <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar s from the methods <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5b67b47917c44aac1c86372e",
            "content": ") Co-training strategies, which train two or more learners <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar s from the methods <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5550417545ce0a409eb3b767",
            "content": "edge distillation (KD), which exploits the predictions of other model(s), usually termed teacher(s) <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Self LC methods include Pseudo-Label <ref type=\"bibr\" tar ef><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref> and Non-self LC <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The parameter \u01eb defines how much a predicted label distri rrect noisy labels.</p><p>LC and knowledge distillation (KD) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Mathematically, we derive that some KD methods also modify he probability mass function.</p><p>Label smoothing. In LS <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, we soften one-hot targets by adding a uniform distribution In KD, an auxiliary teacher model can provide a student model the similarity structure information <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>; (2) In Self LC, e"
        },
        {
            "pid": "5c5ce4fd17c44a400fc38b9b",
            "content": "et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "ing self KD, two examples of the same class are constrained to have consistent output distributions <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. In another self K learned in the first stage. Our focus is to improve the end-to-end self LC. First, self KD methods <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5550415945ce0a409eb3a820",
            "content": "</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set <ref type=\"bibr\" target=\"#b38\">[39]</ref>. We use SGD with a start learning rate of 2e \u2212 3. A polyno ]</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set<ref type=\"bibr\" target=\"#b38\">[39]</ref>. We use SGD with a start learning rate of 2e \u2212 3. A polyno"
        },
        {
            "pid": null,
            "content": "ef type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. In another self KD <ref type=\"bibr\" target=\"#b56\">[57]</ref>, the deepest classifier provides knowledge for shallower c t, self KD methods <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> maximise the consistency of intraclass images' predictions"
        },
        {
            "pid": "5aed14d617c44a4438159292",
            "content": "Boot-hard) <ref type=\"bibr\" target=\"#b34\">[35]</ref>, Joint Optimisation (Jointsoft and Joint-hard) <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and Tf-KD self <ref type=\"bibr\" target=\"#b54\">[55]</ref>. o hard (SH). The other baselines have been introduced heretofore.</p><p>Training details. We follow <ref type=\"bibr\" target=\"#b42\">[43]</ref> to train ResNet-50 and initialise it by a trained model on C contains Self LC <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref> and Non-self LC <re et=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "en two models are trained, the consistency between their predictions of a data point is promoted in <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>, while the distance"
        },
        {
            "pid": null,
            "content": "ef type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. In another self KD <ref type=\"bibr\" target=\"#b56\">[57]</ref>, the deepest classifier provides knowledge for shallower c t, self KD methods <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> maximise the consistency of intraclass images' predictions"
        },
        {
            "pid": "5b67b47917c44aac1c863874",
            "content": "f type=\"bibr\" target=\"#b31\">[32]</ref>. D2L monitors the subspace dimensionality change at training <ref type=\"bibr\" target=\"#b26\">[27]</ref>. GCE denotes generalised cross entropy <ref type=\"bibr\" ta"
        },
        {
            "pid": "5550415945ce0a409eb3a820",
            "content": "</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set <ref type=\"bibr\" target=\"#b38\">[39]</ref>. We use SGD with a start learning rate of 2e \u2212 3. A polyno ]</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set<ref type=\"bibr\" target=\"#b38\">[39]</ref>. We use SGD with a start learning rate of 2e \u2212 3. A polyno"
        },
        {
            "pid": null,
            "content": "rget=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" tar used only for validation, which is generally necessary for any method and differs from the methods <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5b67b47917c44aac1c86372e",
            "content": ") Co-training strategies, which train two or more learners <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar s from the methods <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5ce2d0b7ced107d4c63ac3e1",
            "content": "arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, which relate to ou"
        },
        {
            "pid": "58d82fc8d649053542fd5ae7",
            "content": "\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and confidence penalty (CP) <ref type=\"bibr\" target=\"#b32\">[33]</ref>; <ref type=\"bibr\" target=\"#b1\">(2)</ref> Label correction 3 1/6 1 0 0 0 0 1 1</formula><p>(a) OR includes LS <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CP <ref type=\"bibr\" target=\"#b32\">[33]</ref>. LS softens a target by adding a uniform label distributio ) = E qLS (\u2212 log p) = (1 \u2212 \u01eb)H(q, p)+\u01ebH(u, p).<label>(3)</label></formula><p>Confidence penalty. CP <ref type=\"bibr\" target=\"#b32\">[33]</ref> penalises highly confident predictions:</p><formula xml:id e penalty practice <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Finally, we summaris marked contrast to recent practices of confidence penalty <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>.</p></div><figure xml"
        },
        {
            "pid": "5ce3ad3fced107d4c65b6bd9",
            "content": "ibr\" target=\"#b28\">29]</ref> and confidence penalty (CP) <ref type=\"bibr\" target=\"#b32\">[33]</ref>; <ref type=\"bibr\" target=\"#b1\">(2)</ref> Label correction (LC). On the one hand, LC regularises neura sible knowledge from other learners or itself (Figure <ref type=\"figure\" target=\"#fig_1\">1a</ref>); <ref type=\"bibr\" target=\"#b1\">(2)</ref> Non-self LC relies on accurate auxiliary models to generate layer, while Masking <ref type=\"bibr\" target=\"#b11\">[12]</ref> exploits human cognition. MD-DYR-SH <ref type=\"bibr\" target=\"#b1\">[2]</ref> is a combination of three techniques: dynamic mixup (MD), dy"
        },
        {
            "pid": "5e621f3d91e01160711d60e7",
            "content": "get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "573697826e3b12023e6690c5",
            "content": "get=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. A noise-transition matrix is difficult and complex to esti <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3.\">Real-world label noise</head><p>Clothing 1M <ref type=\"bibr\" target=\"#b47\">[48]</ref> has around 38.46% label noise in the training data and abo"
        }
    ],
    "5f9be24691e011dcf482d8d6": [
        {
            "pid": "5a260c2e17c44a4ba8a24027",
            "content": "ion.</p><p>Accurately predicting (black-box) workload criticality is itself a challenge. Prior work <ref type=\"bibr\" target=\"#b5\">[6]</ref> associated a diurnal utilization pattern with user interacti system</head><p>To integrate predictions into VM scheduling in practice, we target Resource Central <ref type=\"bibr\" target=\"#b5\">[6]</ref>, the existing ML and predictionserving system in Azure. The e workload of each VM is performance-critical or not, before we can train a model. As in prior work <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we consider a workload critical if it is user-facing, i.e. he problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Obviously, some background VMs may exhibit 24-hour pe identifying periods in time series, such as FFT or the autocorrelation function (ACF). For example, <ref type=\"bibr\" target=\"#b5\">[6]</ref> assumes a workload is user-facing if the FFT indicates a 24- /task length for provisioning or scheduling purposes, e.g. <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\""
        },
        {
            "pid": "558b4f2584ae84d265c2ab16",
            "content": "orage to shave power peaks in oversubscribed datacenters <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. When peaks last long, this approach may require large amo"
        },
        {
            "pid": "53e9a952b7602d97032a78d4",
            "content": "he BMC splits the cap evenly across its sockets and uses Intel's Running Average Power Limit (RAPL) <ref type=\"bibr\" target=\"#b6\">[7]</ref> to lower the blade power (step 5). RAPL throttles the entire"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5aed146117c44a44381527e4",
            "content": "orrelated peaks <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Our work exten ave studied hierarchical capping in production datacenters <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Our paper focu"
        },
        {
            "pid": "53e9a17fb7602d9702a7223e",
            "content": ", researchers have proposed using energy storage to shave power peaks in oversubscribed datacenters <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. When peaks las"
        },
        {
            "pid": "53e9b1d1b7602d9703c64131",
            "content": "=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib"
        },
        {
            "pid": null,
            "content": "\">[10]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>. In contrast, we introduce a new algorithm and ML models f"
        },
        {
            "pid": "53e9b97cb7602d970456a7fb",
            "content": "\">[15]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bib"
        },
        {
            "pid": "53e9af0cb7602d9703940497",
            "content": "7\">[8]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Unfortunately, they are often impractical for a cloud pro"
        },
        {
            "pid": "558b48e6e4b031bae1fc8c5b",
            "content": "b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>. In contrast, w"
        }
    ],
    "5fae6dced4150a363cec41f7": [
        {
            "pid": "5c757dbaf56def9798b16bf3",
            "content": ""
        },
        {
            "pid": "5c8b9ec64895d9cbc69dfbb6",
            "content": "research area, the studies using deep learning approaches mostly focus on dimensionality reduction <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b5\">[6]</ref> and anomaly-based i"
        },
        {
            "pid": "53e9baf2b7602d9704726b8f",
            "content": "large datasets and real-valued outputs <ref type=\"bibr\" target=\"#b63\">[64]</ref>. Nesterovs updater <ref type=\"bibr\" target=\"#b66\">[67]</ref> is selected because it uses the momentum that supports the"
        },
        {
            "pid": "53e9a75fb7602d9703097666",
            "content": "g an appropriate metric. For a binary classification, the results can be separated into four groups <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref> </p><p>Receiver"
        },
        {
            "pid": "5736961b6e3b12023e52dbe4",
            "content": "located around hosts of the networks. The Ward Clustering approach was recommended by Satoh et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> to identify simple and malicious attacks from the SSH dict"
        },
        {
            "pid": null,
            "content": ", the normalization was carried out in the features of the dataset using the feature scaling method <ref type=\"bibr\" target=\"#b70\">[72]</ref>, which was also called unity-based normalization, to bring"
        },
        {
            "pid": null,
            "content": ", the normalization was carried out in the features of the dataset using the feature scaling method <ref type=\"bibr\" target=\"#b70\">[72]</ref>, which was also called unity-based normalization, to bring"
        },
        {
            "pid": null,
            "content": "Netflow records into kernel function and forwarding the calculated results to an OCSVM. Umer et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> proposed an intrusion detection model, which handled the f"
        },
        {
            "pid": "53e99a2bb7602d9702284699",
            "content": "et=\"#b44\">[45]</ref>.</p><p>The SVM method is modified into a One-Class SVM (OCSVM) as explained in <ref type=\"bibr\" target=\"#b45\">[46]</ref>. The dataset, which is given as an input to the algorithm tliers'', are employed as negative examples. The formulation of OCSVM can be carried out as follows <ref type=\"bibr\" target=\"#b45\">[46]</ref>:</p><formula xml:id=\"formula_5\">f (x) = +1, if x ? S -1, i"
        },
        {
            "pid": "5c757447f56def979892b4d5",
            "content": "tures, too, is used to evaluate the methods in the studies <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The NSL-KDD dataset <ref type=\"bibr\" target=\"#b12\">[13]</"
        },
        {
            "pid": null,
            "content": "o find a hyperplane to separate these transformed feature vectors from the origin by maximum margin <ref type=\"bibr\" target=\"#b46\">[47]</ref>. Given a dataset (x 1 , y 1 ), . . . , (x N , y N ) ? R n"
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": "bounded between 0 and 1. If AUC value is less than 0.5, it means that the classifier is unrealistic <ref type=\"bibr\" target=\"#b53\">[54]</ref>. A rough classifying system can serve as a guidance to the"
        },
        {
            "pid": "558c033fe4b0cfb70a1b1eaa",
            "content": "nown, multi-layer and radial basis function networks are used to classify the attack. Jadidi et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> proposed a method that was based on Multi-Layer Perceptron"
        },
        {
            "pid": "53e9a53ab7602d9702e5f540",
            "content": "IDS proposed was trained with malicious data, as opposed to the previous approaches. Wagner et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> presented an anomaly detection method by processing large"
        },
        {
            "pid": "5550437e45ce0a409eb47cf5",
            "content": "iew of this manuscript and approving it for publication was Kim-Kwang Raymond Choo . on the network <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Intrusion detection approaches are basically divided into t r\" target=\"#b0\">[1]</ref>. Intrusion detection approaches are basically divided into two categories <ref type=\"bibr\" target=\"#b0\">[1]</ref>: misuse-based and anomaly-based. Misuse-based techniques wor S, which was supplied by NetFlow data, it was added in a high-frequency FPGA board. Abuadlla et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>, proposed an IDS to detect and classify certain intrusions i"
        },
        {
            "pid": "5c0f8549da562944ac906b35",
            "content": "in the optimization of hyperparameters as the training process is not executed using anomalous data <ref type=\"bibr\" target=\"#b62\">[63]</ref>. Thus, the tuning of hyperparameters is performed by takin"
        },
        {
            "pid": "5c0f8549da562944ac906b35",
            "content": "in the optimization of hyperparameters as the training process is not executed using anomalous data <ref type=\"bibr\" target=\"#b62\">[63]</ref>. Thus, the tuning of hyperparameters is performed by takin"
        },
        {
            "pid": "5b8c9f1617c44af36f8b2f1d",
            "content": "rget=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b5\">[6]</ref> and anomaly-based intrusion detection <ref type=\"bibr\" target=\"#b6\">[7]</ref>- <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>The KDDCUP9"
        },
        {
            "pid": "5736971c6e3b12023e610be0",
            "content": "f type=\"bibr\" target=\"#b56\">[57]</ref>, CTU-13 <ref type=\"bibr\" target=\"#b57\">[58]</ref>, UNSW-NB15 <ref type=\"bibr\" target=\"#b58\">[59]</ref>, CIDDS-001 <ref type=\"bibr\" target=\"#b59\">[60]</ref> and C"
        },
        {
            "pid": "5550491445ce0a409eb73fba",
            "content": "logic that trains the input vectors to reconstruct as output vectors with an unsupervised approach <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Its architecture is basically constructed by an encoder a s used in AE configuration, which are commonly used and constitute the basis of the studies such as <ref type=\"bibr\" target=\"#b40\">[41]</ref>, <ref type=\"bibr\" target=\"#b68\">[69]</ref>, the sigmoid ac"
        },
        {
            "pid": null,
            "content": "-based intrusion detection can be found in studies of <ref type=\"bibr\" target=\"#b14\">[15]</ref> and <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>The objective of ANNs is to model the human brain,"
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": "bounded between 0 and 1. If AUC value is less than 0.5, it means that the classifier is unrealistic <ref type=\"bibr\" target=\"#b53\">[54]</ref>. A rough classifying system can serve as a guidance to the"
        }
    ],
    "5efb0d5691e011063336d39c": [
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "rained language models (e.g., ELMo <ref type=\"bibr\" target=\"#b29\">(Peters et al., 2018)</ref>, BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, XLnet <ref type=\"bibr\" target=\"#b45\">(Yan e achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al., entations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Limsopatham"
        },
        {
            "pid": "5bbacb9e17c44aecc4eaff64",
            "content": "opose to train an NER model with annotated language that are closely related to the target language.<ref type=\"bibr\" target=\"#b43\">Xie et al. (2018)</ref> propose to use the bilingual dictionaries to"
        },
        {
            "pid": "5a73cb5d17c44a0b303573b1",
            "content": "arch focuses on leveraging cross lingual information to improve the model performance. For examples,<ref type=\"bibr\" target=\"#b4\">Cotterell and Duh (2017)</ref>;<ref type=\"bibr\" target=\"#b6\">Feng et a"
        },
        {
            "pid": "5550456245ce0a409eb55cee",
            "content": "s, we implement LSTM-CNN-CRF with Pytorch 15 and use the pre-trained 100 dimension GloVe Embeddings <ref type=\"bibr\" target=\"#b28\">(Pennington et al., 2014)</ref> as the input vector. Then, we set the"
        },
        {
            "pid": "5da052ba3a55acfef148243e",
            "content": "ditional language model with a KB and use entity type information to enhance the model. (ii) ConNET <ref type=\"bibr\" target=\"#b16\">(Lan et al., 2020a)</ref> leverages multiple crowd annotation and dyn"
        },
        {
            "pid": null,
            "content": "extra training data. Therefore, we only compare with the results reported in their papers. (i) KALM <ref type=\"bibr\" target=\"#b20\">(Liu et al., 2019a)</ref> augments a traditional language model with rove the performance, as shown in the ablation study in our experiments.</p><p>Other related works: <ref type=\"bibr\" target=\"#b20\">Liu et al. (2019a)</ref> propose a language model-based method -KALM"
        },
        {
            "pid": "5b67b46417c44aac1c861297",
            "content": "ve the model performance. For examples,<ref type=\"bibr\" target=\"#b4\">Cotterell and Duh (2017)</ref>;<ref type=\"bibr\" target=\"#b6\">Feng et al. (2018)</ref> consider NER for a low resource target langua"
        },
        {
            "pid": "5b1642d68fbcbf6e5a9b85a3",
            "content": "question answering <ref type=\"bibr\" target=\"#b13\">(Khalid et al., 2008)</ref> and dialogue systems <ref type=\"bibr\" target=\"#b1\">(Bowden et al., 2018)</ref>. Traditional approaches to NER mainly trai"
        },
        {
            "pid": null,
            "content": "ity. However, this filtering strategy improves the precision at the expense of lowering the recall. <ref type=\"bibr\" target=\"#b2\">Cao et al. Cao et al. (2019)</ref> attempt to induce labels for entity possible labels and then maximizing the overall likelihood using a fuzzy LSTM-CRF model; (iii) LRNT <ref type=\"bibr\" target=\"#b2\">(Cao et al., 2019)</ref> is the state-of-the-art model for low-resourc"
        },
        {
            "pid": null,
            "content": "\"bibr\" target=\"#b22\">Liu et al., 2019b;</ref><ref type=\"bibr\" target=\"#b45\">Yang et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Lan et al., 2020b;</ref><ref type=\"bibr\" target=\"#b30\">Raffel et al., RT and its variants (e.g., RoBERTa <ref type=\"bibr\" target=\"#b22\">(Liu et al., 2019b)</ref>, ALBERT <ref type=\"bibr\" target=\"#b17\">(Lan et al., 2020b)</ref> and T5 <ref type=\"bibr\" target=\"#b30\">(Raff"
        },
        {
            "pid": "5bbacb9e17c44aecc4eaff64",
            "content": "opose to train an NER model with annotated language that are closely related to the target language.<ref type=\"bibr\" target=\"#b43\">Xie et al. (2018)</ref> propose to use the bilingual dictionaries to"
        }
    ],
    "5f75aa6a9fced0a24b64599d": [
        {
            "pid": "5f75aa6a9fced0a24b64599d",
            "content": "memory-hierarchy parallelism (MHP). <ref type=\"foot\" target=\"#foot_0\">1</ref> Load Slice Core (LSC) <ref type=\"bibr\" target=\"#b4\">[5]</ref> was the first work to propose an sOoO core; Freeway <ref typ TIVATION</head><p>In this section, we briefly cover the background on the two prior sOoO cores -LSC <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> -and we FSC.</p><p>Restricted Out-of-Order Microarchitectures. We extensively discussed the Load Slice Core <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> through"
        },
        {
            "pid": "53e999ffb7602d970224d68c",
            "content": "mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.8\">Comparison Against CESP</head><p>Palacharla et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose the complexity-effective superscalar processor (CE oint out the most closely related work.</p><p>Complexity-Effective Architectures. Palacharla et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose the complexity-effective superscalar processors (C"
        },
        {
            "pid": "53e9b089b7602d9703af3ed8",
            "content": "br\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also exploit critical instruction slices <ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": "tch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type=\"bibr\" target=\"#b13\">[14]</ref> evaluate CESP in the context of a realistic baseline and p"
        },
        {
            "pid": "53e9af0db7602d9703945fad",
            "content": "/ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also exploit critical instruction slices <ref type=\"bibr\" target=\"#b23\">[24]</ref> for improving performance. More recently, Clairvoyance <re"
        },
        {
            "pid": "5736982b6e3b12023e6fd299",
            "content": "nates all out-of-order structures and is therefore more area-and power-efficient. Long-term parking <ref type=\"bibr\" target=\"#b15\">[16]</ref> saves power in an OoO core by allocating back-end resource"
        },
        {
            "pid": null,
            "content": "tch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type=\"bibr\" target=\"#b13\">[14]</ref> evaluate CESP in the context of a realistic baseline and p"
        },
        {
            "pid": "53e9ba64b7602d970468183c",
            "content": "ecute phases of a program through coordinated queues. Proposals such as speculative-slice execution <ref type=\"bibr\" target=\"#b22\">[23]</ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target"
        },
        {
            "pid": "55323c6d45cec66b6f9dc0c5",
            "content": ">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> throughout the paper. Shioya et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> propose the front-end execution architecture which execute"
        },
        {
            "pid": "5c9df4643cb210d271bea0de",
            "content": "e (LSC) <ref type=\"bibr\" target=\"#b4\">[5]</ref> was the first work to propose an sOoO core; Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> builds upon the LSC proposal and exposes more MHP than LSC ueue), while all other instructions execute from the main, arithmetic queue (A-queue). Kumar et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> observe that, in LSC, an independent load may be stuck behi the background on the two prior sOoO cores -LSC <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> -and we elaborate on their shortcomings. Figure <ref type=\" d cannot issue to the memory hierarchy, hindering the opportunity to expose MHP.</p><p>Kumar et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose Freeway, a core microarchitecture that overcomes LS \" target=\"#tab_2\">2</ref>. We evaluate LSC and Freeway following the configurations by Kumar et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>. The size of the A and B queues in LSC is 16-entries each. s. We extensively discussed the Load Slice Core <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> throughout the paper. Shioya et al. <ref type=\"bibr\" target"
        },
        {
            "pid": "53e9b998b7602d97045880d6",
            "content": "ssociative cache with 2/2 read/write ports. We estimate power consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> a div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.7\">Power Consumption</head><p>We use McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> to calculate InO and OoO core power consumption. power con"
        },
        {
            "pid": "53e9a6d0b7602d97030065a6",
            "content": "</ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also"
        },
        {
            "pid": "53e9ab7eb7602d9703526dce",
            "content": "ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor"
        },
        {
            "pid": null,
            "content": "ower consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> assuming a 22 nm technology node. Area and per-access powe er and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref> to estimate chip area. CACTI accounts for the area of circ power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Table <ref type=\"table\" target=\"#tab_3\">3</ref> reports p"
        },
        {
            "pid": "53e9b998b7602d97045880d6",
            "content": "ssociative cache with 2/2 read/write ports. We estimate power consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> a div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.7\">Power Consumption</head><p>We use McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> to calculate InO and OoO core power consumption. power con"
        },
        {
            "pid": null,
            "content": "ower consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> assuming a 22 nm technology node. Area and per-access powe er and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref> to estimate chip area. CACTI accounts for the area of circ power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Table <ref type=\"table\" target=\"#tab_3\">3</ref> reports p"
        },
        {
            "pid": "53e9b089b7602d9703af3ed8",
            "content": "br\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also exploit critical instruction slices <ref type=\"bibr\" ta"
        },
        {
            "pid": "5550428045ce0a409eb42cc1",
            "content": "evaluate FSC using the most detailed, cycle-level, and hardwarevalidated core model in Sniper v6.0 <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The configurations for the InO, FSC and OoO cores are provi"
        },
        {
            "pid": "5550428045ce0a409eb42cc1",
            "content": "evaluate FSC using the most detailed, cycle-level, and hardwarevalidated core model in Sniper v6.0 <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The configurations for the InO, FSC and OoO cores are provi"
        },
        {
            "pid": "53e9af0db7602d9703945fad",
            "content": "/ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also exploit critical instruction slices <ref type=\"bibr\" target=\"#b23\">[24]</ref> for improving performance. More recently, Clairvoyance <re"
        },
        {
            "pid": "53e9bc97b7602d9704918095",
            "content": "lative-slice execution <ref type=\"bibr\" target=\"#b22\">[23]</ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref>"
        },
        {
            "pid": null,
            "content": "ower consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> assuming a 22 nm technology node. Area and per-access powe er and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref> to estimate chip area. CACTI accounts for the area of circ power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Table <ref type=\"table\" target=\"#tab_3\">3</ref> reports p"
        }
    ],
    "5f53599a91e0110c40a7bc91": [
        {
            "pid": "634a168690e50fcafd663551",
            "content": "mising application area for deep learning <ref type=\"bibr\" target=\"#b11\">(Graves et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Pereira et aphQA <ref type=\"bibr\" target=\"#b1\">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref type=\"bibr\" target=\"#b16\">(Ingraham et al., 2019)</ref> on CPD, and ProteinSolver <ref type=\"bi bstantial improvement both in terms of perplexity and sequence recovery over Structured Transformer <ref type=\"bibr\" target=\"#b16\">(Ingraham et al., 2019)</ref>, a GNN method which was trained using t ve model over the space of protein sequences conditioned on the given backbone structure. Following <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref>, we frame this as an autoregressive task the same training and validation sets (Table <ref type=\"table\" target=\"#tab_3\">3</ref>). Following <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref>, we report evaluation on short (100 or f >Protein design As described in the main text, we use the CATH 4.2 dataset and splits as curated by <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref> and the TS50 test set as curated by <ref et should bear minimal similarity to the training structures. We use the CATH 4.2 dataset curated by<ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref> in which all available structures with 4"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "target=\"#b16\">Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Pereira et al., 2016;</ref><ref type=\"bibr\" target=\"#b29\">Townshend et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Won et al"
        },
        {
            "pid": "57a4e921ac44365e35c98d6a",
            "content": ""
        },
        {
            "pid": "5f3512499fced0a24b11c515",
            "content": "d as a connectivity pattern rather than a rigid shape. Recent state-of-the-art GNNs include GraphQA <ref type=\"bibr\" target=\"#b1\">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref t </ref> and Ornate <ref type=\"bibr\" target=\"#b23\">(Pag\u00e8s et al., 2019)</ref>, the GNN method GraphQA <ref type=\"bibr\" target=\"#b1\">(Baldassarre et al., 2020)</ref>, and three methods that use sequentia T1009, T1011, and T1016. This information is obtained from the CASP download center as described by <ref type=\"bibr\" target=\"#b1\">Baldassarre et al. (2020)</ref>. The exact numbers of targets and stru comparison for CASP 13. All predictions were obtained from the CASP download center as described by <ref type=\"bibr\" target=\"#b1\">Baldassarre et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "f> and Ornate <ref type=\"bibr\" target=\"#b23\">(Pag\u00e8s et al., 2019)</ref> and a number of CPD methods <ref type=\"bibr\" target=\"#b0\">(Anand et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Zhang et al.,"
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b0\">(Anand et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Shroff et al., 2019)</ref> exemplify the power of this approach.</p><"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "=\"bibr\" target=\"#b6\">(Cohen &amp; Shashua, 2017)</ref> and attention in natural language processing <ref type=\"bibr\" target=\"#b31\">(Vaswani et al., 2017)</ref>. What are the relevant considerations in unctions. <ref type=\"foot\" target=\"#foot_3\">4</ref>\u2022 A sinusoidal encoding of i \u2212 j as described in <ref type=\"bibr\" target=\"#b31\">Vaswani et al. (2017)</ref>, representing distance along the backbone"
        },
        {
            "pid": null,
            "content": "9)</ref>, physicsinspired energy terms <ref type=\"bibr\" target=\"#b21\">(O'Connell et al., 2018;</ref><ref type=\"bibr\" target=\"#b30\">Uziela et al., 2017)</ref>, or context-free grammars of protein topol lovas, 2017)</ref>, SBROD <ref type=\"bibr\" target=\"#b17\">(Karasikov et al., 2019)</ref>, and ProQ3D <ref type=\"bibr\" target=\"#b30\">(Uziela et al., 2017)</ref>. All of these methods learn solely from p ude ProQ3D because it is an improved version of the best single-model method in CASP 11 and CASP 12 <ref type=\"bibr\" target=\"#b30\">(Uziela et al., 2017)</ref>. GVP-GNN outperforms all other structural heng et al., 2019)</ref>. FaeNNz 7<ref type=\"bibr\" target=\"#b28\">(Studer et al., 2020)</ref>, ProQ3D<ref type=\"bibr\" target=\"#b30\">(Uziela et al., 2017)</ref>, and VoroMQA</figDesc><table /><note>8<re"
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b0\">(Anand et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Shroff et al., 2019)</ref> exemplify the power of this approach.</p><"
        }
    ],
    "5fef1dfc91e0113b265a0220": [
        {
            "pid": "5db1765a3a55ac101c887e97",
            "content": "ve to automatically generate prompts given the few-shot training data using the generative T5 model <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>. This allows us to cheaply obtain effecti lly from a fixed set of label words M(Y). To address this challenging problem, we propose to use T5 <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>, a large pre-trained text-to-text Transfo e <ref type=\"table\">B</ref>.1.</p><p>For automatic template search with T5, we take the T5-3B model <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>, which is the largest publicly available"
        },
        {
            "pid": null,
            "content": "r\" target=\"#b9\">(Dagan et al., 2005;</ref><ref type=\"bibr\" target=\"#b3\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b15\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b5\">Bentivog"
        },
        {
            "pid": "5f980b1f91e0112e0cda7ee9",
            "content": "pts is a significant challenge. <ref type=\"bibr\" target=\"#b34\">Schick and Sch\u00fctze (2020a)</ref> and <ref type=\"bibr\" target=\"#b33\">Schick et al. (2020)</ref> explore ways of identifying label words au This approach is similar to the automatic verbalizer search methods in Schick and Sch\u00fctze (2020a); <ref type=\"bibr\" target=\"#b33\">Schick et al. (2020)</ref>, except that we use a much simpler search"
        },
        {
            "pid": "56d81308dabfae2eee5eff4b",
            "content": "f> and datasets from GLUE <ref type=\"bibr\" target=\"#b41\">(Wang et al., 2019)</ref>, including SST-2 <ref type=\"bibr\" target=\"#b36\">(Socher et al., 2013)</ref>, CoLA <ref type=\"bibr\" target=\"#b42\">(War imply randomly sample 2,000 examples as the testing set and leave them out from training. For SST-5 <ref type=\"bibr\" target=\"#b36\">(Socher et al., 2013)</ref> and TREC <ref type=\"bibr\" target=\"#b40\">("
        },
        {
            "pid": null,
            "content": "knowledge from pre-trained language models <ref type=\"bibr\" target=\"#b38\">(Trinh and Le, 2018;</ref><ref type=\"bibr\" target=\"#b26\">Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Davison et"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "get=\"#b28\">(Radford et al., 2018</ref><ref type=\"bibr\" target=\"#b29\">(Radford et al., , 2019;;</ref><ref type=\"bibr\" target=\"#b7\">Brown et al., 2020)</ref> for zero-shot prediction and recently studie get=\"#b28\">(Radford et al., 2018</ref><ref type=\"bibr\" target=\"#b29\">(Radford et al., , 2019;;</ref><ref type=\"bibr\" target=\"#b7\">Brown et al., 2020)</ref> fueled the development of prompt-based few-s"
        },
        {
            "pid": "573697556e3b12023e63e99f",
            "content": "lliams et al., 2018)</ref>, QNLI <ref type=\"bibr\" target=\"#b31\">(Rajpurkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b9\">(Dagan et al., 2005;</ref><ref type=\"bibr\" target=\"#b3\">Bar-Haim et al"
        },
        {
            "pid": null,
            "content": "P, including (1) semi-supervised learning <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">Xie et al., 2020)</ref>, where a set of unlabeled examples are given;"
        },
        {
            "pid": null,
            "content": "hile in inference we ensemble predictions across all sets. This is reminiscent of Matching Networks <ref type=\"bibr\" target=\"#b39\">(Vinyals et al., 2016)</ref>, and more specifically BERT-PAIR <ref ty"
        },
        {
            "pid": null,
            "content": "mple examples that are semantically close to x in . Specifically, we use a pretrained Sentence-BERT <ref type=\"bibr\" target=\"#b32\">(Reimers and Gurevych, 2019)</ref> model to obtain embeddings for all"
        }
    ],
    "5f8fffb591e01125c27ddec9": [
        {
            "pid": "5ce3acd5ced107d4c65ad719",
            "content": "urbations to the input node features with graph structures unchanged. FLAG leverages \"free\" methods <ref type=\"bibr\" target=\"#b27\">(Shafahi et al., 2019)</ref> to conduct efficient adversarial trainin de feature space.</p><p>Augmentation for \"free\". We leverage the \"free\" adversarial training method <ref type=\"bibr\" target=\"#b27\">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations."
        },
        {
            "pid": "5cede0efda562983788ce286",
            "content": ""
        },
        {
            "pid": "5b67b45517c44aac1c860884",
            "content": "ef>, meta-learning <ref type=\"bibr\" target=\"#b10\">(Garcia &amp; Bruna, 2017)</ref>, social analysis <ref type=\"bibr\" target=\"#b25\">(Qiu et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Li &amp; Goldw"
        },
        {
            "pid": "5d9ed28247c8f76646f6e046",
            "content": ""
        },
        {
            "pid": "5cede0fada562983788d97ed",
            "content": "n. To clarify, FLAG is intrinsically different from the previous graph adversarial training methods <ref type=\"bibr\" target=\"#b8\">(Feng et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Deng et al., 20 \" target=\"#b5\">Deng et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Jin &amp; Zhang, 2019)</ref>. <ref type=\"bibr\" target=\"#b8\">Feng et al. (2019)</ref> proposed to reinforce local smoothness to mak"
        },
        {
            "pid": "5cede0efda562983788ce286",
            "content": ""
        },
        {
            "pid": null,
            "content": ", 2020)</ref>. Despite the rich literature about adversarial training of GNNs for security purposes <ref type=\"bibr\" target=\"#b44\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Dai et al.,"
        },
        {
            "pid": "5b67b45517c44aac1c860884",
            "content": "ef>, meta-learning <ref type=\"bibr\" target=\"#b10\">(Garcia &amp; Bruna, 2017)</ref>, social analysis <ref type=\"bibr\" target=\"#b25\">(Qiu et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Li &amp; Goldw"
        },
        {
            "pid": "5cede0fada562983788d97ed",
            "content": "n. To clarify, FLAG is intrinsically different from the previous graph adversarial training methods <ref type=\"bibr\" target=\"#b8\">(Feng et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Deng et al., 20 \" target=\"#b5\">Deng et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Jin &amp; Zhang, 2019)</ref>. <ref type=\"bibr\" target=\"#b8\">Feng et al. (2019)</ref> proposed to reinforce local smoothness to mak"
        },
        {
            "pid": null,
            "content": "16</ref>) and its variants have been applied to a wide range of tasks, including visual recognition <ref type=\"bibr\" target=\"#b42\">(Zhao et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Shen et al.,"
        },
        {
            "pid": null,
            "content": "16</ref>) and its variants have been applied to a wide range of tasks, including visual recognition <ref type=\"bibr\" target=\"#b42\">(Zhao et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Shen et al.,"
        }
    ],
    "5fd0a7f691e01147f1d1e367": [
        {
            "pid": "5f8eab549e795e9e76f6f69e",
            "content": "oding. They have been utilized to perform multi-purpose text generation with a single unified model <ref type=\"bibr\" target=\"#b36\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al bility present in large pretrained models <ref type=\"bibr\" target=\"#b28\">(McCann et al., 2018;</ref><ref type=\"bibr\" target=\"#b36\">Radford et al., 2019;</ref><ref type=\"bibr\">Keskar et al., 2019;</ref ng with the vanilla BART model, we also include the zero-shot performance from GPT2 language models <ref type=\"bibr\" target=\"#b36\">(Radford et al., 2019)</ref> (without fine-tuning) as a reference poi"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "et al., 2018)</ref>, and (2) transformer seq2seq with the same hyperparameters as the base model in <ref type=\"bibr\" target=\"#b46\">(Vaswani et al., 2017)</ref>. Note that the transformer model is trai"
        },
        {
            "pid": "5a260c8617c44a4ba8a31cdb",
            "content": "s we will show in this paper. In contrast, prior work primarily rely on pre-defined \"control codes\" <ref type=\"bibr\" target=\"#b6\">(Fan et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Liu et al., 201 ntity or length as supervision to train the model conditioned on both the code and article together <ref type=\"bibr\" target=\"#b6\">(Fan et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Liu et al., 201 100 documents and repeatedly acquire every entity in the document to generate summaries, following <ref type=\"bibr\" target=\"#b6\">Fan et al. (2018)</ref>. Then we compute Success Rate, the fraction of _2\">3</ref> shows the Success Rate and factual correctness evaluations. We include the numbers from <ref type=\"bibr\" target=\"#b6\">Fan et al. (2018)</ref> (EntityCode) for reference point. We note that additional information to help generate the reference summary. We also run the LengthCode baseline <ref type=\"bibr\" target=\"#b6\">(Fan et al., 2018)</ref> based on BART, where the ground-truth length that CTRLsum achieves a very high success rate (\u223c 95%) of entity control, compared to previous work <ref type=\"bibr\" target=\"#b6\">(Fan et al., 2018)</ref> which can only succeed 61.2% and 33.8% of the 2seq <ref type=\"bibr\" target=\"#b8\">(Gehring et al., 2017)</ref> with the same hyperparameters as in <ref type=\"bibr\" target=\"#b6\">(Fan et al., 2018)</ref>, and (2) transformer seq2seq with the same hy BART numbers are in terms of unconstrained generated summaries. EntityCode numbers are directly from<ref type=\"bibr\" target=\"#b6\">(Fan et al., 2018)</ref>, which is obtained with a weaker convolutiona"
        },
        {
            "pid": null,
            "content": "4\">(Tang et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Huang et al., 2019)</ref>, and templates <ref type=\"bibr\" target=\"#b12\">(Guu et al., 2018;</ref><ref type=\"bibr\" target=\"#b49\">Wiseman et al."
        },
        {
            "pid": null,
            "content": "4\">(Tang et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Huang et al., 2019)</ref>, and templates <ref type=\"bibr\" target=\"#b12\">(Guu et al., 2018;</ref><ref type=\"bibr\" target=\"#b49\">Wiseman et al."
        },
        {
            "pid": null,
            "content": "rget=\"#b13\">He et al., 2020b)</ref>, topics <ref type=\"bibr\" target=\"#b44\">(Tang et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Huang et al., 2019)</ref>, and templates <ref type=\"bibr\" target=\"#b1"
        },
        {
            "pid": "58d82fced649053542fd6da7",
            "content": "acts important portions of a document <ref type=\"bibr\" target=\"#b2\">(Cheng &amp; Lapata, 2016;</ref><ref type=\"bibr\" target=\"#b31\">Nallapati et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Narayan e"
        },
        {
            "pid": null,
            "content": "g details can be found in Appendix A.1.</p><p>For evaluation, we measure commonly used ROUGE scores <ref type=\"bibr\" target=\"#b26\">(Lin, 2004)</ref> and the recently proposed BERTScore <ref type=\"bibr"
        },
        {
            "pid": "5736960b6e3b12023e51e1f2",
            "content": ""
        },
        {
            "pid": "5736960b6e3b12023e51e1f2",
            "content": ""
        },
        {
            "pid": null,
            "content": "ing was used in machine translation <ref type=\"bibr\" target=\"#b21\">(Knowles &amp; Koehn, 2016;</ref><ref type=\"bibr\" target=\"#b51\">Wuebker et al., 2016)</ref> and also to demonstrate the multi-task ab"
        }
    ],
    "5f0423a69e795e06bbe12b1e": [
        {
            "pid": "5f03f3b611dc830562231fce",
            "content": "supervised tasks, e.g., discriminating whether two subsequences come from the same user's behaviors <ref type=\"bibr\" target=\"#b35\">[36]</ref>. We further improve upon CLRec and propose Multi-CLRec, wh the regular task where \ud835\udc65 is a sequence of clicks and \ud835\udc66 is the next click to be predicted. Task u2u <ref type=\"bibr\" target=\"#b35\">[36]</ref> adds an auxiliary loss where \ud835\udc65 and \ud835\udc66 are both sequences fr /div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B.4 Complex Pretext Tasks</head><p>In task u2u <ref type=\"bibr\" target=\"#b35\">[36]</ref>, \ud835\udc65 and \ud835\udc66 are both sequences from the same user, before and"
        },
        {
            "pid": "60b9a253e4510cd7c8f78b46",
            "content": "get=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\">63</ref>]. However, these existing works mo"
        },
        {
            "pid": "599c7ef7601a182cd28dd2af",
            "content": ">27]</ref>, Taobao <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b60\">61]</ref>, and Pinterest <ref"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf3190e",
            "content": "eration. Deep candidate generation methods are widely deployed in industrial systems, e.g., YouTube <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar and evaluating recommender systems has been explored before <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targ rent from the existing literature on debiasing a recommender <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "rness <ref type=\"bibr\" target=\"#b7\">[8]</ref>, i.e. fairness towards the under-recommended products <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>Contrastive Learning. Contrastive learning, which aim"
        },
        {
            "pid": "53e9a645b7602d9702f7362e",
            "content": "rget=\"#b25\">26]</ref> usually outperforms the binary-cross-entropy based approximations such as NCE <ref type=\"bibr\" target=\"#b17\">[18]</ref> and negative sampling <ref type=\"bibr\" target=\"#b37\">[38]< with each positive example. Sampled softmax in general outperforms other approximations such as NCE <ref type=\"bibr\" target=\"#b17\">[18]</ref> and negative sampling <ref type=\"bibr\" target=\"#b37\">[38]<"
        },
        {
            "pid": "5b1643ba8fbcbf6e5a9bc53b",
            "content": "topic of reducing the bias in training and evaluating recommender systems has been explored before <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe s. We note that our fomulation is different from the existing literature on debiasing a recommender <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": "5a9cb60d17c44a376ffb3c6d",
            "content": "b30\">[31]</ref> 0.7155 0.8371 GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]</ref> 0.7760 0.8471 Caser <ref type=\"bibr\" target=\"#b46\">[47]</ref> 0.7582 0.8745 DIEN <ref type=\"bibr\" target=\"#b59\">[60]</re"
        },
        {
            "pid": null,
            "content": "rness <ref type=\"bibr\" target=\"#b7\">[8]</ref>, i.e. fairness towards the under-recommended products <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>Contrastive Learning. Contrastive learning, which aim"
        },
        {
            "pid": "5bbacbad17c44aecc4eb00ee",
            "content": "ref> achieves great success in various NLP tasks and is adopted by sequential recommendation models <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>. We simplify the m \"><head>Table 8 :</head><label>8</label><figDesc>Results on public benchmarks preprocessed by SASRec<ref type=\"bibr\" target=\"#b28\">[29]</ref> and BERT4Rec<ref type=\"bibr\" target=\"#b45\">[46]</ref>. We"
        },
        {
            "pid": "57a4e91dac44365e35c98c19",
            "content": "type=\"bibr\" target=\"#b25\">26]</ref>, among which the following variant is integrated by TensorFlow <ref type=\"bibr\" target=\"#b0\">[1]</ref> and commonly used:</p><formula xml:id=\"formula_2\">arg min \ud835\udf03 max <ref type=\"bibr\" target=\"#b25\">[26]</ref>. We use the sampled softmax implemented in Tensorflow <ref type=\"bibr\" target=\"#b0\">[1]</ref>, which subtracts the correction term to make the sampled los"
        }
    ],
    "5f7fdd328de39f0828397e22": [
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "hborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, in the re ifferent neighbors more precisely as a weighted average of the ego-and neighbor-features. GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> generalizes the aggregation beyond averaging, and models t -and the aggregated neighbor-embeddings without 'mixing' them is with concatenation as in GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>-rather than averaging all of them as in the GCN model by K ef type=\"bibr\" target=\"#b35\">[36]</ref> GCN-Cheby <ref type=\"bibr\" target=\"#b6\">[7]</ref> GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> H2GCN (prop ding transformations per round in H 2 GCN? GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> and other GNN models embed the intermediate representation &amp; GCN-Cheby <ref type=\"bibr\" target=\"#b16\">[17]</ref>: https://github.com/tkipf/gcn \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch : 3 * early_stopping: 40 We report the best performance, for Set 1 with a = 64, b = 5e-4.\u2022 GraphSAGE<ref type=\"bibr\" target=\"#b10\">[11]</ref>:-hid_units: a \u2208 {64, 128} lr: b \u2208 {0.1, 0.7} epochs: 500</ led the default feature normalization in the official implementation for this baseline. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>:</p><p>-hid_units: a \u2208 {64, 128} lr: b \u2208 {0.1, 0.7} epochs led the default feature normalization in the official implementation for this baseline. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>\u2022 M intermediate representations. While these designs have been utilized separately in some prior works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" targ 1)d)</formula><p>Solving the above inequality for \u03b4 1 , we get the amount of perturbation needed as <ref type=\"bibr\" target=\"#b10\">(11)</ref> and the least absolute amount of perturbation needed is |\u03b4"
        },
        {
            "pid": "5b67b47917c44aac1c8637c6",
            "content": "target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we are the first to discuss their importance under heterop COMBINE functions that leverage each representation separately-e.g., concatenation, LSTM-attention <ref type=\"bibr\" target=\"#b37\">[38]</ref>. This design is introduced in jumping knowledge networks < <ref type=\"bibr\" target=\"#b37\">[38]</ref>. This design is introduced in jumping knowledge networks <ref type=\"bibr\" target=\"#b37\">[38]</ref> and shown to increase the representation power of GCNs und ons from two rounds with the embedded ego-representation (following the jumping knowledge framework <ref type=\"bibr\" target=\"#b37\">[38]</ref>), GCN's accuracy increases to 58.93%\u00b13.17 for h = 0.1, a 2 label>(7)</label></formula><p>where we empirically find concatenation works better than max-pooling <ref type=\"bibr\" target=\"#b37\">[38]</ref> as the COMBINE function.</p><p>In the classification stage e compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections <ref type=\"bibr\" target=\"#b37\">[38]</ref>. GCN and GCN-Cheby benefit significantly from D3 in hetero with and without JK connections is similar (gaps mostly less than 2%), matching the observations in <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p><p>While other design choices and implementation detai K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=\"bibr\" target=\"#b37\">[38]</ref> without changing the number of layers or other hyperparame K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=\"bibr\" target=\"#b37\">[38]</ref> without changing the number of layers or other hyperparame"
        },
        {
            "pid": "5e5e18ba93d709897ce2b48e",
            "content": "cy matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank). Geom-GCN <ref type=\"bibr\" target=\"#b25\">[26]</ref> precomputes unsupervised node embeddings and uses neighbor 0.7), and across the full spectrum (\"Overall\"). The \"*\" denotes ranks based on results reported in <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Real datasets &amp; setup We now evaluate the performance of nodes per class for train/validation/test<ref type=\"foot\" target=\"#foot_2\">2</ref> ) provided by <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>For Cora-Full, we generate 3 random splits, with 25 the best results among the three recentlyproposed GEOM-GCN variants ( \u00a7 4), directly from the paper <ref type=\"bibr\" target=\"#b25\">[26]</ref>: other models (including ours) outperform this method sign ng universities, originally collected by the CMU WebKB project. We used the preprocessed version in <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In these networks, nodes are web pages, which are classif br\" target=\"#b28\">[29]</ref>. For the classification task, we utilize the class labels generated by <ref type=\"bibr\" target=\"#b25\">[26]</ref>, where the nodes are categorized into 5 classes based on t erage traffic. \u2022 Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by <ref type=\"bibr\" target=\"#b25\">[26]</ref> based on the film-director-actor-writer network in <ref ty ter network in <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We also use the class labels generated by <ref type=\"bibr\" target=\"#b25\">[26]</ref>. \u2022 Cora, Pubmed and Citeseer are citation graphs originall fferent data splits. Best model per benchmark highlighted in gray. The \"*\" results are obtained from<ref type=\"bibr\" target=\"#b25\">[26]</ref> and \"N/A\" denotes non-reported results.</figDesc><table><r ix of A + I.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_2\"><ref type=\"bibr\" target=\"#b25\">[26]</ref> claims that the ratios are 60%/20%/20%, which is different atent space to define graph convolution. Some of these works <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> acknowledge the cha"
        },
        {
            "pid": "53e9a57db7602d9702ea83f0",
            "content": "ph-based semi-supervised learning, which can be used for graphs exhibiting homophily or heterophily <ref type=\"bibr\" target=\"#b18\">[19]</ref> and has fast linearized versions <ref type=\"bibr\" target=\""
        },
        {
            "pid": "53e9983db7602d970206633b",
            "content": "Y t as before, with the modification that in this case we have for signal Y s (similarly for Y t ): <ref type=\"bibr\" target=\"#b19\">(20)</ref>. The rest of the proof is similar to Proof 3.</p><formula imate inference algorithms (e.g., iterative classification <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. B"
        },
        {
            "pid": null,
            "content": "aining a model to predict whether pairs of nodes share the same label; Graph Markov Neural Networks <ref type=\"bibr\" target=\"#b26\">[27]</ref> model the joint label distribution with a conditional rand"
        },
        {
            "pid": "5f03f3b611dc830562232042",
            "content": "al random field, trained with expectation maximization using GNNs; Correlated Graph Neural Networks <ref type=\"bibr\" target=\"#b14\">[15]</ref> model the correlation structure in the residuals of a regr"
        },
        {
            "pid": null,
            "content": "dominant and thus provide more relevant context. This observation is also confirmed by recent works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> in the context of bina"
        },
        {
            "pid": null,
            "content": "oposed architectures <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Recent work has investigated GNN's ability to capture grap"
        },
        {
            "pid": "58d82fced649053542fd6bca",
            "content": "ked into formulations which can better address heterophily: Before applying label propagation, Peel <ref type=\"bibr\" target=\"#b24\">[25]</ref> transforms the original graph into either a similarity gra"
        },
        {
            "pid": "573695fe6e3b12023e511794",
            "content": "om the corresponding class in a real benchmark (e.g., Cora <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or ogbn-products <ref type=\"bibr\" target=\"#b12\">[13]</ref>)"
        },
        {
            "pid": "58437722ac44360f1082efeb",
            "content": "ggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta borhood is N 1 (v)-i.e., the 1-hop neighbors of v. As for f , in graph convolutional networks (GCN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> each node repeatedly averages its own features and those o ssification (differences in accuracy of MLP for different h are due to randomness). Especially, GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref> and GAT <ref type=\"bibr\" target=\"#b35\">[36]</ref> show up target=\"#b10\">[11]</ref>-rather than averaging all of them as in the GCN model by Kipf and Welling <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>Intuition. In heterophily settings, by definition ( (v)) may be different. However, the typical GCN design that mixes the embeddings through an average <ref type=\"bibr\" target=\"#b16\">[17]</ref> or weighted average <ref type=\"bibr\" target=\"#b35\">[36]</r ility matrix notion from belief propagation <ref type=\"bibr\" target=\"#b9\">[10]</ref> into GNNs. GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref> GAT <ref type=\"bibr\" target=\"#b35\">[36]</ref> GCN-Cheby <r ditional conceptual and mechanism differences.</p><p>As we have mentioned, H 2 GCN differs from GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref> in a number of ways: <ref type=\"bibr\" target=\"#b0\">(1)</re t are critical in heterophily.</p><p>Non-linear embedding transformations per round in H 2 GCN? GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> and o c.org/ns/1.0\"><head>F Experimental Setup &amp; Hyperparameter Tuning</head><p>\u2022 GCN &amp; GCN-Cheby <ref type=\"bibr\" target=\"#b16\">[17]</ref>: https://github.com/tkipf/gcn \u2022 GraphSAGE <ref type=\"bibr\" Function \u03c3: ReLU -Dropout Rate: a \u2208 {0, 0.5}</p><p>We report the best performance, for a = 0. \u2022 GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>:</p><p>-hidden1: a \u2208 {16, 32, 64} -early_stopping: b \u2208 {40 \u2208 {40, 100, 200} epochs: 2000</p><p>We report the best performance, for a = 32, b = 40. \u2022 GCN-Cheby <ref type=\"bibr\" target=\"#b16\">[17]</ref>:</p><p>-Set 1:</p><p>-hidden1: 64 -early_stopping: {40, 10 method hurts the performance significantly. We report the best performance, for a = 40. \u2022 GCN-Cheby <ref type=\"bibr\" target=\"#b16\">[17]</ref>:</p><p>-hidden1: 64 -max_degree: 2 -early_stopping: 40 epo unction \u03c3: {ReLU, None} -Dropout Rate: {0, 0.5} -L2 Regularization Weight: {1e-5, 1e-6}</p><p>\u2022 GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>: </p></div>\t\t\t</div> \t\t\t<div type=\"references\">  \t\t\t\t<list"
        },
        {
            "pid": "5db929b747c8f766461fa94f",
            "content": "apart from MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> (cf. \u00a7 3.1), Graph Diffusion Convolution <ref type=\"bibr\" target=\"#b17\">[18]</ref> replaces the adjacency matrix with a sparsified version of"
        },
        {
            "pid": "573695fe6e3b12023e511794",
            "content": "om the corresponding class in a real benchmark (e.g., Cora <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or ogbn-products <ref type=\"bibr\" target=\"#b12\">[13]</ref>)"
        },
        {
            "pid": "53e997ddb7602d9701fd7250",
            "content": "rs are more likely to connect to accomplices than to other fraudsters in online purchasing networks <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Since many existing GNNs assume strong homophily, they fa tendency of connection between each pair of classes. For instance, in an online purchasing network <ref type=\"bibr\" target=\"#b23\">[24]</ref> with three classes-fraudsters, accomplices, and honest use"
        },
        {
            "pid": "5db929b747c8f766461fa94f",
            "content": "apart from MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> (cf. \u00a7 3.1), Graph Diffusion Convolution <ref type=\"bibr\" target=\"#b17\">[18]</ref> replaces the adjacency matrix with a sparsified version of"
        },
        {
            "pid": "5db929b747c8f766461fa94f",
            "content": "apart from MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> (cf. \u00a7 3.1), Graph Diffusion Convolution <ref type=\"bibr\" target=\"#b17\">[18]</ref> replaces the adjacency matrix with a sparsified version of"
        },
        {
            "pid": "573695fe6e3b12023e511794",
            "content": "om the corresponding class in a real benchmark (e.g., Cora <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or ogbn-products <ref type=\"bibr\" target=\"#b12\">[13]</ref>)"
        },
        {
            "pid": "5f03f3b611dc830562232042",
            "content": "al random field, trained with expectation maximization using GNNs; Correlated Graph Neural Networks <ref type=\"bibr\" target=\"#b14\">[15]</ref> model the correlation structure in the residuals of a regr"
        },
        {
            "pid": null,
            "content": "/ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type=\"bibr\" target=\"#b2\">[3]</ref>: The number of class labels |Y| in the synthetic graph is pr"
        },
        {
            "pid": "5d9edc8347c8f76646042a37",
            "content": "filtering-, the higher order polynomials of the normalized adjacency matrix A is a low-pass filter <ref type=\"bibr\" target=\"#b36\">[37]</ref>, so intermediate outputs from earlier rounds contain highe eighbor-embeddings. We found that removing the usual nonlinear transformations per round, as in SGC <ref type=\"bibr\" target=\"#b36\">[37]</ref>, works better (App. D.2), in which case we only need to in a learnable matrix. Our design in Eq. 5 aggregates different neighborhoods in a similar way to SGC <ref type=\"bibr\" target=\"#b36\">[37]</ref>, which has shown that removing non-linearities does not ne"
        },
        {
            "pid": null,
            "content": "aining a model to predict whether pairs of nodes share the same label; Graph Markov Neural Networks <ref type=\"bibr\" target=\"#b26\">[27]</ref> model the joint label distribution with a conditional rand"
        },
        {
            "pid": null,
            "content": "/ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type=\"bibr\" target=\"#b2\">[3]</ref>: The number of class labels |Y| in the synthetic graph is pr"
        },
        {
            "pid": null,
            "content": "/ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type=\"bibr\" target=\"#b2\">[3]</ref>: The number of class labels |Y| in the synthetic graph is pr"
        },
        {
            "pid": "573695fe6e3b12023e511794",
            "content": "om the corresponding class in a real benchmark (e.g., Cora <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or ogbn-products <ref type=\"bibr\" target=\"#b12\">[13]</ref>)"
        },
        {
            "pid": null,
            "content": "/ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type=\"bibr\" target=\"#b2\">[3]</ref>: The number of class labels |Y| in the synthetic graph is pr"
        },
        {
            "pid": "53e9ac18b7602d97035d9131",
            "content": "eal-world datasets <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar of node classification by leveraging the correlations between the node labels and their attributes <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Since exact inference is NP-hard, approximate inference a d by sampling feature vectors of nodes from the corresponding class in a real benchmark (e.g., Cora <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or ogbn-products < target=\"#b25\">[26]</ref>. \u2022 Cora, Pubmed and Citeseer are citation graphs originally introduced in <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, which are among t"
        },
        {
            "pid": "53e9a57db7602d9702ea83f0",
            "content": "ph-based semi-supervised learning, which can be used for graphs exhibiting homophily or heterophily <ref type=\"bibr\" target=\"#b18\">[19]</ref> and has fast linearized versions <ref type=\"bibr\" target=\""
        },
        {
            "pid": null,
            "content": "aining a model to predict whether pairs of nodes share the same label; Graph Markov Neural Networks <ref type=\"bibr\" target=\"#b26\">[27]</ref> model the joint label distribution with a conditional rand"
        },
        {
            "pid": "53e9983db7602d970206633b",
            "content": "Y t as before, with the modification that in this case we have for signal Y s (similarly for Y t ): <ref type=\"bibr\" target=\"#b19\">(20)</ref>. The rest of the proof is similar to Proof 3.</p><formula imate inference algorithms (e.g., iterative classification <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. B"
        },
        {
            "pid": "53e9ac18b7602d97035d9131",
            "content": "eal-world datasets <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar of node classification by leveraging the correlations between the node labels and their attributes <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Since exact inference is NP-hard, approximate inference a d by sampling feature vectors of nodes from the corresponding class in a real benchmark (e.g., Cora <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or ogbn-products < target=\"#b25\">[26]</ref>. \u2022 Cora, Pubmed and Citeseer are citation graphs originally introduced in <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, which are among t"
        },
        {
            "pid": "58d82fced649053542fd6bca",
            "content": "ked into formulations which can better address heterophily: Before applying label propagation, Peel <ref type=\"bibr\" target=\"#b24\">[25]</ref> transforms the original graph into either a similarity gra"
        },
        {
            "pid": null,
            "content": "Since exact inference is NP-hard, approximate inference algorithms (e.g., iterative classification <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, loopy belief prop"
        }
    ],
    "5f7fdd328de39f0828397fae": [
        {
            "pid": "58437722ac44360f1082efeb",
            "content": "h each other. Current state-of-the-art models highly depend on Graph Convoluational Networks (GCNs) <ref type=\"bibr\" target=\"#b12\">[13]</ref> originated from the theory of Graph Fourier Transform (GFT tp://www.tei-c.org/ns/1.0\"><head>Spectral Graph Convolution</head><p>The Spectral Graph Convolution <ref type=\"bibr\" target=\"#b12\">[13]</ref> is composed of three steps.</p><p>(1) The multivariate tim"
        },
        {
            "pid": "5e807d589fced0a24b30b594",
            "content": "domain. In this paper, StemGNN is proposed to address these issues. We refer you to recent surveys <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5ce2d090ced107d4c6391fd1",
            "content": "r a collection of multiple time-series as a unified entity <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. TCN <ref type=\"bibr\" target=\"#b2\">[3]</ref> is a representat"
        },
        {
            "pid": null,
            "content": "\"bibr\" target=\"#b17\">18]</ref> and multivariate techniques <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar local dependence patterns among variables and discover long-term patterns of time series. DeepState <ref type=\"bibr\" target=\"#b20\">[21]</ref> marries state space models with deep recurrent neural netw f type=\"bibr\" target=\"#b13\">[14]</ref>, ST-GCN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, DeepState <ref type=\"bibr\" target=\"#b20\">[21]</ref>, TCN <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Graph Wavene -BEATS<ref type=\"bibr\" target=\"#b18\">[19]</ref> TCN<ref type=\"bibr\" target=\"#b2\">[3]</ref> DeepState<ref type=\"bibr\" target=\"#b20\">[21]</ref> GraphWaveNet<ref type=\"bibr\" target=\"#b28\">[29]</ref> Deel"
        },
        {
            "pid": "5550433b45ce0a409eb468a1",
            "content": "ng topic in machine learning, which can be divided into two major categories: univariate techniques <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5550417045ce0a409eb3b421",
            "content": "Long Short-Term Memory (LSTM) <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Gated Recurrent Units (GRU) <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Gated Linear Units (GLU) <ref type=\"bibr\" target=\"#b6\">[7]<"
        },
        {
            "pid": "59a02d92b161e8ad1a7b6d75",
            "content": "m (DFT) is also useful for time-series analysis. For instance, State Frequency Memory (SFM) network <ref type=\"bibr\" target=\"#b31\">[32]</ref> combines the advantages of DFT and LSTM jointly for stock target=\"#b29\">[30]</ref> forecasts univariate time-series with LSTM and fully-connected layers. SMF <ref type=\"bibr\" target=\"#b31\">[32]</ref> improves the LSTM model by breaking down the cell states o ith other state-of-the-art models, including FC-LSTM <ref type=\"bibr\" target=\"#b25\">[26]</ref>, SFM <ref type=\"bibr\" target=\"#b31\">[32]</ref>, N-BEATS <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DCRNN elations</cell><cell></cell></row></table><note>FC-LSTM<ref type=\"bibr\" target=\"#b25\">[26]</ref> SFM<ref type=\"bibr\" target=\"#b31\">[32]</ref> N-BEATS<ref type=\"bibr\" target=\"#b18\">[19]</ref> TCN<ref t get=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "ized by 0.001 and decayed with rate 0.7 after every 5 epochs. We use the Mean Absolute Errors (MAE) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Mean Absolute Percentage Errors (MAPE) <ref type=\"bibr\" t lute Errors (MAE) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Mean Absolute Percentage Errors (MAPE) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and Root Mean Squared Errors (RMSE) <ref type=\"bibr\" targ entage Errors (MAPE) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and Root Mean Squared Errors (RMSE) <ref type=\"bibr\" target=\"#b10\">[11]</ref> to measure the performances, which are averaged by H steps"
        },
        {
            "pid": "5b67b46417c44aac1c86142d",
            "content": "target=\"#b12\">[13]</ref> originated from the theory of Graph Fourier Transform (GFT). These models <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> stack GCN and temp et=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targe temporal dependencies in the convolutional recurrent neural network for traffic forecasting. ST-GCN <ref type=\"bibr\" target=\"#b30\">[31]</ref> is another deep learning framework for traffic prediction, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LSTNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>, ST-GCN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, DeepState <ref type=\"bibr\" target=\"#b20\">[21]</ref>, TCN of each graph convolution layer is set as 64 and the kernel size of 1D convolution is 3. Following <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we adopt the RMSprop optimizer, and the number of trainin"
        },
        {
            "pid": "5b67b46417c44aac1c86142d",
            "content": "target=\"#b12\">[13]</ref> originated from the theory of Graph Fourier Transform (GFT). These models <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> stack GCN and temp et=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targe temporal dependencies in the convolutional recurrent neural network for traffic forecasting. ST-GCN <ref type=\"bibr\" target=\"#b30\">[31]</ref> is another deep learning framework for traffic prediction, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LSTNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>, ST-GCN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, DeepState <ref type=\"bibr\" target=\"#b20\">[21]</ref>, TCN of each graph convolution layer is set as 64 and the kernel size of 1D convolution is 3. Following <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we adopt the RMSprop optimizer, and the number of trainin"
        },
        {
            "pid": null,
            "content": "datasets. Table <ref type=\"table\" target=\"#tab_3\">3</ref> summarizes the results obtained on PEMS07 <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and more results on other datasets can be found in Appendix"
        }
    ],
    "5f8eab4c91e01153024c4ba2": [
        {
            "pid": "5e3d3645df1a9c0c41ec6d65",
            "content": "ackground knowledge <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Although Neural Attention Models (NAM) <ref type=\"bibr\" target=\"#b1\">[2]</ref> are endowed with a certain degree of interpretability in vis 8]</ref>), (c) discover inherent bias in the model's predictive strategy (e.g., contextual modeling <ref type=\"bibr\" target=\"#b1\">[2]</ref>[9]), (d) prevent prediction errors in unintuitive scenarios e-infusion during the model learning using information-theoretic loss function (e.g., KL divergence <ref type=\"bibr\" target=\"#b1\">[2]</ref>) can check conceptual drifting at the representational level ge in DL models can be categorized into the shallow infusion, semi-deep infusion, and deep infusion <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In shallow infusion, both the external information and meth here sentences are purposefully paraphrased to elicit meaningful responses from an agent (or user); <ref type=\"bibr\" target=\"#b1\">(2)</ref> The clinical conversation contains implicit references to he"
        },
        {
            "pid": "5ff8596cd4150a363c4480ca",
            "content": "blem is a low resource (insufficient benchmark datasets and unlabeled corpus for transfer learning) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Also, knowledge-infusion during the model learning using"
        },
        {
            "pid": "5ff8596cd4150a363c4480ca",
            "content": "blem is a low resource (insufficient benchmark datasets and unlabeled corpus for transfer learning) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Also, knowledge-infusion during the model learning using"
        },
        {
            "pid": "5f0993869fced0a24b63d0db",
            "content": "r conceptual (using either generic or domain-specific KG <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>) in nature pertaining to its inner functioning.\"</p><p>Expla"
        },
        {
            "pid": "5d0b007b8607575390fc9bae",
            "content": ""
        },
        {
            "pid": null,
            "content": "s a human-understandable justification for the prediction <ref type=\"bibr\" target=\"#b12\">[13]</ref> <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Such systems are considered to be potentially useful for"
        },
        {
            "pid": "5ff8596cd4150a363c4480ca",
            "content": "blem is a low resource (insufficient benchmark datasets and unlabeled corpus for transfer learning) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Also, knowledge-infusion during the model learning using"
        },
        {
            "pid": "5d0b007b8607575390fc9bae",
            "content": ""
        },
        {
            "pid": "5f0993869fced0a24b63d0db",
            "content": "r conceptual (using either generic or domain-specific KG <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>) in nature pertaining to its inner functioning.\"</p><p>Expla"
        },
        {
            "pid": "5d8dded23a55acd1b5496944",
            "content": "hensible explanations around the decision-making process <ref type=\"bibr\" target=\"#b4\">[5]</ref>[6] <ref type=\"bibr\" target=\"#b6\">[7]</ref>. In contrast, interpretability is the ability to discern the"
        },
        {
            "pid": "5c5c55b4e1cd8e03e71686ab",
            "content": "1\">[2]</ref>[9]), (d) prevent prediction errors in unintuitive scenarios (e.g. adversarial examples <ref type=\"bibr\" target=\"#b9\">[10]</ref>, CHECKLIST <ref type=\"bibr\" target=\"#b3\">[4]</ref>), (e) ma rchangeably in the prior research without clear distinctions and the different roles that they play <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>I"
        }
    ],
    "5f576c1591e011f4c3d5dd7e": [
        {
            "pid": "573696ce6e3b12023e5ce95a",
            "content": "ifferent domains require specialized normalization methods. In computer vision, batch normalization <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a standard component. While in natural language process h set of feature values the normalization is applied to. For example, in computer vision, BatchNorm <ref type=\"bibr\" target=\"#b15\">[16]</ref> is the de facto method that normalizes the feature values ring testing, the estimated dataset-level statistics are used instead of the batch-level statistics <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>In GNNs, for each feature dimension, the BatchNorm -invariant\" property <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In comparison, BatchNorm in the lower branch suffers from e-invariant\" property<ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In comparison, BatchNorm in the lower branch suffers from ed to improve the training process in different applications <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar h of the parameters; <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> show that the norma rom BatchNorm layers under different settings of batch sizes <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b31\">32</ref>, 64], as in Figure < matrix in layer k. We apply the normalization after the linear transformation as in previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta hich try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "53e9ba23b7602d970462d60d",
            "content": "5\">6]</ref>, and similar ideas are also used to accelerate the optimization of deep neural networks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In the case of opti tch size. We visualize the statistics from BatchNorm layers under different settings of batch sizes <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "luding their representation power <ref type=\"bibr\" target=\"#b36\">[37]</ref>, generalization ability <ref type=\"bibr\" target=\"#b38\">[39]</ref>, and infinite-width asymptotic behavior <ref type=\"bibr\" t"
        },
        {
            "pid": "58d82fc8d649053542fd59aa",
            "content": "gregation strategy <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "nel model <ref type=\"bibr\" target=\"#b29\">[30]</ref>, diffusion-convolutional neural networks (DCNN) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Deep Graph CNN (DGCNN) <ref type=\"bibr\">[42]</ref> and Anon"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": "NN architecture variants, e.g., neighbor aggregation modules, that learn good graph representations <ref type=\"bibr\" target=\"#b36\">[37]</ref>. To that end, many theoretical aspects of GNNs have been s that end, many theoretical aspects of GNNs have been studied, including their representation power <ref type=\"bibr\" target=\"#b36\">[37]</ref>, generalization ability <ref type=\"bibr\" target=\"#b38\">[39 important problem remains: the optimization of GNNs is often unstable, and the convergence is slow <ref type=\"bibr\" target=\"#b36\">[37]</ref>. This raises the question:</p><p>Can we provably improve t tional Networks (GCN) <ref type=\"bibr\" target=\"#b18\">[19]</ref> and Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b36\">[37]</ref>. In GCN, the AGGREGATE function is defined as:</p><formula , as the aggregation function is similar to the convolutional operation, BatchNorm is usually used. <ref type=\"bibr\" target=\"#b36\">[37]</ref> uses BatchNorm in the GIN model, where the BatchNorm is ap es in a single graph. Then a natural question is whether using a batch-level normalization for GNNs <ref type=\"bibr\" target=\"#b36\">[37]</ref> can lead to similar advantages. In batch normalization (Ba ion holds for batch normalization.</p><p>To study this, we train a 5-layer GIN with BatchNorm as in <ref type=\"bibr\" target=\"#b36\">[37]</ref> on the PROTEINS dataset and train a ResNet18 <ref type=\"bi there are no available node features, then X i is set to be the one-hot encoding of the node degree <ref type=\"bibr\" target=\"#b36\">[37]</ref>. In a r-regular graph, all nodes have the same encoding, a arget=\"#tab_1\">1</ref>. We evaluate our proposed GraphNorm on two typical graph neural networks GIN <ref type=\"bibr\" target=\"#b36\">[37]</ref> and GCN <ref type=\"bibr\" target=\"#b18\">[19]</ref> and comp readout for MUTAG, PTC, PROTEINS and NCI1 datasets, and use MEAN readout for other datasets, as in <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Details of the experimental settings are presented in App mmarized in Table <ref type=\"table\" target=\"#tab_3\">2</ref>. Those information can be also found in <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Social netw d social network datasets, we use 5-layer GIN/GCN with a linear output head for prediction followed <ref type=\"bibr\" target=\"#b36\">[37]</ref> with residual connection. The hidden dimension of GIN/GCN ion of GIN/GCN is set to be 64. For the large-scale ogbgmolhiv dataset, we also use 5-layer GIN/GCN <ref type=\"bibr\" target=\"#b36\">[37]</ref> architecture with residual connection. Following <ref type \u2212 3, 5e \u2212 4, 5e \u2212 5} \u222a {0.0}, the learning rate \u2208 {1e \u2212 4, 1e \u2212 3, 1e \u2212 2}. We follow previous work <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b14\">[15]</ref> to select th <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We report the accuracies reported in the original paper <ref type=\"bibr\" target=\"#b36\">[37]</ref>. For the large-scale ogbg-molhiv dataset, we use the basel ref>, including the Graph-agnostic MLP model, GCN <ref type=\"bibr\" target=\"#b18\">[19]</ref> and GIN <ref type=\"bibr\" target=\"#b36\">[37]</ref>. We also report the roc-auc values reported in the origina ormance over different random seeds (or cross-validation). For the medium-scale datasets, following <ref type=\"bibr\" target=\"#b36\">[37]</ref>, we perform a 10-fold cross-validation as these datasets d sks (PROTEINS, PTC, NCI1, MUTAG, IMDB-BINARY datasets), we train a 5-layer GIN with BatchNorm as in <ref type=\"bibr\" target=\"#b36\">[37]</ref> and the number of sub-layers in MLP is set to 2. For image in previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. We can instantiate Eq. ( <ref type=\"formula\" target=\"#form ings</head><p>We use eight popularly used benchmark datasets of different scales in the experiments <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, including four me"
        },
        {
            "pid": "58d82fc8d649053542fd59aa",
            "content": "gregation strategy <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5b67b47917c44aac1c86392e",
            "content": "the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. During testing, the estimated dataset-level statistics are tings of batch sizes <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b31\">32</ref>, 64], as in Figure <ref type=\"figure\">9</ref>. We can see th"
        },
        {
            "pid": null,
            "content": "type=\"bibr\" target=\"#b20\">21]</ref> show that the normalization implicitly tunes the learning rate. <ref type=\"bibr\" target=\"#b27\">[28]</ref> reveals that normalization smooths the optimization landsc"
        },
        {
            "pid": "5bdc31b817c44a1f58a0c79b",
            "content": "for the mean and standard deviation over the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. During testing, th"
        },
        {
            "pid": null,
            "content": "nel model <ref type=\"bibr\" target=\"#b29\">[30]</ref>, diffusion-convolutional neural networks (DCNN) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Deep Graph CNN (DGCNN) <ref type=\"bibr\">[42]</ref> and Anon"
        }
    ],
    "5fdc8e9d91e01104c91811a8": [
        {
            "pid": "5e15adcb3a55ac47ab5b0b8c",
            "content": "on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. <ref type=\"bibr\" target=\"#b35\">(Yun et al. 2019;</ref><ref type=\"bibr\" target=\"#b33\">Xu, Joshi, and"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "ous success in the field of natural language processing (NLP) since the development of Transformers <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> which are currently the best performing n nal information. Since Laplacian PEs are generalization of the PE used in the original transformers <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> to graphs and these better help encode di r</head><p>The Graph Transformer is closely the same transformer architecture initially proposed in <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref>, see Figure <ref type=\"figure\">1</ref> (L target=\"#tab_2\">2</ref>), which employs multi-headed attention inspired by the original transformer <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> and have been often used in the literatur e each word attending to each other word in a sentence, as followed by the Transformer architecture <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017</ref>). -b) Next, the so-called graph considered"
        },
        {
            "pid": "53e9b360b7602d9703e41a15",
            "content": "<p>We evaluate the performance of proposed Graph Transformer on three benchmark graph datasets-ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a rec chmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012</ref>) is a molecular dataset with the task of gra"
        },
        {
            "pid": "599c797d601a182cd2643e8a",
            "content": "and have achieved significant success on a wide range of applications, such as in knowledge graphs <ref type=\"bibr\" target=\"#b25\">(Schlichtkrull et al. 2018;</ref><ref type=\"bibr\" target=\"#b5\">Chami"
        },
        {
            "pid": "599c797d601a182cd2643e8a",
            "content": "and have achieved significant success on a wide range of applications, such as in knowledge graphs <ref type=\"bibr\" target=\"#b25\">(Schlichtkrull et al. 2018;</ref><ref type=\"bibr\" target=\"#b5\">Chami"
        },
        {
            "pid": "5c8a11324895d9cbc6121c34",
            "content": "r\" target=\"#b13\">Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b17\">Monti et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Gilmer et al. 2017;</ref><ref type=\"bibr\" target=\"#b28\">Veli\u010dkovi\u0107 et"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": ""
        },
        {
            "pid": null,
            "content": "f type=\"bibr\" target=\"#b33\">Xu, Joshi, and Bresson 2019;</ref><ref type=\"bibr\">Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b37\">Zhou et al. 2020</ref>). The model proposed in <ref type=\"bibr\" targe based on the timestamp differences of the central node and the message-passing nodes. Furthermore, <ref type=\"bibr\" target=\"#b37\">Zhou et al. (2020)</ref> proposed a transformer based generative mode"
        },
        {
            "pid": "599c795f601a182cd263556f",
            "content": "fication PATTERN is a node classification dataset generated using the Stochastic Block Models (SBM) <ref type=\"bibr\" target=\"#b0\">(Abbe 2017</ref>). The task is classify the nodes into 2 communities."
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2018;</ref><ref type=\"bibr\" target=\"#b23\">Radford et al or node Laplacian position eigenvectors <ref type=\"bibr\" target=\"#b2\">(Belkin and Niyogi 2003;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020;</ref><ref type=\"bibr\" target=\"#b14\">Li et al. 202 arget=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type=\"bibr\" target=\"#b8\">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER"
        },
        {
            "pid": "599c795f601a182cd263556f",
            "content": "fication PATTERN is a node classification dataset generated using the Stochastic Block Models (SBM) <ref type=\"bibr\" target=\"#b0\">(Abbe 2017</ref>). The task is classify the nodes into 2 communities."
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2018;</ref><ref type=\"bibr\" target=\"#b23\">Radford et al or node Laplacian position eigenvectors <ref type=\"bibr\" target=\"#b2\">(Belkin and Niyogi 2003;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020;</ref><ref type=\"bibr\" target=\"#b14\">Li et al. 202 arget=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type=\"bibr\" target=\"#b8\">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER"
        },
        {
            "pid": null,
            "content": "ositions is challenging as there are symmetries which prevent canonical node positional information <ref type=\"bibr\" target=\"#b19\">(Murphy et al. 2019)</ref>. In fact, most of the GNNs which are train ormance on graph datasets. The issue of positional embeddings has been explored in recent GNN works <ref type=\"bibr\" target=\"#b19\">(Murphy et al. 2019;</ref><ref type=\"bibr\" target=\"#b34\">You, Ying, a"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2018;</ref><ref type=\"bibr\" target=\"#b23\">Radford et al or node Laplacian position eigenvectors <ref type=\"bibr\" target=\"#b2\">(Belkin and Niyogi 2003;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020;</ref><ref type=\"bibr\" target=\"#b14\">Li et al. 202 arget=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type=\"bibr\" target=\"#b8\">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": ""
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2018;</ref><ref type=\"bibr\" target=\"#b23\">Radford et al or node Laplacian position eigenvectors <ref type=\"bibr\" target=\"#b2\">(Belkin and Niyogi 2003;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020;</ref><ref type=\"bibr\" target=\"#b14\">Li et al. 202 arget=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type=\"bibr\" target=\"#b8\">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER"
        },
        {
            "pid": null,
            "content": "\u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "\u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe"
        },
        {
            "pid": "53e9b360b7602d9703e41a15",
            "content": "<p>We evaluate the performance of proposed Graph Transformer on three benchmark graph datasets-ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a rec chmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012</ref>) is a molecular dataset with the task of gra"
        },
        {
            "pid": null,
            "content": "\u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe"
        },
        {
            "pid": "558c06e3e4b0cfb70a1b341d",
            "content": "s <ref type=\"bibr\" target=\"#b36\">(Zhang et al. 2020)</ref>, or node Laplacian position eigenvectors <ref type=\"bibr\" target=\"#b2\">(Belkin and Niyogi 2003;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et ormula\">2020</ref>) make the use of available graph structure to pre-compute Laplacian eigenvectors <ref type=\"bibr\" target=\"#b2\">(Belkin and Niyogi 2003)</ref> and use them as node positional informa"
        },
        {
            "pid": null,
            "content": "\u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe"
        }
    ],
    "5f0ed12691e011ead96652e9": [
        {
            "pid": "53e9b115b7602d9703b9264f",
            "content": "by all the modalities.</p><p>Inspired by such an assumption and the fact that the Total Correlation <ref type=\"bibr\" target=\"#b27\">[29]</ref> can measure the amount of information shared by M (M \u2265 2) sifiers of each modality.</p><p>Total Correlation/Mutual information maximization Total Correlation <ref type=\"bibr\" target=\"#b27\">[29]</ref>, as an extension of Mutual Information, measures the amoun"
        },
        {
            "pid": "53e99e0cb7602d97026d0b95",
            "content": "ed by Blum et. al <ref type=\"bibr\" target=\"#b4\">[6]</ref>. <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "x 1 , ..., x M ) and q [M ] (x) \u2236= \u220f M i=1 p(x i ).</formula><p>According to dual representation in <ref type=\"bibr\" target=\"#b25\">[27]</ref>, we have the following lower bound for KL divergence betwe bound for KL divergence between p and q, and hence TC.</p><p>Lemma 1 (Dual version of f -divergence <ref type=\"bibr\" target=\"#b25\">[27]</ref>).</p><formula xml:id=\"formula_5\">D KL p [M ] q [M ] \u2265 sup"
        },
        {
            "pid": "5ac1829d17c44a1fda91821f",
            "content": "tions within graph-structured data <ref type=\"bibr\" target=\"#b28\">[30]</ref>). Kong and Schoenebeck <ref type=\"bibr\" target=\"#b17\">[19]</ref> provide another mutual information estimator in the co-tra identification of ground truth classifiers on semi-supervised multi-modality data, by generalizing <ref type=\"bibr\" target=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b6\">8]</ref> that can only handle"
        },
        {
            "pid": "53e9b666b7602d97041ca838",
            "content": "hat maps all the data points to the same representation. A common belief in multi-modality learning <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" targ ared by these M modalities, which is commonly assumed in the literature of semi-supervised learning <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "=\"#b29\">[31]</ref> learns representation by maximizing correlation of different modalities; and LMF <ref type=\"bibr\" target=\"#b21\">[23]</ref> performs multimodal fusion using low-rank tensors. The opt et. The MOSI is composed of 93 opinion videos from YouTube movie reviews. We follow the settings in <ref type=\"bibr\" target=\"#b21\">[23]</ref> for the data splits of training, validation and test set. 5%} for IEMOCAP and {1%, 2%, 3%} for MOSI. For a fair comparison, we follow architecture setting in <ref type=\"bibr\" target=\"#b21\">[23]</ref>. We adopt the modality encoder architectures in <ref type= etting in <ref type=\"bibr\" target=\"#b21\">[23]</ref>. We adopt the modality encoder architectures in <ref type=\"bibr\" target=\"#b21\">[23]</ref> as the single modality classifiers for CE and TCGM, while"
        },
        {
            "pid": null,
            "content": "hed clustering algorithms K-means++ <ref type=\"bibr\" target=\"#b1\">[2]</ref> and spectral clustering <ref type=\"bibr\" target=\"#b23\">[25]</ref>. Based on the promising unsupervised learning result, as s"
        },
        {
            "pid": "573696846e3b12023e58fa50",
            "content": "arget=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b9\">11]</ref> use weak classifiers trained by the labeled data from each m"
        },
        {
            "pid": null,
            "content": "x 1 , ..., x M ) and q [M ] (x) \u2236= \u220f M i=1 p(x i ).</formula><p>According to dual representation in <ref type=\"bibr\" target=\"#b25\">[27]</ref>, we have the following lower bound for KL divergence betwe bound for KL divergence between p and q, and hence TC.</p><p>Lemma 1 (Dual version of f -divergence <ref type=\"bibr\" target=\"#b25\">[27]</ref>).</p><formula xml:id=\"formula_5\">D KL p [M ] q [M ] \u2265 sup"
        },
        {
            "pid": null,
            "content": "ning algorithms and have shown superior performance on various tasks. Belghazi et allet@tokeeonedot <ref type=\"bibr\" target=\"#b3\">[4]</ref> presents a mutual information neural estimator, which are ut = E p [M ] (PTC).</formula><p>The Lemma 1 is commonly utilized for estimation of Mutual information <ref type=\"bibr\" target=\"#b3\">[4]</ref> or optimization as variational lower bound in the machine le"
        },
        {
            "pid": null,
            "content": "nt representation across modalities in an unsupervised way <ref type=\"bibr\" target=\"#b24\">[26,</ref><ref type=\"bibr\" target=\"#b26\">28]</ref>. These methods suffer from either too strong assumptions or nt with the real settings.</p><p>The second branch of work <ref type=\"bibr\" target=\"#b24\">[26,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" target=\"#b7\">9,</ref><ref type=\"bibr\" targe"
        }
    ],
    "5f0277e911dc830562231dac": [
        {
            "pid": "59939d49ffdae9cf10039e6f",
            "content": "ll a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref ty e Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type=\"bibr\" target=\"#b18\">[19]</ref> which consists of two information retrieval models in it. \ud835\udc5e, \ud835\udc46)) 1 + exp(\ud835\udc53 \ud835\udf19 (\ud835\udc51 |\ud835\udc5e, \ud835\udc46)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DVGAN-doc has an additional component \ud835\udc46 to represent the , it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we generate negative document set \ud835\udc37 \u2032 by selecting the do"
        },
        {
            "pid": "59a02ff0b161e8ad1a7b6eb9",
            "content": "hes have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b23\">[24]</ref>. The explicit approaches <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar to solve this problem <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar tly leverage subtopics to determine the diversity of results <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and supervised approaches such as DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref xplicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces the machine learning method into explicit appro ent \ud835\udc36 to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> score function for the generator. We introduce the score f <p>In the training process, we first train R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> respectively using MLE loss in both ways. It is because ou br\" target=\"#b5\">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> as supervised baseline methods. Top 20 results of Lemur ar b7\">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type=\"bibr\" target=\"#b11\">[12]</ref> to train DSSA method. The feature vector 1 http://playbigd DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": "IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> into search result diversification. Generator generates neg g/ns/1.0\"><head n=\"2.2\">Generative Adversarial Network</head><p>Generative Adversarial Network(GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> is initially used in the area of computer vision to generat"
        },
        {
            "pid": null,
            "content": "IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> into search result diversification. Generator generates neg g/ns/1.0\"><head n=\"2.2\">Generative Adversarial Network</head><p>Generative Adversarial Network(GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> is initially used in the area of computer vision to generat"
        },
        {
            "pid": "5736974d6e3b12023e638ae7",
            "content": "thod into explicit approaches. It calculates the distribution using the RNN and attention mechanism <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In our framework, we mainly use the score function of the . As DSSA, we will introduce the distribution of subtopic \ud835\udc34(\ud835\udc56 |\ud835\udc46). DSSA uses both RNN and attention <ref type=\"bibr\" target=\"#b15\">[16]</ref> mechanism to calculate it. Noticed that the selected docum"
        },
        {
            "pid": "57d063b4ac4436735428dbc4",
            "content": "et=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" t <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The explicit approaches <ref type=\"bibr\" target=\"#b5\">[6, et=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" t ximize the score margin of positive and negative rankings. Furthermore, Neural Tensor Network (NTN) <ref type=\"bibr\" target=\"#b23\">[24]</ref> was introduced into search result diversification to measu =\"bibr\" target=\"#b25\">[26]</ref>, PAMM <ref type=\"bibr\" target=\"#b5\">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> as sup implicit approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> emphasize the novel ervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26</ref>] are able to outperf"
        },
        {
            "pid": null,
            "content": "t=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>Most previ t=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref>. Depending on whe get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> stress the relevance between the documents and the subtopic"
        },
        {
            "pid": null,
            "content": "IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> into search result diversification. Generator generates neg g/ns/1.0\"><head n=\"2.2\">Generative Adversarial Network</head><p>Generative Adversarial Network(GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> is initially used in the area of computer vision to generat"
        },
        {
            "pid": "5550441845ce0a409eb4b330",
            "content": "=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>Most previous methods of diversification can be des ted rules such as MMR <ref type=\"bibr\" target=\"#b0\">[1]</ref> or a supervised measure such as R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and NTN < ppropriate number of relevant documents from the candidate document set. Some methods such as R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> only use the top documents in the ideal rankings while oth f the existing approaches solve this problem completely. The quality of training data used by R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> is high but its quantity is too small to train the model w =\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref>. Depending on whether the subtopics of query are explicitl use the machine learning method to improve the performance. The relational learning-to-rank (R-LTR) <ref type=\"bibr\" target=\"#b25\">[26]</ref> replaces the \ud835\udc46 div score by using the relationship matrix for document \ud835\udc51 given the query \ud835\udc5e and selected document ranking \ud835\udc46. In our method, we adapt the R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> score function for discriminator. We introduce the score f to generate the final document ranking result.</p><p>In the training process, we first train R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> respect our unsupervised baseline methods. We use ListMLE <ref type=\"bibr\" target=\"#b21\">[22]</ref>, R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PAMM <ref type=\"bibr\" target=\"#b5\">[6]</ref>, R-LTR-NTN, ble\" target=\"#tab_3\">2</ref> with two more features: linkbased diversity and URL-based diversity in <ref type=\"bibr\" target=\"#b25\">[26]</ref>, for PAMM, we use \ud835\udefc-nDCG@20 as the optimization metrics an rget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> emphasize the novelty of the documents, which infers that t et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26</ref>] are able to outperform the heuristic approaches <ref type=\""
        },
        {
            "pid": null,
            "content": "arget=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" ta rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> stress the relevanc arget=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" ta lso be categorized into heuristic approaches such as xQuAD <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and PM2 <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"b e heuristic approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> by learning an optimized ranking function. However, the lar rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Most explicit approaches focus on the subtopic coverage of relevance to the query \ud835\udc5e and the \ud835\udc46 sub function reflects the \ud835\udc51's relevance to the subtopics. xQuAD <ref type=\"bibr\" target=\"#b17\">[18]</ref> is one of the representative methods of unsupervised expli fication methods. We use Lemur as our non-diversified baseline method. We use xQuAD, TxQuAD, HxQuAD <ref type=\"bibr\" target=\"#b17\">[18]</ref>, PM2 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, TPM2 <ref ty"
        },
        {
            "pid": "573698486e3b12023e7110e7",
            "content": "b17\">18]</ref> and PM2 <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and supervised approaches such as DSSA <ref type=\"bibr\" targe en at subtopic level. We use google query suggestions as subtopics, which are released by Hu et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> on their website 1 . we only use the first level subtopics a PM2 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, TPM2 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> as our unsupervised baseline methods. We use ListMLE <ref ty"
        },
        {
            "pid": "5736974d6e3b12023e638ae7",
            "content": "thod into explicit approaches. It calculates the distribution using the RNN and attention mechanism <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In our framework, we mainly use the score function of the . As DSSA, we will introduce the distribution of subtopic \ud835\udc34(\ud835\udc56 |\ud835\udc46). DSSA uses both RNN and attention <ref type=\"bibr\" target=\"#b15\">[16]</ref> mechanism to calculate it. Noticed that the selected docum"
        },
        {
            "pid": null,
            "content": "representation of document. It can be constructed in different ways, In this paper, we use doc2vec <ref type=\"bibr\" target=\"#b13\">[14]</ref> to get document embeddings.</p><p>\ud835\udc65 \ud835\udc51,\ud835\udc5e and \ud835\udc65 \ud835\udc51,\ud835\udc56 : Releva"
        },
        {
            "pid": null,
            "content": "IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> into search result diversification. Generator generates neg g/ns/1.0\"><head n=\"2.2\">Generative Adversarial Network</head><p>Generative Adversarial Network(GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> is initially used in the area of computer vision to generat"
        },
        {
            "pid": "573696c56e3b12023e5c5e06",
            "content": "get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" t b0\">[1]</ref> or a supervised measure such as R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The ex \"#b25\">[26]</ref> only use the top documents in the ideal rankings while other methods such as PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> sample the training rankings by judging it through diversi ll to train the model which may lead to underfit. The quantity of the training dataset used by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> is large enough but the quality of it depends on some hype get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" t robability of optimal rankings. Based on the same score function of R-LTR, Xia et al. proposed PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> in which loss function is designed to directly maximize th uch as \ud835\udefc-NDCG and ERR-IA <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The form of \u2111 is inspired by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> method which aims to maximize the margin between positive |\ud835\udc5e, \ud835\udc36)] .<label>(11)</label></formula><p>The loss function of the discriminator is inspired by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> method which is aiming to maximize the margin between the approaches and implicit approaches. The implicit approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar /ref>.</p><p>Studies have shown that supervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "53e99ebdb7602d9702786088",
            "content": "/ref>) using Plackett-Luce model and the \ud835\udc38 is the diversification metrics such as \ud835\udefc-NDCG and ERR-IA <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The form of \u2111 is inspired by PAMM <ref type=\"bibr\" target=\" trics</head><p>Among all the evaluation metrics <ref type=\"bibr\">[2-4, 20, 21]</ref>, we use ERR-IA <ref type=\"bibr\" target=\"#b1\">[2]</ref>, \ud835\udefc-NDCG <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and NRBP <r gs but makes it hard for generator to imitate the real distribution of data because it is too ideal <ref type=\"bibr\" target=\"#b1\">(2)</ref>. random sampling makes it easy to imitate for generator but"
        },
        {
            "pid": "573698486e3b12023e7110e7",
            "content": "b17\">18]</ref> and PM2 <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and supervised approaches such as DSSA <ref type=\"bibr\" targe en at subtopic level. We use google query suggestions as subtopics, which are released by Hu et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> on their website 1 . we only use the first level subtopics a PM2 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, TPM2 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> as our unsupervised baseline methods. We use ListMLE <ref ty"
        },
        {
            "pid": null,
            "content": "target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar explicit approaches <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar diversity of results <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Most explicit appr explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and PM2 <ref type="
        },
        {
            "pid": null,
            "content": "is large enough but the quality of it depends on some hyper-parameters such as the range of \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b3\">[4]</ref> of negative ranking samples, which may cause the model hard that \ud835\udc46 is ranked. So the sampler also needs to re-rank \ud835\udc46 by the diversification metric like \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b3\">[4]</ref>. In practice, half of the selected document ranking \ud835\udc46 is sam <ref type=\"bibr\">[2-4, 20, 21]</ref>, we use ERR-IA <ref type=\"bibr\" target=\"#b1\">[2]</ref>, \ud835\udefc-NDCG <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and NRBP <ref type=\"bibr\" target=\"#b2\">[3]</ref> as our div train. We use 5-fold cross validation to tune the parameters in all experiments based on \ud835\udefc-nDCG@20 <ref type=\"bibr\" target=\"#b3\">[4]</ref>. A brief introduction to these baselines is as follows.</p><"
        },
        {
            "pid": null,
            "content": "or a broad query. In recent years, many search result diversification approaches have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target sification can be divided into explicit approaches and implicit approaches. The implicit approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ \">24,</ref><ref type=\"bibr\" target=\"#b25\">26</ref>] are able to outperform the heuristic approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target e way to solve the problem of query ambiguity, many models have been proposed to solve this problem <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target nts. The diversification score function of implicit approaches can be handcrafted rules such as MMR <ref type=\"bibr\" target=\"#b0\">[1]</ref> or a supervised measure such as R-LTR <ref type=\"bibr\" targe ments. In the early years' research on diversification, implicit methods are most unsupervised. MMR <ref type=\"bibr\" target=\"#b0\">[1]</ref> can be regarded as the foundation of implicit methods. Its d"
        },
        {
            "pid": "5736974d6e3b12023e638ae7",
            "content": "thod into explicit approaches. It calculates the distribution using the RNN and attention mechanism <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In our framework, we mainly use the score function of the . As DSSA, we will introduce the distribution of subtopic \ud835\udc34(\ud835\udc56 |\ud835\udc46). DSSA uses both RNN and attention <ref type=\"bibr\" target=\"#b15\">[16]</ref> mechanism to calculate it. Noticed that the selected docum"
        },
        {
            "pid": null,
            "content": "IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> into search result diversification. Generator generates neg g/ns/1.0\"><head n=\"2.2\">Generative Adversarial Network</head><p>Generative Adversarial Network(GAN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> is initially used in the area of computer vision to generat"
        }
    ],
    "5fae6daad4150a363cec035c": [
        {
            "pid": "5a260c8117c44a4ba8a30771",
            "content": "tention to, attention networks achieve a better performance with fewer layers. As an example, SENet <ref type=\"bibr\" target=\"#b5\">[5]</ref> introduces Squeeze-and-Excitation (SE) blocks to study the c rs in the spatial pyramid structure are not learnable, which is nearly cost-free. Compared to SENet <ref type=\"bibr\" target=\"#b5\">[5]</ref>, our structure only modifies the first fully-connected layer ter concatenation as input to the next block. More recently, attention based networks such as SENet <ref type=\"bibr\" target=\"#b5\">[5]</ref> and CBAM <ref type=\"bibr\">[6]</ref> provide an independent a s suppress insignificant features. Thus, visual features could be better captured and exploited. In <ref type=\"bibr\" target=\"#b5\">[5]</ref>, a Squeeze-and-Extraction block was proposed to learn the ch e last feature map which is small in size (7 ? 7 for example). However, attention based CNNs (e.g., <ref type=\"bibr\" target=\"#b5\">[5]</ref>, <ref type=\"bibr\">[6]</ref>, <ref type=\"bibr\" target=\"#b7\">[ effectiveness of the attention mechanism. To address this problem, we leverage the excitation block <ref type=\"bibr\" target=\"#b5\">[5]</ref> to encode v and generate a 1D attention map ?. The excitatio re ? is a rectified linear unit (ReLU) function and sig denotes the sigmoid function. Like in SENet <ref type=\"bibr\" target=\"#b5\">[5]</ref>, we set r to 16.</p></div> <div xmlns=\"http://www.tei-c.org/ ls and whistles, SPANet outperforms related stateof-art work <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b5\">5,</ref><ref type=\"bibr\" target=\"#b10\">10,</ref><ref type=\"bibr\" targe tention map from a feature map and then apply the learned attention map to the original feature map <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref> . However, being con rg/ns/1.0\"><head n=\"3.3.\">Spatial Pyramid Attention</head><p>Many existing attention based networks <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\">6,</ref><ref type=\"bibr\">6,</ref><ref type=\""
        },
        {
            "pid": "573696f46e3b12023e5f1198",
            "content": "sidered similar to SPPNet <ref type=\"bibr\" target=\"#b8\">[8]</ref> and Region of Interesting Pooling <ref type=\"bibr\" target=\"#b9\">[9]</ref>. In contrast, our spatial pyramid structure encodes a featur"
        },
        {
            "pid": "5d9edbea47c8f7664602cdf8",
            "content": "lated stateof-art work <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b5\">5,</ref><ref type=\"bibr\" target=\"#b10\">10,</ref><ref type=\"bibr\" target=\"#b11\">11]</ref>. Experimental resul eved by ResNet has made shortcut connections attractive. As a more dense reformulation, the work in <ref type=\"bibr\" target=\"#b10\">[10]</ref> connects every convolutional layer in a deep convolutional pe=\"bibr\" target=\"#b25\">[25]</ref>, MobileNetV2 <ref type=\"bibr\" target=\"#b11\">[11]</ref>, DenseNet <ref type=\"bibr\" target=\"#b10\">[10]</ref>, and ResNext <ref type=\"bibr\" target=\"#b26\">[26]</ref>, to ght-weight model MobileNetV2 <ref type=\"bibr\" target=\"#b11\">[11]</ref>, heavy-weight model DenseNet <ref type=\"bibr\" target=\"#b10\">[10]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b26\">[26]</ref>, and VG"
        },
        {
            "pid": "5736986b6e3b12023e73002f",
            "content": "ection in deep learning was first used in Highway Networks <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. By allowing an unimpeded information flowed across several"
        },
        {
            "pid": "5736986b6e3b12023e73002f",
            "content": "ection in deep learning was first used in Highway Networks <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. By allowing an unimpeded information flowed across several"
        },
        {
            "pid": "599c797a601a182cd2641cc7",
            "content": "(which output big-size feature maps) unable to fully leverage the advantages of attention mechanism <ref type=\"bibr\" target=\"#b7\">[7]</ref>. Following this argument, we present Spatial Pyramid Attenti r, attention based CNNs (e.g., <ref type=\"bibr\" target=\"#b5\">[5]</ref>, <ref type=\"bibr\">[6]</ref>, <ref type=\"bibr\" target=\"#b7\">[7]</ref>, etc.) apply global average pooling on each feature map. As"
        },
        {
            "pid": "59ae3bf12bbe271c4c71bd86",
            "content": "bileNetV2 is typically designed for lightweight models like<ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b28\">28]</ref>. DenseNet includes"
        },
        {
            "pid": "5736986b6e3b12023e73002f",
            "content": "ection in deep learning was first used in Highway Networks <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. By allowing an unimpeded information flowed across several"
        },
        {
            "pid": "59ae3bf12bbe271c4c71bd86",
            "content": "bileNetV2 is typically designed for lightweight models like<ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b28\">28]</ref>. DenseNet includes"
        },
        {
            "pid": "59ae3bf12bbe271c4c71bd86",
            "content": "bileNetV2 is typically designed for lightweight models like<ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b28\">28]</ref>. DenseNet includes"
        },
        {
            "pid": "5b67b4b417c44aac1c866e4f",
            "content": ""
        }
    ],
    "5f350c4191e011d4254d01da": [
        {
            "pid": "5eb78919da5629cf24430385",
            "content": "ial for training large and complex deep learning architectures. RNN-T models are difficult to train <ref type=\"bibr\" target=\"#b10\">[11]</ref> and also require significantly large amount of data to joi oral classification (CTC) model <ref type=\"bibr\" target=\"#b1\">[2]</ref> or cross entropy (CE) model <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and the prediction network with LSTM language model (LM) on.</p><p>model for encoder and prediction network in the context of TL the RNN-T model. Authors in <ref type=\"bibr\" target=\"#b10\">[11]</ref> have shown that CE initialized RNN-T models perform better in blocks. gets (necessary for CE training), is obtained from word level alignments as discussed in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. From the word alignments, the start frame, end frame and ned by using byte pair encoding <ref type=\"bibr\" target=\"#b25\">[26]</ref> algorithm as described in <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>We also report the word error rate (WER) on hybrid"
        },
        {
            "pid": "56d862e1dabfae2eee89dc58",
            "content": "ined with same amount of data. The AM consists of 6 layers of latency-controlled bidirectional LSTM <ref type=\"bibr\" target=\"#b26\">[27]</ref> with 1024 hidden dimension and 512 projection dimension. A"
        },
        {
            "pid": "573696016e3b12023e5154b2",
            "content": "lt;blank&gt; symbol. The word piece targets for en-US model is obtained by using byte pair encoding <ref type=\"bibr\" target=\"#b25\">[26]</ref> algorithm as described in <ref type=\"bibr\" target=\"#b10\">["
        },
        {
            "pid": "56d862e1dabfae2eee89dc58",
            "content": "ined with same amount of data. The AM consists of 6 layers of latency-controlled bidirectional LSTM <ref type=\"bibr\" target=\"#b26\">[27]</ref> with 1024 hidden dimension and 512 projection dimension. A"
        },
        {
            "pid": "5ecbc6339fced0a24b4f0810",
            "content": "the E2E framework <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. Authors in <ref type=\"bibr\" target=\"#b20\">[21]</ref> propo ased multi-lingual E2E model, along with methods to incorporate language information is proposed in <ref type=\"bibr\" target=\"#b22\">[23]</ref>. Although multi-lingual methods are attractive to address"
        },
        {
            "pid": "59ae3bf12bbe271c4c71bb11",
            "content": "r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target="
        },
        {
            "pid": null,
            "content": "r\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Recurrent neural netw"
        },
        {
            "pid": "558c90d4e4b02b9f07a7d5e9",
            "content": "el (LM) and pronunciation model with a single neural network <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target= pretrained models. Initializing the encoder with connectionist temporal classification (CTC) model <ref type=\"bibr\" target=\"#b1\">[2]</ref> or cross entropy (CE) model <ref type=\"bibr\" target=\"#b10\">["
        },
        {
            "pid": null,
            "content": "single neural network <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target="
        },
        {
            "pid": "573696016e3b12023e5154b2",
            "content": "lt;blank&gt; symbol. The word piece targets for en-US model is obtained by using byte pair encoding <ref type=\"bibr\" target=\"#b25\">[26]</ref> algorithm as described in <ref type=\"bibr\" target=\"#b10\">["
        },
        {
            "pid": null,
            "content": "r\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target="
        }
    ],
    "5f8d6be69fced0a24bbab005": [
        {
            "pid": "5843774bac44360f1083973e",
            "content": "t, thus it's effective to learn information from their interactions, such as low-order interactions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> or high-order interact"
        },
        {
            "pid": null,
            "content": "360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> 0.8093 0.4422 0.7878 0.3752 0.6709 0.1361 FED (Ours) 0.8113"
        },
        {
            "pid": "5a260c8617c44a4ba8a3203a",
            "content": "0.1457 DeepFM <ref type=\"bibr\" target=\"#b2\">[3]</ref> 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=\"bi dy clear compared with recent works such as xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> and DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Since DRM learns the relations of dimensions in embe"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "ased on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations among latent fields. Since each latent"
        },
        {
            "pid": null,
            "content": "ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "ased on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations among latent fields. Since each latent"
        },
        {
            "pid": null,
            "content": "360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> 0.8093 0.4422 0.7878 0.3752 0.6709 0.1361 FED (Ours) 0.8113"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "ased on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations among latent fields. Since each latent"
        },
        {
            "pid": null,
            "content": "ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "ased on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations among latent fields. Since each latent"
        },
        {
            "pid": null,
            "content": "360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> 0.8093 0.4422 0.7878 0.3752 0.6709 0.1361 FED (Ours) 0.8113"
        }
    ],
    "5f803c8f91e01119a5df749b": [
        {
            "pid": "5ece3bcb91e011dc23c22581",
            "content": "assignment, non-maximum suppression (NMS) post-processing. They are not fully end-to-end. Recently, <ref type=\"bibr\" target=\"#b2\">Carion et al. (2020)</ref> proposed DETR to eliminate the need for suc d attention module suffers from a quadratic complexity growth with the feature map size. DETR. DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref> is built upon the Transformer encoder-deco ng different feature levels. Other hyper-parameter setting and training strategy mainly follow DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref>, except that Focal Loss <ref type=\"bibr\" t or 50 epochs and the learning rate is decayed at the 40-th epoch by a factor of 0.1. Following DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref>, we train our models using Adam optimizer"
        },
        {
            "pid": "599c7949601a182cd262c13a",
            "content": ""
        },
        {
            "pid": "573696026e3b12023e515eec",
            "content": "s follows.</p><p>Given the input feature maps x \u2208 R C\u00d7H\u00d7W extracted by a CNN backbone (e.g., ResNet <ref type=\"bibr\" target=\"#b8\">(He et al., 2016)</ref>), DETR exploits a standard Transformer encoder eature maps {x l } L\u22121 l=1 (L = 4) from the output feature maps of stages C 3 through C 5 in ResNet <ref type=\"bibr\" target=\"#b8\">(He et al., 2016)</ref> (transformed by a 1 \u00d7 1 convolution), where C ion Details. ImageNet <ref type=\"bibr\" target=\"#b6\">(Deng et al., 2009)</ref> pre-trained ResNet-50 <ref type=\"bibr\" target=\"#b8\">(He et al., 2016)</ref> is utilized as the backbone for ablations. Mul"
        },
        {
            "pid": null,
            "content": "due to the intrinsic limitation in memory access patterns.</p><p>On the other hand, as discussed in <ref type=\"bibr\" target=\"#b41\">Zhu et al. (2019a)</ref>, there are variants of convolution, such as"
        },
        {
            "pid": "5dd6604a3a55ac78684ad02a",
            "content": "</ref> are proposed to automatically design cross-scale connections via neural architecture search. <ref type=\"bibr\" target=\"#b28\">Tan et al. (2020)</ref> proposes the BiFPN, which is a repeated simpl arget=\"#b27\">(Song et al., 2020)</ref> SENet154 + DCN 51.2 71.9 56.0 33.8 54.8 64.2 EfficientDet-D7 <ref type=\"bibr\" target=\"#b28\">(Tan et al., 2020)</ref> EfficientNet </p></div> <div xmlns=\"http://w"
        },
        {
            "pid": "599c7949601a182cd262c13a",
            "content": ""
        },
        {
            "pid": "5c04966a17c44a2c7470874a",
            "content": "ibr\" target=\"#b22\">Parmar et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Child et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Ho et al., 201 t loses global information. To compensate, <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2 target=\"#b22\">Parmar et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2"
        },
        {
            "pid": null,
            "content": "due to the intrinsic limitation in memory access patterns.</p><p>On the other hand, as discussed in <ref type=\"bibr\" target=\"#b41\">Zhu et al. (2019a)</ref>, there are variants of convolution, such as"
        },
        {
            "pid": "5e09a885df1a9c0c4168d067",
            "content": "features. Recently, NAS-FPN <ref type=\"bibr\" target=\"#b7\">(Ghiasi et al., 2019)</ref> and Auto-FPN <ref type=\"bibr\" target=\"#b37\">(Xu et al., 2019)</ref> are proposed to automatically design cross-sc"
        },
        {
            "pid": "53e9b844b7602d970440513c",
            "content": "he train set, and evaluated on the val set and test-dev set.</p><p>Implementation Details. ImageNet <ref type=\"bibr\" target=\"#b6\">(Deng et al., 2009)</ref> pre-trained ResNet-50 <ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "e=\"bibr\" target=\"#b11\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Ho et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Hu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Parmar et al., 2 ibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b10\">Hu et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b23\">Parmar et al. te the theoretically reduced complexity, <ref type=\"bibr\" target=\"#b23\">Parmar et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b10\">Hu et al. (2019)</ref> admit such approaches are much slower in imple eature of query elements. Different from <ref type=\"bibr\" target=\"#b23\">Parmar et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b10\">Hu et al. (2019)</ref>, deformable attention is just slightly slower"
        }
    ],
    "5fdb2e1691e0118a02c4f566": [
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "lculation.</p><p>Input: PSL Rules R, Prediction \u0177i , and Probability P(y|s i ), i = {1, 2, 3}; BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref>, to derive the sentence representation v i etails</head><p>In the framework of CTRL-PG, any contextualized word embedding method, such as BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref>, ELMo <ref type=\"bibr\" target=\"#b41\">(Pete nd RoBERTa <ref type=\"bibr\" target=\"#b37\">(Liu et al. 2019b)</ref>, can be utilized. We choose BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to derive contextualized sentence embeddin ch tokenized sequence and learns an embedding vector for it. We follow the experimental settings in <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to use 12 Transformer layers and attention #b29\">(Kingma and Ba 2014)</ref> to optimize the parameters. We follow the experimental settings in <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to set the dropout rate, and batch size as"
        },
        {
            "pid": null,
            "content": "nd Uzuner 2013)</ref> and Clinical TempEval <ref type=\"bibr\" target=\"#b3\">(Bethard et al. 2015</ref><ref type=\"bibr\" target=\"#b4\">(Bethard et al. , 2016</ref><ref type=\"bibr\" target=\"#b5\">(Bethard et"
        },
        {
            "pid": "59ae3c152bbe271c4c71e959",
            "content": ""
        },
        {
            "pid": "53e9b338b7602d9703e0c7f5",
            "content": "onal machine learning methods (Llorens, Saquete, and Navarro 2010; Sun, Rumshisky, and Uzuner 2013; <ref type=\"bibr\" target=\"#b49\">Xu et al. 2013;</ref><ref type=\"bibr\" target=\"#b47\">Tang et al. 2013;"
        },
        {
            "pid": null,
            "content": "r\" target=\"#b3\">(Bethard et al. 2015</ref><ref type=\"bibr\" target=\"#b4\">(Bethard et al. , 2016</ref><ref type=\"bibr\" target=\"#b5\">(Bethard et al. , 2017) )</ref> are some great efforts of building cli"
        },
        {
            "pid": null,
            "content": "nd Uzuner 2013)</ref> and Clinical TempEval <ref type=\"bibr\" target=\"#b3\">(Bethard et al. 2015</ref><ref type=\"bibr\" target=\"#b4\">(Bethard et al. , 2016</ref><ref type=\"bibr\" target=\"#b5\">(Bethard et"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "53e9aa1eb7602d970338e2de",
            "content": "es and relationships from the narratives <ref type=\"bibr\" target=\"#b1\">(Aronson and Lang 2010;</ref><ref type=\"bibr\" target=\"#b44\">Savova et al. 2010;</ref><ref type=\"bibr\" target=\"#b45\">Soysal et al."
        },
        {
            "pid": "5550446545ce0a409eb4d498",
            "content": "n is a generic algorithm and can be easily adapted to other domains, we also test it on the TB-dense<ref type=\"bibr\" target=\"#b7\">(Cassidy et al. 2014</ref>) dataset, which is based on TimeBank News C"
        },
        {
            "pid": "59ae3c152bbe271c4c71e959",
            "content": ""
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b16\">(Deng and Wiebe 2015;</ref><ref type=\"bibr\" target=\"#b12\">Chen et al. 2019;</ref><ref type=\"bibr\" target=\"#b28\">Hu et al. 2016</ref>) have explored Probabilistic Soft Logic (PSL) <r ess <ref type=\"bibr\" target=\"#b18\">(Farnadi, Babaki, and Getoor 2019)</ref>, Model Interpretability <ref type=\"bibr\" target=\"#b28\">(Hu et al. 2016)</ref>, Probabilistic Reasoning (Augustine, Rekatsina"
        }
    ],
    "5f8d6be69fced0a24bbaaf7b": [
        {
            "pid": "5b67b45517c44aac1c860823",
            "content": "atterns make the size of the model continuously increasing <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar required for the inference are also increasing accordingly <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar distillation (KD) in the computer vision field, a few work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have employed KD for RS to reduce the size of models while eacher, and also has a lower latency due to its small size <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>The core idea behind this process is that the soft l sions from the teacher model, the state-of-the-art methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have achieved comparable or even better performance to the p>However, there are still limitations in existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. First, the learning of the student is only guided by the t at Figure <ref type=\"figure\">1</ref>: The existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> distill the knowledge only based on the teacher's predictio of ranking orders among items. Unlike the existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that distill the knowledge of an item at a time, RRD formul ation performance compared to the state-of-the-art methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. An unified framework. We propose a novel framework-DE-RRD- uge success of KD in the computer vision field, a few work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillati sting items, we adopt a ranking position importance scheme <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that places more emphasis on the higher positions in the ra .1, 0.5, 1.0}. Following the notation of the previous work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, we call the student model trained without the help of the recommendation list would have strong correlations to the items that the user has interacted before <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Also, the soft labels provide guidance for distinguishing e=\"bibr\" target=\"#b24\">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b24\">[25]</ref> which applies KD for the ranking problem; Providing recomm items); the high-ranked items in the recommendation list would have strong correlations to the user <ref type=\"bibr\" target=\"#b24\">[25]</ref>. By using such additional supervisions from the teacher, t s. The proposed framework is compared with the following methods:</p><p>\u2022 Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b24\">[25]</ref>: A KD method for recommender system that uses items with t"
        },
        {
            "pid": "53e9b76eb7602d9704310d8f",
            "content": "he teacher and that of the student. To this end, RRD adopts the list-wise learning-to-rank approach <ref type=\"bibr\" target=\"#b28\">[29]</ref> and learns to ensure the student to preserve the ranking o hat of the student model. To this end, RRD adopts the classical list-wise learning-to-rank approach <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Its core idea is to define a probability of a permutation d of the ground-truth ranking order. For more details about the list-wise approach, please refer to <ref type=\"bibr\" target=\"#b28\">[29]</ref>.</p><p>However, merely adopting the list-wise loss can hav p>Relaxed permutation probability. Then, RRD defines a relaxed permutation probability motivated by <ref type=\"bibr\" target=\"#b28\">[29]</ref>. For user \ud835\udc62, \ud835\udf45 \ud835\udc96 denotes a ranked list of all the sampled"
        },
        {
            "pid": "5ec3ae5291e0112b16089e44",
            "content": "ly, it has a limitation in accurately maintaining the ranking orders predicted by the teacher model <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which leads to degraded recommendation performance.</p><p sly, it has a limitation in accurately maintaining the ranking orders in the teacher's ranking list <ref type=\"bibr\" target=\"#b23\">[24]</ref>. This can lead to limited recommendation performance.</p>< ms (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach <ref type=\"bibr\" target=\"#b23\">[24]</ref>. RRD enables the model to capture the ranking orders among"
        },
        {
            "pid": "5c2ffab33a55ac1a8c597363",
            "content": "ut evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we leave out a single interacted item for t"
        },
        {
            "pid": null,
            "content": "ques <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and approximated nearest neighbor search techniques <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> have been successful"
        },
        {
            "pid": null,
            "content": "s have adopted hash techniques to reduce the inference cost <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5ec3ae5291e0112b16089e44",
            "content": "ly, it has a limitation in accurately maintaining the ranking orders predicted by the teacher model <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which leads to degraded recommendation performance.</p><p sly, it has a limitation in accurately maintaining the ranking orders in the teacher's ranking list <ref type=\"bibr\" target=\"#b23\">[24]</ref>. This can lead to limited recommendation performance.</p>< ms (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach <ref type=\"bibr\" target=\"#b23\">[24]</ref>. RRD enables the model to capture the ranking orders among"
        },
        {
            "pid": "53e9a9d3b7602d9703334a58",
            "content": "head n=\"5.1\">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref>, Foursquare <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We"
        },
        {
            "pid": "5e9ef9b69fced0a24b1b64a4",
            "content": "e temperature annealing schedule, which gradually decays the temperature from \ud835\udf0f 0 to \ud835\udf0f \ud835\udc43 as done in <ref type=\"bibr\" target=\"#b10\">[11]</ref>: \ud835\udf0f (\ud835\udc5d) = \ud835\udf0f 0 (\ud835\udf0f \ud835\udc43 /\ud835\udf0f 0 ) \ud835\udc5d/\ud835\udc43 where \ud835\udf0f (\ud835\udc5d) is the temperatur"
        },
        {
            "pid": null,
            "content": "ompact\" model (student) by transferring knowledge from a previously trained \"large\" model (teacher) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target the one-hot class label, which leads to improved learning of the student model. Subsequent methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> have focused on dist e layers. Because teacher's intermediate layers are generally bigger than that of the student, they <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> utilize additional l"
        },
        {
            "pid": "599c7edc601a182cd28d0a2a",
            "content": "d on accelerating the inference of the existing recommenders <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Specifically, tree"
        },
        {
            "pid": "5d04e905da56295d08dd7af3",
            "content": "nuously increasing <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. A large model with easing accordingly <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Due to the high la"
        },
        {
            "pid": "5c2ffab33a55ac1a8c597363",
            "content": "ut evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we leave out a single interacted item for t"
        },
        {
            "pid": "5c2ffab33a55ac1a8c597363",
            "content": "ut evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we leave out a single interacted item for t"
        },
        {
            "pid": null,
            "content": "et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. A large model with numerous parameters has a high capacity et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Due to the high latency, it becomes difficult to apply suc get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. They first learn binary representations of users and items"
        },
        {
            "pid": "53e9a335b7602d9702c3f01c",
            "content": "=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Specifically, tree-based data structures <ref type=\"bibr\" target=\"#b1\">[2]</ref>, data compression techniques <ref type=\"bibr\" target=\"#b25\">"
        },
        {
            "pid": null,
            "content": "ompact\" model (student) by transferring knowledge from a previously trained \"large\" model (teacher) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target the one-hot class label, which leads to improved learning of the student model. Subsequent methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> have focused on dist e layers. Because teacher's intermediate layers are generally bigger than that of the student, they <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> utilize additional l"
        },
        {
            "pid": "5d4d46fb3a55acff992fdc60",
            "content": "problems such as applicable only to specific models (e.g., k-d tree for metric learningbased models <ref type=\"bibr\" target=\"#b11\">[12]</ref>), or easily falling into a local optimum due to the local"
        },
        {
            "pid": "53e9b7d3b7602d97043808b0",
            "content": "e range of the rest. To sample the interesting items, we adopt a ranking position importance scheme <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that places more e"
        },
        {
            "pid": "53e9a335b7602d9702c3f01c",
            "content": "=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Specifically, tree-based data structures <ref type=\"bibr\" target=\"#b1\">[2]</ref>, data compression techniques <ref type=\"bibr\" target=\"#b25\">"
        },
        {
            "pid": "53e9a9d3b7602d9703334a58",
            "content": "head n=\"5.1\">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref>, Foursquare <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We"
        },
        {
            "pid": "5ec3ae5291e0112b16089e44",
            "content": "ly, it has a limitation in accurately maintaining the ranking orders predicted by the teacher model <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which leads to degraded recommendation performance.</p><p sly, it has a limitation in accurately maintaining the ranking orders in the teacher's ranking list <ref type=\"bibr\" target=\"#b23\">[24]</ref>. This can lead to limited recommendation performance.</p>< ms (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach <ref type=\"bibr\" target=\"#b23\">[24]</ref>. RRD enables the model to capture the ranking orders among"
        }
    ],
    "5f2bd70491e011b36ba9ce89": [
        {
            "pid": "5ce3aebeced107d4c65ebaaf",
            "content": "n this paper, we introduce TOAD-GAN as a solution to these problems. Our work is inspired by SinGAN <ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019)</ref>, a recent Generative Adversa s of TOAD-GAN on Super Mario Bros. level 1-2. The architecture is adapted from SinGAN (cf. Fig. 4 of<ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019)</ref>). We use a downsampling meth e training process.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>SinGAN</head><p>SinGAN <ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019</ref>) is a novel GAN architecture p at the lowest scale. For a more in-depth explanation please refer to the original SinGAN paper by <ref type=\"bibr\" target=\"#b15\">Shaham, Dekel, and Michaeli (2019)</ref>.</p></div> <div xmlns=\"http:"
        },
        {
            "pid": "53e9ad56b7602d970373ceed",
            "content": "ar to a SMB level. We tested the validity of our generated content using the A* agent by Baumgarten <ref type=\"bibr\" target=\"#b19\">(Togelius, Karakovskiy, and Baumgarten 2010)</ref>, who was able to w"
        },
        {
            "pid": null,
            "content": "the vast amount of PCG approaches.</p><p>For a review of pattern-based level generators for SMB see <ref type=\"bibr\" target=\"#b11\">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation"
        },
        {
            "pid": null,
            "content": "\" target=\"#b3\">(Dahlskog and Togelius 2012</ref>) and combined them using simple statistical models <ref type=\"bibr\" target=\"#b17\">(Snodgrass and Ontan\u00f3n 2013)</ref>. The quality of these algorithms c lations, which have to be defined manually. Recent approaches used PCG via Machine Learning (PCGML) <ref type=\"bibr\" target=\"#b17\">(Summerville et al. 2018)</ref> to learn the patterns and relations f 17\">(Summerville et al. 2018)</ref> to learn the patterns and relations from the data automatically <ref type=\"bibr\" target=\"#b17\">(Summerville and Mateas 2016;</ref><ref type=\"bibr\" target=\"#b22\">Vol cts the height of a token in a level slice, given the heights of all tokens in the previous slices. <ref type=\"bibr\" target=\"#b17\">Summerville and Mateas (2016)</ref> trained their model on levels by generation of SMB levels. There are 15 original SMB levels provided by the Video Game Level Corpus <ref type=\"bibr\" target=\"#b17\">(Summerville et al. 2016)</ref>, each with different characteristics."
        },
        {
            "pid": null,
            "content": "rrence of tokens (e.g. a single enemy or ground block) in existing game levels to identify patterns <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2012</ref>) and combined them using simple stat ref type=\"bibr\" target=\"#b11\">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation <ref type=\"bibr\" target=\"#b3\">Dahlskog and Togelius (2012)</ref> identified and analyzed patterns wi lined how those patterns could be combined and varied to create new levels. In their continued work <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2014)</ref>, they additionally defined micro-(v"
        },
        {
            "pid": null,
            "content": "the vast amount of PCG approaches.</p><p>For a review of pattern-based level generators for SMB see <ref type=\"bibr\" target=\"#b11\">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation"
        },
        {
            "pid": null,
            "content": "ether and can result in repeating patterns.</p><p>Tile Pattern KL-Divergence We use the TPKL-Div by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref> to evaluate the similarity of our generat \"#fig_3\">4(i)-(l)</ref>.</p><p>Tab. 2 shows our resulting divergences compared to those reported by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref> and <ref type=\"bibr\" target=\"#b6\">Green e ously generated samples and found that an average of 90.62% of them were unique.</p><p>2 Results by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref>, on level 1-1 averaged over 2 \u00d7 2, 3 \u00d7 3,"
        },
        {
            "pid": null,
            "content": "ether and can result in repeating patterns.</p><p>Tile Pattern KL-Divergence We use the TPKL-Div by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref> to evaluate the similarity of our generat \"#fig_3\">4(i)-(l)</ref>.</p><p>Tab. 2 shows our resulting divergences compared to those reported by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref> and <ref type=\"bibr\" target=\"#b6\">Green e ously generated samples and found that an average of 90.62% of them were unique.</p><p>2 Results by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref>, on level 1-1 averaged over 2 \u00d7 2, 3 \u00d7 3,"
        },
        {
            "pid": null,
            "content": "ether and can result in repeating patterns.</p><p>Tile Pattern KL-Divergence We use the TPKL-Div by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref> to evaluate the similarity of our generat \"#fig_3\">4(i)-(l)</ref>.</p><p>Tab. 2 shows our resulting divergences compared to those reported by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref> and <ref type=\"bibr\" target=\"#b6\">Green e ously generated samples and found that an average of 90.62% of them were unique.</p><p>2 Results by <ref type=\"bibr\" target=\"#b13\">Lucas and Volz (2019)</ref>, on level 1-1 averaged over 2 \u00d7 2, 3 \u00d7 3,"
        },
        {
            "pid": null,
            "content": "\" target=\"#b3\">(Dahlskog and Togelius 2012</ref>) and combined them using simple statistical models <ref type=\"bibr\" target=\"#b17\">(Snodgrass and Ontan\u00f3n 2013)</ref>. The quality of these algorithms c lations, which have to be defined manually. Recent approaches used PCG via Machine Learning (PCGML) <ref type=\"bibr\" target=\"#b17\">(Summerville et al. 2018)</ref> to learn the patterns and relations f 17\">(Summerville et al. 2018)</ref> to learn the patterns and relations from the data automatically <ref type=\"bibr\" target=\"#b17\">(Summerville and Mateas 2016;</ref><ref type=\"bibr\" target=\"#b22\">Vol cts the height of a token in a level slice, given the heights of all tokens in the previous slices. <ref type=\"bibr\" target=\"#b17\">Summerville and Mateas (2016)</ref> trained their model on levels by generation of SMB levels. There are 15 original SMB levels provided by the Video Game Level Corpus <ref type=\"bibr\" target=\"#b17\">(Summerville et al. 2016)</ref>, each with different characteristics."
        },
        {
            "pid": null,
            "content": "sed using an embedding of the level slices that is inspired by the Fr\u00e9chet Inception Distance (FID) <ref type=\"bibr\" target=\"#b8\">(Heusel et al. 2017</ref>). In the second part, we show the generality"
        }
    ],
    "5efdaf7b91e01191d3d28242": [
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "entation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ rvation that a larger number of negative/positive examples in the objective leads to better results <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The last two terms </ref>, or different views of the same scene <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Chen et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> extensively study verious data augmentation methods. For lan br\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with ResNet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as ef type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. Following <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we set the temperature t = 0.5 and the dimension of the lat /ns/1.0\"><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with Resnet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as"
        },
        {
            "pid": "5cede10dda562983788edd28",
            "content": "ure the representation quality. All of these works sample negative examples from p(x). Arora et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> theoretically analyze the effect of contrastive representati erparameter. Without loss of generality, we set t = 1 for all theoretical results.</p><p>Similar to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we assume an underlying set of discrete latent classes C th f ) = inf W\u2208R K\u00d7d L Softmax (T , W f ).<label>(10)</label></formula><p>In line with the approach of <ref type=\"bibr\" target=\"#b0\">[1]</ref> we analyze the supervised loss of a mean classifier <ref typ commonly the case. The dependence on on N and T in Theorem 5 is roughly equivalent to the result in <ref type=\"bibr\" target=\"#b0\">[1]</ref>, but the two bounds are not directly comparable since the pr .</formula><p>In order to derive our bound we will exploit a concentration of measure result due to <ref type=\"bibr\" target=\"#b0\">[1]</ref>. They consider an objective of the form</p><formula xml:id=\""
        },
        {
            "pid": null,
            "content": "resting avenue of future work to adopt our debiased objective to a semi-supervised learning setting <ref type=\"bibr\" target=\"#b36\">[37]</ref> where true positive samples are accessible.</p></div> <div"
        },
        {
            "pid": null,
            "content": "e=\"bibr\" target=\"#b20\">[21]</ref> and examine six classification tasks: movie review sentiment (MR) <ref type=\"bibr\" target=\"#b28\">[29]</ref>, product reviews (CR) <ref type=\"bibr\" target=\"#b16\">[17]<"
        },
        {
            "pid": null,
            "content": "\uf8f9 \uf8fb . (<label>6</label></formula><formula xml:id=\"formula_10\">)</formula><p>The limiting objective <ref type=\"bibr\" target=\"#b5\">(6)</ref>, which we denote by L Q Debiased , still samples examples x \">CIFAR10 and STL10</head><p>First, for CIFAR10 <ref type=\"bibr\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref>"
        },
        {
            "pid": null,
            "content": "\uf8f9 \uf8fb . (<label>6</label></formula><formula xml:id=\"formula_10\">)</formula><p>The limiting objective <ref type=\"bibr\" target=\"#b5\">(6)</ref>, which we denote by L Q Debiased , still samples examples x \">CIFAR10 and STL10</head><p>First, for CIFAR10 <ref type=\"bibr\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref>"
        },
        {
            "pid": null,
            "content": "be learned from colorization <ref type=\"bibr\" target=\"#b37\">[38]</ref>, predicting transformations <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, or generative model"
        },
        {
            "pid": null,
            "content": "oach of <ref type=\"bibr\" target=\"#b0\">[1]</ref> we analyze the supervised loss of a mean classifier <ref type=\"bibr\" target=\"#b29\">[30]</ref>, where for each class c, the rows of W are set to the mean"
        },
        {
            "pid": null,
            "content": "assification (TREC) <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and paraphrase identification (MSRP) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Our experimental settings follow those for quick-thought (Q"
        },
        {
            "pid": null,
            "content": "50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. Followin 50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. We set t"
        },
        {
            "pid": null,
            "content": "type=\"bibr\" target=\"#b25\">26]</ref>, or generative modeling <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Remarkable success"
        },
        {
            "pid": "5ca600ae6558b90bfa4d76e3",
            "content": "rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. The key idea of contrastive learning is to contrast semant b1\">[2]</ref> extensively study verious data augmentation methods. For language, Logeswaran and Lee <ref type=\"bibr\" target=\"#b23\">[24]</ref> treat the context sentences as positive samples to efficie r\" target=\"#b7\">[8]</ref>. Our experimental settings follow those for quick-thought (QT) vectors in <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>In contrast to vision tasks, positive pairs here ar s/1.0\"><head>Sentence Embedding</head><p>We adopt the official code 3 of quick-thought (QT) vectors <ref type=\"bibr\" target=\"#b23\">[24]</ref>. To implement the debiased objective, we only modify the \""
        },
        {
            "pid": null,
            "content": "50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. Followin 50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. We set t"
        },
        {
            "pid": null,
            "content": "the language domain <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>Recently, self-supervised representation learning al"
        },
        {
            "pid": "5ca600ae6558b90bfa4d76e3",
            "content": "rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. The key idea of contrastive learning is to contrast semant b1\">[2]</ref> extensively study verious data augmentation methods. For language, Logeswaran and Lee <ref type=\"bibr\" target=\"#b23\">[24]</ref> treat the context sentences as positive samples to efficie r\" target=\"#b7\">[8]</ref>. Our experimental settings follow those for quick-thought (QT) vectors in <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>In contrast to vision tasks, positive pairs here ar s/1.0\"><head>Sentence Embedding</head><p>We adopt the official code 3 of quick-thought (QT) vectors <ref type=\"bibr\" target=\"#b23\">[24]</ref>. To implement the debiased objective, we only modify the \""
        },
        {
            "pid": "5736986b6e3b12023e73011a",
            "content": "rkable success has also been achieved in the language domain <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>Recently, se ext, we test the debiased objective for learning sentence embeddings. We use the BookCorpus dataset <ref type=\"bibr\" target=\"#b20\">[21]</ref> and examine six classification tasks: movie review sentime the \"src/s2v-model.py\" file and left the rest of the code unchanged. Since the official BookCorpus <ref type=\"bibr\" target=\"#b20\">[21]</ref> dataset is missing, we use the unofficial version 4 for th"
        },
        {
            "pid": "5736960b6e3b12023e51e01a",
            "content": "=\"bibr\" target=\"#b37\">[38]</ref>, predicting transformations <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, or generative modeling <ref type=\"bibr\" target=\"#b2\">[3,</"
        },
        {
            "pid": null,
            "content": "contrastive loss have outperformed even supervised learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "573696026e3b12023e515eec",
            "content": "target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with ResNet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <re d STL10 We adopt PyTorch to implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with Resnet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <re"
        },
        {
            "pid": "5736986b6e3b12023e73011a",
            "content": "rkable success has also been achieved in the language domain <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>Recently, se ext, we test the debiased objective for learning sentence embeddings. We use the BookCorpus dataset <ref type=\"bibr\" target=\"#b20\">[21]</ref> and examine six classification tasks: movie review sentime the \"src/s2v-model.py\" file and left the rest of the code unchanged. Since the official BookCorpus <ref type=\"bibr\" target=\"#b20\">[21]</ref> dataset is missing, we use the unofficial version 4 for th"
        },
        {
            "pid": null,
            "content": "assification (TREC) <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and paraphrase identification (MSRP) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Our experimental settings follow those for quick-thought (Q"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "pe=\"bibr\" target=\"#b19\">20]</ref>. Remarkable success has also been achieved in the language domain <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" targ + are shown in Figure <ref type=\"figure\" target=\"#fig_4\">4(a,b</ref>). Increasing \u03c4 + in Objective <ref type=\"bibr\" target=\"#b6\">(7)</ref> leads to more correction, and gradually improves the perform ile and left the rest of the code unchanged. We again constrain the estimator described in equation <ref type=\"bibr\" target=\"#b6\">(7)</ref> to be greater than zero since the feature vector of CURL is"
        }
    ],
    "5f75eed591e0111c1eb4da5f": [
        {
            "pid": "5d3ed25a275ded87f97deae1",
            "content": "rial attacks, designing defense mechanisms or building robust variants of GNNs have become critical <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019)</ref>.</p><p>In this paper, we propose a new approa nism to attenuate the influence of neighbors with large variance (potentially corrupted). Following <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019)</ref>, we set hidden dimensions at 16 and assume a poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>);</p><p>\u2022 UM-GNN achieves significantly lower tions for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>) has been the most effective, when compared to h a weighted aggregation of features in a closed neighborhood where the weights are trainable. RGCN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>): This is a recently proposed ap-proach that e random structural perturbations and its low performance strongly corroborates with the findings in <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>).</p><p>(ii) DICE Attack: In this challenging <ref type=\"bibr\" target=\"#b26\">(Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018)</ref>. Recently, Zhu et al. <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>) introduced a robust variant of GCN based on a"
        },
        {
            "pid": null,
            "content": "tacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020)</ref> library. Due to the lack of computationally e rnejad, and G\u00fcnnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al. 2018 accard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by tr"
        },
        {
            "pid": null,
            "content": "tacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020)</ref> library. Due to the lack of computationally e rnejad, and G\u00fcnnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al. 2018 accard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by tr"
        },
        {
            "pid": null,
            "content": "r three benchmark citation networks extensively used in similar studies: Cora, Citeseer, and Pubmed <ref type=\"bibr\" target=\"#b14\">(Sen et al. 2008)</ref>. The documents are represented by nodes, and"
        },
        {
            "pid": null,
            "content": "r three benchmark citation networks extensively used in similar studies: Cora, Citeseer, and Pubmed <ref type=\"bibr\" target=\"#b14\">(Sen et al. 2008)</ref>. The documents are represented by nodes, and"
        },
        {
            "pid": null,
            "content": "sarial attacks on images <ref type=\"bibr\" target=\"#b10\">(Goodfellow, Shlens, and Szegedy 2014;</ref><ref type=\"bibr\" target=\"#b18\">Szegedy et al. 2013)</ref> and their countermeasures <ref type=\"bibr\""
        },
        {
            "pid": "573696006e3b12023e513cb6",
            "content": "amples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type=\"bibr\" target=\"#b9\">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes t"
        },
        {
            "pid": "58437722ac44360f1082f135",
            "content": "ed all the baselines and the proposed approach using the Pytorch Deep Graph Library (version 0.5.1) <ref type=\"bibr\" target=\"#b21\">(Wang et al. 2019</ref>). In our implementation of UM-GNN, the GNN mo"
        },
        {
            "pid": null,
            "content": "onally designed in the graph structure can lead to a non-trivial performance degradation as seen in <ref type=\"bibr\" target=\"#b26\">(Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018)</ref>. This limits their app ng, and Leskovec 2017)</ref>. The vulnerability of GNNs to adversarial attacks was first studied in <ref type=\"bibr\" target=\"#b26\">(Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018)</ref>. Since then, several g proximation of the given graph and showed that it can defend against specific types of graph attack <ref type=\"bibr\" target=\"#b26\">(Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018)</ref>. Recently, Zhu et al. target=\"#b24\">(Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b22\">Wu et al. 2019)</ref>, gray-box <ref type=\"bibr\" target=\"#b26\">(Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018;</ref><ref type=\"bibr\" target"
        },
        {
            "pid": "5736986c6e3b12023e7308a0",
            "content": "adopted, there also exists models that implement convolutions directly using spatial neighborhoods <ref type=\"bibr\" target=\"#b7\">(Duvenaud et al. 2015;</ref><ref type=\"bibr\" target=\"#b0\">Atwood and T"
        },
        {
            "pid": null,
            "content": "m 100 target nodes with FGA attack. A lower value implies improved robustness. and black-box attacks<ref type=\"bibr\" target=\"#b2\">(Bojchevski and G\u00fcnnemann 2019)</ref>. (ii) Attacker capability: based"
        }
    ],
    "5f06e5e591e0117f54657c19": [
        {
            "pid": "53e99a48b7602d97022a8346",
            "content": "rg/ns/1.0\"><head n=\"1\">Introduction</head><p>The majority of the research efforts on improving VAEs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> is dedicated to the st e posterior up to the (l \u2212 1) th group. The objective is trained using the reparameterization trick <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>The main questi and bidirectional encoder networks <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>The goal of VAEs <ref type=\"bibr\" target=\"#b0\">[1]</ref> is to train a generative model in the form of p(x x x, z z z"
        },
        {
            "pid": null,
            "content": "8\">[19]</ref>). Moreover, the neural networks for VAEs should model long-range correlations in data <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "573696ce6e3b12023e5ce95a",
            "content": "\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> omit batch normalization (BN) <ref type=\"bibr\" target=\"#b36\">[37]</ref> to combat the sources of randomness that could potentially"
        },
        {
            "pid": null,
            "content": "#b46\">[47]</ref>, f (u) = u 1+e \u2212u , has been recently shown promising results in many applications <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. We also observe t 1]</ref> is a simple channel-wise gating layer that has been used widely in classification problems <ref type=\"bibr\" target=\"#b47\">[48]</ref>. We show that SE can also improve VAEs.</p><p>Final cell:"
        },
        {
            "pid": "5b67b47917c44aac1c863771",
            "content": "r\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target="
        },
        {
            "pid": "5a260c8617c44a4ba8a31e2b",
            "content": "get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, or tackling posterior collapse <ref type=\"bibr\" target=\"#b"
        },
        {
            "pid": "573696026e3b12023e515eec",
            "content": "_1\">2a</ref>). IAF-VAEs <ref type=\"bibr\" target=\"#b3\">[4]</ref> relies on regular residual networks <ref type=\"bibr\" target=\"#b43\">[44]</ref> for both top-down and bottom-up models without any batch n ld of the networks. Since the encoder and decoder in NVAE are implemented by deep residual networks <ref type=\"bibr\" target=\"#b43\">[44]</ref>, this can be done by increasing the kernel sizes in the co We empirically observe that BN-Activation-Conv performs better than the original Conv-BN-Activation <ref type=\"bibr\" target=\"#b43\">[44]</ref> in regular residual cells. A similar observation was made"
        },
        {
            "pid": "5bdc31b817c44a1f58a0c572",
            "content": "ll temperatures<ref type=\"foot\" target=\"#foot_2\">3</ref> . A similar observation was made in BigGAN <ref type=\"bibr\" target=\"#b74\">[75]</ref> and DCGAN <ref type=\"bibr\" target=\"#b75\">[76]</ref>. Howev"
        },
        {
            "pid": null,
            "content": "et=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. The role of neural network architectures for VAEs is somew"
        },
        {
            "pid": "53e9b844b7602d970440513c",
            "content": "ynamically binarized MNIST <ref type=\"bibr\" target=\"#b70\">[71]</ref>, CIFAR-10 [72], ImageNet 32\u00d732 <ref type=\"bibr\" target=\"#b72\">[73]</ref>, CelebA HQ 256\u00d7256 <ref type=\"bibr\" target=\"#b27\">[28]</re amically binarized MNIST <ref type=\"bibr\" target=\"#b70\">[71]</ref>, CIFAR-10 [72], ImageNet 32 \u00d7 32 <ref type=\"bibr\" target=\"#b72\">[73]</ref>, CelebA HQ <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and"
        },
        {
            "pid": "58d83051d649053542fe9c0b",
            "content": "=\"bibr\" target=\"#b9\">10]</ref>, formulating tighter bounds <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar"
        }
    ],
    "5f75cce491e0111c1eb4d6e6": [
        {
            "pid": "5e5e18ba93d709897ce2b48e",
            "content": "ing significantly fewer training data comparing to the few previous works which address heterophily <ref type=\"bibr\" target=\"#b22\">(Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b38\">Zhu et al. 2020 nificantly smaller fraction of training samples compared to previous works that address heterophily <ref type=\"bibr\" target=\"#b22\">(Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b38\">Zhu et al. 2020 pe=\"bibr\" target=\"#b20\">Namata et al. 2012)</ref>. We use the features and class labels provided by <ref type=\"bibr\" target=\"#b22\">Pei et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n ch benchmark with an identity matrix I. We use the training, validation and test splits provided by <ref type=\"bibr\" target=\"#b22\">Pei et al. (2020)</ref>.</p><p>Heterophily. We report results on grap"
        },
        {
            "pid": "53e9983db7602d970206633b",
            "content": "2</ref>) can be solved with iterative methods (J. <ref type=\"bibr\" target=\"#b10\">Neville 2000;</ref><ref type=\"bibr\" target=\"#b17\">Lu and Getoor 2003)</ref>, graph-based regularization and probabilist"
        },
        {
            "pid": "57a4e91aac44365e35c97c6e",
            "content": "nable parameters, and R (0) = X. We call our method with MLP-based estimator CPGNN-MLP. \u2022 GCN-Cheby <ref type=\"bibr\" target=\"#b3\">(Defferrard, Bresson, and Vandergheynst 2016)</ref>. We instantiate th pervised node classification problems thanks to their ability to learn through end-to-end training. <ref type=\"bibr\" target=\"#b3\">Defferrard, Bresson, and Vandergheynst (2016)</ref> proposed an early"
        },
        {
            "pid": "5a9cb66717c44a376ffb8c25",
            "content": "ding recommendation systems <ref type=\"bibr\" target=\"#b37\">(Ying et al. 2018)</ref>, bioinformatics <ref type=\"bibr\" target=\"#b39\">(Zitnik, Agrawal, and Leskovec 2018;</ref><ref type=\"bibr\" target=\"#b"
        },
        {
            "pid": null,
            "content": "omophily, which are Cora, Pubmed and Citeseer <ref type=\"bibr\" target=\"#b27\">(Sen et al. 2008;</ref><ref type=\"bibr\" target=\"#b20\">Namata et al. 2012)</ref>. We use the features and class labels provi"
        },
        {
            "pid": "5a260c8117c44a4ba8a30f54",
            "content": "eterophily <ref type=\"bibr\" target=\"#b38\">(Zhu et al. 2020)</ref>: GCN (Kipf and Welling 2017), GAT <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al. 2018)</ref>, <ref type=\"bibr\">GCN-Cheby (Defferrar ave looked into designs which strengthen the effectiveness of GNN to capture graph information: GAT <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al. 2018)</ref> and AGNN <ref type=\"bibr\" target=\"#b30"
        },
        {
            "pid": "5a260c0c17c44a4ba8a1e113",
            "content": "(Pandit et al. 2007;</ref><ref type=\"bibr\">Dou et al. 2020)</ref> or analysis of protein structures <ref type=\"bibr\" target=\"#b6\">(Fout et al. 2017)</ref>, where heterophilous connections are common."
        },
        {
            "pid": "53e9983db7602d970206633b",
            "content": "2</ref>) can be solved with iterative methods (J. <ref type=\"bibr\" target=\"#b10\">Neville 2000;</ref><ref type=\"bibr\" target=\"#b17\">Lu and Getoor 2003)</ref>, graph-based regularization and probabilist"
        },
        {
            "pid": "5a9cb66717c44a376ffb8c25",
            "content": "ding recommendation systems <ref type=\"bibr\" target=\"#b37\">(Ying et al. 2018)</ref>, bioinformatics <ref type=\"bibr\" target=\"#b39\">(Zitnik, Agrawal, and Leskovec 2018;</ref><ref type=\"bibr\" target=\"#b"
        },
        {
            "pid": "5d9edc8347c8f76646042a37",
            "content": "\"bibr\" target=\"#b1\">(Ahmed et al. 2018;</ref><ref type=\"bibr\" target=\"#b23\">Rossi et al. 2020;</ref><ref type=\"bibr\" target=\"#b32\">Wu et al. 2019)</ref>. However, there are also many instances in the worse for graphs without strong homophily. Theorem 1. The forward pass formulation of a 1-layer SGC <ref type=\"bibr\" target=\"#b32\">(Wu et al. 2019)</ref>, a simplified version of GCN without the non-l"
        },
        {
            "pid": "53e9ac48b7602d9703614de5",
            "content": "d learning (SSL) or collective classification <ref type=\"bibr\" target=\"#b27\">(Sen et al. 2008;</ref><ref type=\"bibr\" target=\"#b18\">McDowell, Gupta, and Aha 2007;</ref><ref type=\"bibr\" target=\"#b24\">Ro"
        }
    ],
    "5ef96b048806af6ef2772036": [
        {
            "pid": "5c2c7a9217c44a4e7cf3187a",
            "content": "ww.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>The task of Grounded video description (GVD) <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate existing works either encode region proposals independently or using selfattention-based mechanisms <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider impl a]</ref>, many works model the video in both global video features and regional object features. In <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, they encode the objects with transformer <r /head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ... ose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Feature Enhancement</head><p>In this part, we follow <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal an feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div> <div xmlns=\" > t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the ad n=\"4.1\">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 1 ph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we u 2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to emove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>.</p><p>Table <ref type=\"table\" target=\"#tab_"
        },
        {
            "pid": null,
            "content": "ce outputs through attention-based mechanisms <ref type=\"bibr\" target=\"#b5\">[Xu et al., 2018a;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Gao et al., 2019 e=\"bibr\" target=\"#b5\">[Xu et al., 2018a;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Gao et al., 2019]</ref>. However, since there is no explicit graph str ethod: Relation Graph. Since the region features are extracted by a pre-trained model trained on VG <ref type=\"bibr\" target=\"#b2\">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation o detect 100 region proposals and extract the feature. The detector is pre-trained on Visual Genome <ref type=\"bibr\" target=\"#b2\">[Krishna et al., 2017]</ref>. Finally, for the video feature, the temp to aggregate the features of the nodes modeled by topology A.</p><p>Inspired by resnet architecture <ref type=\"bibr\" target=\"#b2\">[He et al., 2016]</ref>, we propose the basic module of our architectu"
        },
        {
            "pid": null,
            "content": "understanding started attracting more attentions in some close related fields such as image caption <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref>. However, due to the complexity of video unde ibr\" target=\"#b2\">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref> on it. We adopt almost the same operation exc \" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2016]</ref>. These methods are effective but they overlook patial-attention in image caption domain <ref type=\"bibr\" target=\"#b0\">[Anderson et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Li et al., 2019a] {h 1 , h 2 , ..., h m } where v \u2208 R n\u00d7d is the global feature extracted by a pre-trained 3D-ConvNet <ref type=\"bibr\" target=\"#b4\">[Tran et al., 2015]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ eo segment in the dataset, we uniformly sample 10 frames. And for each frame, we use a Faster R-CNN <ref type=\"bibr\" target=\"#b4\">[Ren et al., 2015]</ref> detector with ResNeXt-101 backbone to detect"
        },
        {
            "pid": null,
            "content": "to generate the description of a video using the attention-based encoder-decoder like architectures <ref type=\"bibr\" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al ed encoder-decoder like architectures <ref type=\"bibr\" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2016] which learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type=\"bibr\" target=\"#b5\">[Xu et al., 2018a;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 202"
        },
        {
            "pid": null,
            "content": "understanding started attracting more attentions in some close related fields such as image caption <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref>. However, due to the complexity of video unde ibr\" target=\"#b2\">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref> on it. We adopt almost the same operation exc \" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2016]</ref>. These methods are effective but they overlook patial-attention in image caption domain <ref type=\"bibr\" target=\"#b0\">[Anderson et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Li et al., 2019a] {h 1 , h 2 , ..., h m } where v \u2208 R n\u00d7d is the global feature extracted by a pre-trained 3D-ConvNet <ref type=\"bibr\" target=\"#b4\">[Tran et al., 2015]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ eo segment in the dataset, we uniformly sample 10 frames. And for each frame, we use a Faster R-CNN <ref type=\"bibr\" target=\"#b4\">[Ren et al., 2015]</ref> detector with ResNeXt-101 backbone to detect"
        },
        {
            "pid": null,
            "content": "based methods which model the regions with abundant semantic relations are introduced to this area. <ref type=\"bibr\" target=\"#b6\">[Yao et al., 2018]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/"
        },
        {
            "pid": null,
            "content": "based methods which model the regions with abundant semantic relations are introduced to this area. <ref type=\"bibr\" target=\"#b6\">[Yao et al., 2018]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/"
        },
        {
            "pid": null,
            "content": "to generate the description of a video using the attention-based encoder-decoder like architectures <ref type=\"bibr\" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al ed encoder-decoder like architectures <ref type=\"bibr\" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2016] which learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type=\"bibr\" target=\"#b5\">[Xu et al., 2018a;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 202"
        },
        {
            "pid": null,
            "content": "to generate the description of a video using the attention-based encoder-decoder like architectures <ref type=\"bibr\" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al ed encoder-decoder like architectures <ref type=\"bibr\" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2016] which learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type=\"bibr\" target=\"#b5\">[Xu et al., 2018a;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 202"
        },
        {
            "pid": "5c0495ae17c44a2c74701ab8",
            "content": "es that separated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type=\"bibr\" target=\"#b0\">[Anderson et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al.,"
        },
        {
            "pid": null,
            "content": "entional video description task that generates a human-like sentence to describe the video contents <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref>, GVD has advantages of modelling the video b mance Comparisons</head><p>We compare HAST-Graph2Seq with the SOTA models, i.e., Masked Transformer <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\" i.e., Masked Transformer <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=\"bibr\" target=\"#b8\">[Z"
        }
    ],
    "5f993ec291e011a3fbe2fb5c": [
        {
            "pid": "5f03f3b611dc830562232042",
            "content": ""
        },
        {
            "pid": "5ce2d032ced107d4c635260c",
            "content": "al Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type=\"bibr\" target=\"#b22\">(Klicpera et al., 2018)</ref>. However, they focus on integrating thi ep, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type=\"bibr\" target=\"#b22\">(Klicpera et al., 2018)</ref>, which we compare against later. Howeve"
        },
        {
            "pid": "53e99fe4b7602d97028c3625",
            "content": ""
        },
        {
            "pid": "53e9a5cdb7602d9702ef65cb",
            "content": "diffusion-based semi-supervised learning algorithms on graphs such as the spectral graph transducer <ref type=\"bibr\" target=\"#b20\">(Joachims, 2003)</ref>, Gaussian random field models <ref type=\"bibr\""
        },
        {
            "pid": "5cd7fa07ced107d4c65bf319",
            "content": ""
        },
        {
            "pid": null,
            "content": "on et al., 2017b)</ref>. In our pipeline, we augment features with a regularized spectral embedding <ref type=\"bibr\" target=\"#b3\">(Chaudhuri et al., 2012;</ref><ref type=\"bibr\" target=\"#b48\">Zhang &am"
        },
        {
            "pid": null,
            "content": "somorphism Networks <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2018)</ref>, and various deep models <ref type=\"bibr\" target=\"#b25\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Rong et al., 20"
        },
        {
            "pid": "53e99fe4b7602d97028c3625",
            "content": ""
        },
        {
            "pid": "58437722ac44360f1082efeb",
            "content": ""
        },
        {
            "pid": null,
            "content": "type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017a)</ref>; examples include Graph Attention Networks <ref type=\"bibr\" target=\"#b40\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, Graph Isomorphism Networks <ref type"
        },
        {
            "pid": "5c04965717c44a2c747079b8",
            "content": ""
        },
        {
            "pid": "5e5e18de93d709897ce37796",
            "content": "et al., 2018)</ref>, and various deep models <ref type=\"bibr\" target=\"#b25\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Rong et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 20"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "lutional Network (GCN) <ref type=\"bibr\" target=\"#b21\">(Kipf &amp; Welling, 2017)</ref> or GraphSAGE <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017a)</ref>; examples include Graph Attention Netw"
        },
        {
            "pid": null,
            "content": "ithms designed to increase scalability <ref type=\"bibr\" target=\"#b1\">(Bojchevski et al., 2020;</ref><ref type=\"bibr\" target=\"#b47\">Zeng et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Rossi et al.,"
        },
        {
            "pid": "5d3ed25a275ded87f97deb29",
            "content": ", 2020)</ref> as well as Markov Random fields <ref type=\"bibr\" target=\"#b32\">(Qu et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 2019)</ref>, and some techniques use ad hoc incorporation"
        },
        {
            "pid": "5a260c8117c44a4ba8a30adf",
            "content": "e features <ref type=\"bibr\" target=\"#b15\">(Henderson et al., 2011;</ref><ref type=\"bibr\">2012;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton et al., 2017b)</ref>. In our pipeline, we augment features w"
        },
        {
            "pid": "573695fe6e3b12023e511794",
            "content": "ype=\"bibr\" target=\"#b41\">Wang &amp; Leskovec (2020)</ref> (in contrast to lower label rate settings <ref type=\"bibr\" target=\"#b45\">(Yang et al., 2016)</ref>) to ameliorate sensitivity to hyperparamete"
        },
        {
            "pid": "5992a3b85ba2006b76482e09",
            "content": "nt membership and there are no features <ref type=\"bibr\" target=\"#b24\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b46\">Yin et al., 2017)</ref>.</p><p>Data splits. The training/validation/t 9\">Namata et al., 2012)</ref> and Email <ref type=\"bibr\" target=\"#b24\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b46\">Yin et al., 2017)</ref>: 3 layers and 64 hidden channels with learnin"
        },
        {
            "pid": "5f02f17c91e011ee5e0258c8",
            "content": "bibr\" target=\"#b42\">(Wu et al., 2019)</ref>, as well as algorithms designed to increase scalability <ref type=\"bibr\" target=\"#b1\">(Bojchevski et al., 2020;</ref><ref type=\"bibr\" target=\"#b47\">Zeng et"
        },
        {
            "pid": "53e9bc2db7602d970489c1cf",
            "content": "of a European research institute, where classes are department membership and there are no features <ref type=\"bibr\" target=\"#b24\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b46\">Yin et al target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012)</ref> and Email <ref type=\"bibr\" target=\"#b24\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b46\">Yin et al"
        },
        {
            "pid": "5c04965717c44a2c747079b8",
            "content": ""
        },
        {
            "pid": "53e99fe4b7602d97028c3625",
            "content": ""
        }
    ],
    "5efb0d5691e011063336d27d": [
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "ef type=\"bibr\" target=\"#b49\">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what t"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "e modeled using neural architectures developed for natural language. In particular, the Transformer <ref type=\"bibr\" target=\"#b61\">(Vaswani et al., 2017)</ref>, which has revolutionized unsupervised l g Transformers. The Transformer is a neural architecture that uses attention to accelerate learning <ref type=\"bibr\" target=\"#b61\">(Vaswani et al., 2017)</ref>. In NLP, transformers are the backbone o ation techniques have been used to convey the structure and properties of attention in Transformers <ref type=\"bibr\" target=\"#b61\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">Kovaleva e"
        },
        {
            "pid": null,
            "content": "&amp; Nakamura, 2009)</ref>, and binding sites can reveal evolutionary relationships among proteins <ref type=\"bibr\" target=\"#b31\">(Lee et al., 2017)</ref>. Thus binding sites may provide the model wi"
        },
        {
            "pid": "5b3d98cc17c44a510f801853",
            "content": "er that predicts a property of interest <ref type=\"bibr\" target=\"#b62\">(Veldhoen et al., 2016;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Adi et al., \"bibr\" target=\"#b35\">Mickus et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Adi et al., 2016;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2018</ref>), BERT's learned knowledge of syntax <ref"
        },
        {
            "pid": null,
            "content": "\"bibr\" target=\"#b17\">Fox et al., 2013;</ref><ref type=\"bibr\" target=\"#b5\">Berman et al., 2000;</ref><ref type=\"bibr\" target=\"#b38\">Moult et al., 2018)</ref>, which contains amino acid sequences annota \"bibr\" target=\"#b17\">Fox et al., 2013;</ref><ref type=\"bibr\" target=\"#b5\">Berman et al., 2000;</ref><ref type=\"bibr\" target=\"#b38\">Moult et al., 2018)</ref> and the Secondary Structure dataset <ref ty bibr\" target=\"#b45\">(Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Berman et al., 2000;</ref><ref type=\"bibr\" target=\"#b38\">Moult et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Klausen et al bibr\" target=\"#b45\">(Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Berman et al., 2000;</ref><ref type=\"bibr\" target=\"#b38\">Moult et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Klausen et al e measure precision@L/5, where L is the length of the protein sequence, following standard practice <ref type=\"bibr\" target=\"#b38\">(Moult et al., 2018)</ref>; for binding site prediction, we measure p"
        },
        {
            "pid": "5e8da0d591e011f2de583ba8",
            "content": "\" target=\"#b14\">(Elnaggar et al., 2020)</ref>. Riesselman et al. ( <ref type=\"formula\">2019</ref>); <ref type=\"bibr\" target=\"#b34\">Madani et al. (2020)</ref> trained autoregressive generative models t"
        },
        {
            "pid": "5ce3af9aced107d4c65f6b80",
            "content": "target=\"#b4\">Bepler &amp; Berger, 2019;</ref><ref type=\"bibr\" target=\"#b45\">Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b48\">Rives et al., 2019)</ref>. TAPE created a benchmark of five tasks to effect of mutations and generate natural-like proteins.</p><p>From an interpretability perspective, <ref type=\"bibr\" target=\"#b48\">Rives et al. (2019)</ref> showed that the output embeddings from a pr"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5bdc318017c44a1f58a089e3",
            "content": "ground-truth annotations of contact maps and binding sites. Visualizations based on the NGL Viewer <ref type=\"bibr\" target=\"#b52\">(Rose et al., 2018;</ref><ref type=\"bibr\" target=\"#b51\">Rose &amp; Hi"
        },
        {
            "pid": "5ce3a6abced107d4c6511bbd",
            "content": "or discriminative and generative tasks, for example, using LSTMs or Transformer-based architectures <ref type=\"bibr\" target=\"#b1\">(Alley et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Bepler &amp; B"
        }
    ],
    "5fc61cdb91e0118947381abc": [
        {
            "pid": "53e99acab7602d970234a328",
            "content": "of computing paths and cycles with larger networks, IUAD adopts Weisfeiler-Lehman sub-graph kernel <ref type=\"bibr\" target=\"#b38\">[39]</ref> to evaluate the similarity between two vertices. WL-kernel a j , v a j ) .<label>(4)</label></formula><p>Due to page limitation, more details please refer to <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p><p>For the"
        },
        {
            "pid": "53e9a877b7602d97031c413b",
            "content": "\"#b6\">[7]</ref>, object identification <ref type=\"bibr\" target=\"#b7\">[8]</ref>, duplicate detection <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and entity Fig. < \"#b6\">[7]</ref>, object identification <ref type=\"bibr\" target=\"#b7\">[8]</ref>, duplicate detection <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and entity matchi"
        },
        {
            "pid": "53e9a645b7602d9702f745c5",
            "content": "eps:</p><p>? Step I: Generating ?-Stable Collaborative Relations. We employ the FP-growth algorithm <ref type=\"bibr\" target=\"#b36\">[37]</ref> with support threshold ? to mine all ?-SCRs, denoted as F,"
        },
        {
            "pid": null,
            "content": "ltiple authors.</p><p>Author disambiguation is related to several similar tasks like record linkage <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>, entity resolution \"http://www.tei-c.org/ns/1.0\"><head>II. RELATED WORK</head><p>Our work is related to record linkage <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>, entity resolution serve a performance reduction for incremental author disambiguation. This is due to the facts that: <ref type=\"bibr\" target=\"#b0\">(1)</ref> single paper only provides limited information to identify t"
        },
        {
            "pid": "599c7edc601a182cd28d1076",
            "content": ""
        },
        {
            "pid": "599c7edc601a182cd28d1076",
            "content": ""
        },
        {
            "pid": "5b67b46b17c44aac1c862038",
            "content": ""
        },
        {
            "pid": "53e9a877b7602d97031c413b",
            "content": "\"#b6\">[7]</ref>, object identification <ref type=\"bibr\" target=\"#b7\">[8]</ref>, duplicate detection <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and entity Fig. < \"#b6\">[7]</ref>, object identification <ref type=\"bibr\" target=\"#b7\">[8]</ref>, duplicate detection <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and entity matchi"
        },
        {
            "pid": "57d063c3ac44367354290601",
            "content": "belong to the ego-network of \"Wei Wang (DUT)\". matching <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b14\">[15]</ref>, etc., being helpful to many applications in database, inf type=\"bibr\" target=\"#b10\">[11]</ref> and entity matching <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b14\">[15]</ref>, etc., which are applied in many scenes widely, such as so"
        },
        {
            "pid": "53e9b73ab7602d97042d1332",
            "content": "ration network can well reflect the similarity between vertices. Usually, paths, cycles, or kernels <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bib"
        },
        {
            "pid": "53e9afe8b7602d9703a39438",
            "content": "vised methods <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Among them, em \">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>- <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bib \">[22]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>- <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bib e=\"bibr\">Shin et al. tackle</ref> this problem by splitting vertices in the graph of co-authorships <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Liu et al. introduce a coarse-to-fine multiple clustering s, or kernels <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref> are used to measure the similarity of vertices. Due to the"
        }
    ],
    "5f03f3b611dc83056223205d": [
        {
            "pid": "5e5e193793d709897ce5cdee",
            "content": "tive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ nerated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> attribute this perfo filter on the spectral domain, thus deriving smoothing features across a graph. Another recent work <ref type=\"bibr\" target=\"#b2\">[3]</ref> verify that smoothing is the nature of most typical graph co shallow architectures. A smoothness regularizer term and adaptive edge optimization are proposed in <ref type=\"bibr\" target=\"#b2\">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Net"
        },
        {
            "pid": "5a9cb66717c44a376ffb8c0f",
            "content": "bute this performance degradation to the oversmoothing issue <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, which states that descriptions of the over-smoothing issue simplify the assumption of non-linear activation function <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> or make approximat , which means that representations of nodes converge to indistinguishable limits. To our knowledge, <ref type=\"bibr\" target=\"#b14\">[15]</ref> is the first attempt to demystify the over-smoothing issue whole graph when the number of training nodes is limited under a semi-supervised learning setting. <ref type=\"bibr\" target=\"#b14\">[15]</ref> applies co-training and self-training to overcome the limi tations has a slight downward trend as the number of propagation iterations increases. According to <ref type=\"bibr\" target=\"#b14\">[15]</ref>, the node representations suffering from the oversmoothing ervation when building very deep graph neural networks, which aligns with the over-smoothing issue. <ref type=\"bibr\" target=\"#b14\">[15]</ref> and <ref type=\"bibr\" target=\"#b32\">[33]</ref> study the ov layers, are very difficult to be separated.  Several studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> attribute this performance degradation phenomenon to the ov"
        },
        {
            "pid": null,
            "content": ", graph classification <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5b67b4b917c44aac1c867dbc",
            "content": "get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "f type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, graph classification <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": "arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar AGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and GIN <ref type=\"bibr\" target=\"#b31\">[32]</ref>, can be obtained under this framework by deploying differe"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": "arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar AGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and GIN <ref type=\"bibr\" target=\"#b31\">[32]</ref>, can be obtained under this framework by deploying differe"
        },
        {
            "pid": "5b67b4b917c44aac1c867dbc",
            "content": "get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "baselines: Logistic Regression (LogReg), Multilayer Perceptron (MLP), Label Propagation (LabelProp) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Normalized Laplacian Label Propagation (LabelProp NL) <ref rop) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Normalized Laplacian Label Propagation (LabelProp NL) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Cheb-Net <ref type=\"bibr\" target=\"#b3\">[4]</ref>, Graph Con"
        },
        {
            "pid": null,
            "content": "vided in Appendix A.7. We implemented our proposed DAGNN and some necessary baselines using Pytorch <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Pytorch Geometric <ref type=\"bibr\" target=\"#b4\">[5]</r"
        },
        {
            "pid": null,
            "content": "te that the propagation process of the GCN model is a special symmetric form of Laplacian smoothing <ref type=\"bibr\" target=\"#b28\">[29]</ref>, which makes the representations of nodes in the same clas"
        },
        {
            "pid": null,
            "content": "r proposed metric from the quantitative perspective, we employ a data visualization technique t-SNE <ref type=\"bibr\" target=\"#b18\">[19]</ref>. t-SNE provides an interpretable visualization, especially"
        },
        {
            "pid": "5e5e18d793d709897ce34a43",
            "content": "get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and link prediction"
        },
        {
            "pid": "58d82fc8d649053542fd59aa",
            "content": "arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar aph Attention Network(GAT) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Mixture Model Network (MoNet) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Graph-SAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, APPNP"
        },
        {
            "pid": null,
            "content": "rget=\"#b32\">[33]</ref> by analyzing the connection of nodes' influence distribution and random walk <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Recently, SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref> i"
        },
        {
            "pid": "5e5e18d793d709897ce34a43",
            "content": "get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and link prediction"
        },
        {
            "pid": "5b67b45517c44aac1c8607aa",
            "content": "ral networks. Great successes have been achieved for many applications, such as node classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target"
        },
        {
            "pid": "5e5e18d793d709897ce34a43",
            "content": "get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and link prediction"
        },
        {
            "pid": "5ac1829d17c44a1fda917eb3",
            "content": "and link prediction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Graph convolutions adopt a neighborhood aggregation (or me"
        },
        {
            "pid": "5b67b45517c44aac1c8607aa",
            "content": "ral networks. Great successes have been achieved for many applications, such as node classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target"
        },
        {
            "pid": "5e5e192393d709897ce54907",
            "content": "ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and link prediction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": ", graph classification <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar"
        }
    ],
    "5fc75d8591e0114897921043": [
        {
            "pid": "5ef3247a91e0110c353da822",
            "content": "\"bibr\" target=\"#b8\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Fey et al., 2020)</ref>. Graph neural networks have been proven to be ibr\" target=\"#b0\">Abu-El-Haija et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Loukas, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Fey et al., 2020)</ref>. In the context of molecular property predicti olution on both the input graph and a coarser version of it from which all cycles have been removed <ref type=\"bibr\" target=\"#b6\">(Fey et al., 2020)</ref>. Despite interesting results, this approach g >Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type=\"bibr\" target=\"#b6\">Fey et al. (2020)</ref> who proposed to perform message passing betwee"
        },
        {
            "pid": null,
            "content": "receptive field of each convolution <ref type=\"bibr\" target=\"#b7\">(Flam-Shepherd et al., 2020;</ref><ref type=\"bibr\" target=\"#b21\">Nikolentzos et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Abu-El-H m has been investigated by many recent works <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b21\">Nikolentzos et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Abu-El-H"
        },
        {
            "pid": "57a4e91aac44365e35c97c6e",
            "content": "approximation of spectral graph convolution <ref type=\"bibr\" target=\"#b1\">(Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Defferrard et al., 2016)</ref>  <ref type=\"table\">1</ref>: Main charac"
        },
        {
            "pid": "57a4e91aac44365e35c97c6e",
            "content": "approximation of spectral graph convolution <ref type=\"bibr\" target=\"#b1\">(Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Defferrard et al., 2016)</ref>  <ref type=\"table\">1</ref>: Main charac"
        },
        {
            "pid": null,
            "content": "amentally diverging from the standard convolution based framework, such as invariant graph networks <ref type=\"bibr\" target=\"#b18\">(Maron et al., 2019;</ref><ref type=\"bibr\">2018)</ref> or relational"
        },
        {
            "pid": "53e9a841b7602d970318da4e",
            "content": "ion can be seen as a computationally simple first-order approximation of spectral graph convolution <ref type=\"bibr\" target=\"#b1\">(Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Defferrard et"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b240",
            "content": "is to extend the current framework, to make it equivalent to higher order Weisfeiler Leman kernels <ref type=\"bibr\" target=\"#b19\">(Morris et al., 2019)</ref>, or to increase the receptive field of ea"
        },
        {
            "pid": "53e9a841b7602d970318da4e",
            "content": "ion can be seen as a computationally simple first-order approximation of spectral graph convolution <ref type=\"bibr\" target=\"#b1\">(Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Defferrard et"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": "n the context of chemistry. We propose a very simple correction to the now standard GIN convolution <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref> that enables the network to detect small cycl <p>A second problem is that the most common framework, namely message passing neural network (MPNN) <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref>, is not an universal approximator of function that the operation is injective on multisets, or at least on a large finite ensemble of multisets. <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref> have proposed graph isomorphism network(GIN), ations encoding structure and node features, some drawbacks have been pointed by followup works. In <ref type=\"bibr\" target=\"#b24\">Xu et al. (2019)</ref> it is shown that GCNs are limited in its capab"
        },
        {
            "pid": null,
            "content": "amentally diverging from the standard convolution based framework, such as invariant graph networks <ref type=\"bibr\" target=\"#b18\">(Maron et al., 2019;</ref><ref type=\"bibr\">2018)</ref> or relational"
        },
        {
            "pid": "53e9a841b7602d970318da4e",
            "content": "ion can be seen as a computationally simple first-order approximation of spectral graph convolution <ref type=\"bibr\" target=\"#b1\">(Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Defferrard et"
        }
    ],
    "5f7af09591e011983cc81efc": [
        {
            "pid": "5dcd263a3a55ac58039516c5",
            "content": "earning visual representations in a self-supervised manner <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>. Pushing the embeddings of two transformed versions of the ations learned in an unsupervised way. It is however shown <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref> that increasing the memory/batch size leads to diminishing t also use contrastive learning losses. These include MoCo <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type= contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref> keeps a queue with features of the last few batches as mem t of self-supervised representation learning. We delve deeper into learning with a momentum encoder <ref type=\"bibr\" target=\"#b31\">[30]</ref> and show evidence that harder negatives are required to fa ions. a)</head><p>We delve deeper into a top-performing contrastive self-supervised learning method <ref type=\"bibr\" target=\"#b31\">[30]</ref> and observe the need for harder negatives; b) We propose h air, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in <ref type=\"bibr\" target=\"#b31\">[30]</ref>. A popular and highly successful loss function for contras \"bibr\" target=\"#b65\">64,</ref><ref type=\"bibr\" target=\"#b78\">77]</ref>, a queue of the last batches <ref type=\"bibr\" target=\"#b31\">[30]</ref>, or simply be every other image in the current minibatch < computing them as the encoder keeps changing. The Momentum Contrast (or MoCo) approach of He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref> offers a compromise between the two negative sampling extr and all features in Q are encoded with the key encoder.</p><p>How hard are MoCo negatives? In MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> (resp. SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>) t helping a lot towards learning the proxy task.</p><p>On the difficulty of the proxy task. For MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, InfoMin i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> and MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref>. MoC rop testing. For object detection on PASCAL VOC <ref type=\"bibr\" target=\"#b18\">[19]</ref> we follow <ref type=\"bibr\" target=\"#b31\">[30]</ref> and fine-tune a Faster R-CNN <ref type=\"bibr\" target=\"#b55 pe=\"foot\" target=\"#foot_2\">3</ref> code and report the common AP, AP50 and AP75 metrics. Similar to <ref type=\"bibr\" target=\"#b31\">[30]</ref>, we do not perform hyperparameter tuning for the object de tic segmentation on the COCO dataset <ref type=\"bibr\" target=\"#b42\">[41]</ref>. Following He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref>, we use Mask R-CNN <ref type=\"bibr\" target=\"#b29\">[29]</re nd on the train2017 set (118k images) and evaluate on val2017. We adopt feature normalization as in <ref type=\"bibr\" target=\"#b31\">[30]</ref> when fine-tuning. MoCHi and MoCo use the same hyper-parame llowing our discussion in Section 3.2, we wanted to verify that hardness of the proxy task for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> is directly correlated to the difficulty of the transforma i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref> and som et=\"#b53\">[52]</ref> study the robustness of contrastive self-supervised learning methods like MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> and PIRL <ref type=\"bibr\" target=\"#b47\">[46]</ref> and saw [11]</ref>, e.g. the addition of a target network whose parameter update is lagging similar to MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>  Synthesizing for supervised metric learning. Recently, sy rt in parenthesis the difference to MoCo-v2. * denotes reproduced results. \u2020 results are copied from<ref type=\"bibr\" target=\"#b31\">[30]</ref>. We bold (resp. underline) the highest results overall (re rg/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">In this section we study contrastive learning for MoCo<ref type=\"bibr\" target=\"#b31\">[30]</ref> on ImageNet-100, a subset of ImageNet consisting of 100 cl et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b47\">46,</ref><ref type=\"bibr\" tar d highly successful loss function for contrastive learning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b65\">64]</ref> is the following:</ 2 -normalized. In a number of recent successful approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b47\">46,</ref><ref type=\"bibr\" tar pervised learning papers do not discuss variance ; in fact only papers from highly resourceful labs <ref type=\"bibr\" target=\"#b31\">[30,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5cede0e6da562983788c532a",
            "content": "so related to metric learning works that employ generators <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>. Apart from not requiring labels, our method exploits the m od exploits the memory component, something not present in <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>. It has no extra parameters or loss terms that need to be o </ref><ref type=\"bibr\" target=\"#b36\">35]</ref>. Works like <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref> use generators to synthesize negatives in a supervised scen and exploit its memory component. What is more, and unlike <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>, we do not require a generator, i.e. have no extra paramete izing negatives was explored in metric learning literature <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87,</ref><ref type=\"bibr\" target=\"#b36\">35]</ref>. Works like <ref ty"
        },
        {
            "pid": null,
            "content": "part from audio, other methods have used use automatic extracted text, e.g. from speech transcripts <ref type=\"bibr\" target=\"#b63\">[62,</ref><ref type=\"bibr\" target=\"#b62\">61,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b47\">46,</ref><ref type=\"bibr\" target=\"#b65\">64]</ref>. As revealed by recent studies <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "part from audio, other methods have used use automatic extracted text, e.g. from speech transcripts <ref type=\"bibr\" target=\"#b63\">[62,</ref><ref type=\"bibr\" target=\"#b62\">61,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "y shown to be a highly effective way of learning visual representations in a self-supervised manner <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>. Pushing the embed evaluate the quality of visual representations learned in an unsupervised way. It is however shown <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref> that increasing th lude MoCo <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, PIRL <ref type=\"b prediction tasks.</p><p>Most of the top-performing contrastive methods leverage data augmentations <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta r\" target=\"#b31\">[30]</ref>. A popular and highly successful loss function for contrastive learning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" ta erature parameter and all embeddings are 2 -normalized. In a number of recent successful approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" ta ww.tei-c.org/ns/1.0\"><head n=\"4.3\">Discussion and analysis of MoCHi</head><p>Recent approaches like <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> use a Multi-layer s used for target tasks-a lower-layer embedding is used instead. Unless otherwise stated, we follow <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and use a 2-layer e contrastive loss, current top contrastive approaches either substantially increase the batch size <ref type=\"bibr\" target=\"#b10\">[11]</ref>, or keep large memory banks. Approaches like <ref type=\"bi <ref type=\"bibr\" target=\"#b31\">[30]</ref>, or simply be every other image in the current minibatch <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>The log-likelihood function of Eq (1) is defined ov target=\"#b82\">81]</ref>. Sampling negatives from the same batch leads to a need for larger batches <ref type=\"bibr\" target=\"#b10\">[11]</ref> while sampling negatives from a memory bank that contains </p><p>How hard are MoCo negatives? In MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> (resp. SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>) the authors show that increasing the memory (resp. batch) ><p>On the difficulty of the proxy task. For MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, InfoMin <ref type=\"bibr\" target=\"#b66\">[65]</ref>, and ot at is not very far from the supervised case.</p><p>Focusing on the positive pair. Works like SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Tia ead the resulting features in the embedding space. BYOL makes a number of modifications over SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, e.g. the addition of a target network whose parameter upd ariance ; in fact only papers from highly resourceful labs <ref type=\"bibr\" target=\"#b31\">[30,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b66\">65]</ref> report averaged res"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5ec7a32791e0118397f3ed9b",
            "content": "rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b66\">65,</ref><ref type=\"bibr\" target=\"#b74\">73]</ref>, heavy data augmentations applied to the same image are cru edding space.</p><p>Measuring the utilization of the embedding space. Very recently, Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref> presented two losses/metrics for assessing contrastive lea /head><p>The uniformity experiment in Figure <ref type=\"figure\">3c</ref> is based on Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref>. We follow the same definitions of the losses/metrics as p erstand the underlying mechanism that make it work so well <ref type=\"bibr\" target=\"#b68\">[67,</ref><ref type=\"bibr\" target=\"#b74\">73,</ref><ref type=\"bibr\" target=\"#b60\">59,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "y shown to be a highly effective way of learning visual representations in a self-supervised manner <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>. Pushing the embed evaluate the quality of visual representations learned in an unsupervised way. It is however shown <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref> that increasing th lude MoCo <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, PIRL <ref type=\"b prediction tasks.</p><p>Most of the top-performing contrastive methods leverage data augmentations <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta r\" target=\"#b31\">[30]</ref>. A popular and highly successful loss function for contrastive learning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" ta erature parameter and all embeddings are 2 -normalized. In a number of recent successful approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" ta ww.tei-c.org/ns/1.0\"><head n=\"4.3\">Discussion and analysis of MoCHi</head><p>Recent approaches like <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> use a Multi-layer s used for target tasks-a lower-layer embedding is used instead. Unless otherwise stated, we follow <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and use a 2-layer e contrastive loss, current top contrastive approaches either substantially increase the batch size <ref type=\"bibr\" target=\"#b10\">[11]</ref>, or keep large memory banks. Approaches like <ref type=\"bi <ref type=\"bibr\" target=\"#b31\">[30]</ref>, or simply be every other image in the current minibatch <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>The log-likelihood function of Eq (1) is defined ov target=\"#b82\">81]</ref>. Sampling negatives from the same batch leads to a need for larger batches <ref type=\"bibr\" target=\"#b10\">[11]</ref> while sampling negatives from a memory bank that contains </p><p>How hard are MoCo negatives? In MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> (resp. SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>) the authors show that increasing the memory (resp. batch) ><p>On the difficulty of the proxy task. For MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, InfoMin <ref type=\"bibr\" target=\"#b66\">[65]</ref>, and ot at is not very far from the supervised case.</p><p>Focusing on the positive pair. Works like SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Tia ead the resulting features in the embedding space. BYOL makes a number of modifications over SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, e.g. the addition of a target network whose parameter upd ariance ; in fact only papers from highly resourceful labs <ref type=\"bibr\" target=\"#b31\">[30,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b66\">65]</ref> report averaged res"
        },
        {
            "pid": "5736960b6e3b12023e51e01a",
            "content": "get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b37\">36,</ref><ref type=\"bibr\" target=\"#b49\">48]</ref>. Instance discrimination <ref type=\"bibr\" target=\"#b78\">[77"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5c0495ae17c44a2c74701845",
            "content": "br\" target=\"#b39\">38,</ref><ref type=\"bibr\" target=\"#b80\">79]</ref>, predicting the \"arrow of time\" <ref type=\"bibr\" target=\"#b75\">[74]</ref>, pace <ref type=\"bibr\" target=\"#b73\">[72]</ref> or predict"
        },
        {
            "pid": "53e9a479b7602d9702d98afa",
            "content": "ab_4\">2</ref> we present results for object detection and semantic segmentation on the COCO dataset <ref type=\"bibr\" target=\"#b42\">[41]</ref>. Following He et al. <ref type=\"bibr\" target=\"#b31\">[30]</"
        },
        {
            "pid": "5b3d98cc17c44a510f801acc",
            "content": "0\">[11]</ref>, or keep large memory banks. Approaches like <ref type=\"bibr\" target=\"#b47\">[46,</ref><ref type=\"bibr\" target=\"#b78\">77]</ref> use memories that contain the whole training set, while the age in the dataset <ref type=\"bibr\" target=\"#b47\">[46,</ref><ref type=\"bibr\" target=\"#b65\">64,</ref><ref type=\"bibr\" target=\"#b78\">77]</ref>, a queue of the last batches <ref type=\"bibr\" target=\"#b31\" e time consuming task of keeping a large memory up-to-date <ref type=\"bibr\" target=\"#b47\">[46,</ref><ref type=\"bibr\" target=\"#b78\">77]</ref>. In the latter case, a trade-off exists between the \"freshn ype=\"bibr\" target=\"#b37\">36,</ref><ref type=\"bibr\" target=\"#b49\">48]</ref>. Instance discrimination <ref type=\"bibr\" target=\"#b78\">[77]</ref> and CPC <ref type=\"bibr\" target=\"#b50\">[49]</ref> were amo"
        },
        {
            "pid": "573696ce6e3b12023e5cec74",
            "content": "size of 128 (resp. 512) and a step learning rate schedule that drops at epochs 30, 40 and 50 (resp. <ref type=\"bibr\" target=\"#b61\">60,</ref><ref type=\"bibr\" target=\"#b81\">80)</ref>. For training we us ed the sequential nature of the temporal dimension, e.g. future frame prediction and reconstruction <ref type=\"bibr\" target=\"#b61\">[60]</ref>, shuffling and then predicting or verifying the order of f"
        },
        {
            "pid": null,
            "content": "d hard negative mixing strategy is presented in Figure <ref type=\"figure\">1</ref>; it shows a t-SNE <ref type=\"bibr\" target=\"#b44\">[43]</ref> plot after running MoCHi on 32-dimensional random embeddin"
        },
        {
            "pid": "599c795d601a182cd2635171",
            "content": "head><p>Hard negatives are critical for contrastive learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5aed14e217c44a4438159ac5",
            "content": "on a single image <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b37\">36,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "58d82fced649053542fd6a0f",
            "content": "t=\"#b75\">[74]</ref>, pace <ref type=\"bibr\" target=\"#b73\">[72]</ref> or predicting the \"odd\" element <ref type=\"bibr\" target=\"#b20\">[21]</ref> from a set of clips. Recently, contrastive, memory-based s"
        },
        {
            "pid": "5ec7a32791e0118397f3ed9b",
            "content": "rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b66\">65,</ref><ref type=\"bibr\" target=\"#b74\">73]</ref>, heavy data augmentations applied to the same image are cru edding space.</p><p>Measuring the utilization of the embedding space. Very recently, Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref> presented two losses/metrics for assessing contrastive lea /head><p>The uniformity experiment in Figure <ref type=\"figure\">3c</ref> is based on Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref>. We follow the same definitions of the losses/metrics as p erstand the underlying mechanism that make it work so well <ref type=\"bibr\" target=\"#b68\">[67,</ref><ref type=\"bibr\" target=\"#b74\">73,</ref><ref type=\"bibr\" target=\"#b60\">59,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "ods in the table, e.g. PCL <ref type=\"bibr\" target=\"#b41\">[40]</ref>, or the clustering approach of <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Both unfortunately use a different setup for PASCAL VOC and"
        }
    ],
    "5f0ee1ca91e011ead96654a4": [
        {
            "pid": "5d0b009e8607575390fd4a5c",
            "content": "ering what causes deep networks to be fragile to adversarial examples and how to improve robustness <ref type=\"bibr\" target=\"#b45\">[47,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" ta t has been theoretically shown that decreasing the input dimensionality of data improves robustness <ref type=\"bibr\" target=\"#b45\">[47]</ref>. Adversarial training <ref type=\"bibr\" target=\"#b33\">[35]< ining performance on natural examples.</p><p>Using the first order vulnerability of neural networks <ref type=\"bibr\" target=\"#b45\">[47]</ref>, we theoretically show that increasing output dimensionali ethods can improve the model's robustness without compromising clean accuracy. Simon-Gabriel et al. <ref type=\"bibr\" target=\"#b45\">[47]</ref> conducted a theoretical analysis of the vulnerability of n ref>. We denote the multitask predictor as F and each individual task predictor as F c . Prior work <ref type=\"bibr\" target=\"#b45\">[47]</ref> showed that the norm of gradients captures the vulnerabili rial noise is imperceptible, i.e., r \u2192 0, we can approximate \u2206L with a first-order Taylor expansion <ref type=\"bibr\" target=\"#b45\">[47]</ref>.</p><p>Lemma 1. For a given neural network F that predicts l></formula><p>Remark 1. By increasing the number of output tasks M , the first order vulnerability <ref type=\"bibr\" target=\"#b45\">[47]</ref> of network decreases. In the ideal case, if the model has s we add more tasks, the norm of the joint gradient decreases, indicating improvement to robustness <ref type=\"bibr\" target=\"#b45\">[47]</ref>. The only exception is the depth estimation task, which we e same dimension for baselines and ours during comparison because input dimension impacts robustness<ref type=\"bibr\" target=\"#b45\">[47]</ref>.</note> \t\t</body> \t\t<back>  \t\t\t<div type=\"acknowledgement\""
        },
        {
            "pid": null,
            "content": "zation procedure. For instance, more training data -both labeled and unlabeled -improves robustness <ref type=\"bibr\" target=\"#b41\">[43,</ref><ref type=\"bibr\" target=\"#b50\">52]</ref>. It has been theor"
        },
        {
            "pid": "599c795b601a182cd2634931",
            "content": "arget=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b44\">46,</ref><ref type=\"bibr\" target=\"#b52\">54,</ref><ref type=\"bibr\" target=\"#b35\">37]</ref> to fool target models. While attacking single output models"
        },
        {
            "pid": "599c795b601a182cd2634931",
            "content": "arget=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b44\">46,</ref><ref type=\"bibr\" target=\"#b52\">54,</ref><ref type=\"bibr\" target=\"#b35\">37]</ref> to fool target models. While attacking single output models"
        },
        {
            "pid": "57a4e921ac44365e35c98d0a",
            "content": "eously attacked. We experiment with up to 11 vision tasks on two natural image datasets, Cityscapes <ref type=\"bibr\" target=\"#b6\">[8]</ref> and Taskonomy <ref type=\"bibr\" target=\"#b58\">[60]</ref>. Whe ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Datasets</head><p>Cityscapes. The Cityscapes dataset <ref type=\"bibr\" target=\"#b6\">[8]</ref> consists of images of urban driving scenes. We study three t"
        },
        {
            "pid": "53e99f28b7602d97027f7737",
            "content": "in the literature. While past theoretical work showed the hardness of multi-objective optimization <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>, we leverage this"
        },
        {
            "pid": null,
            "content": "e=\"bibr\" target=\"#b28\">30]</ref> and optimization procedures <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "57a4e921ac44365e35c98d0a",
            "content": "eously attacked. We experiment with up to 11 vision tasks on two natural image datasets, Cityscapes <ref type=\"bibr\" target=\"#b6\">[8]</ref> and Taskonomy <ref type=\"bibr\" target=\"#b58\">[60]</ref>. Whe ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Datasets</head><p>Cityscapes. The Cityscapes dataset <ref type=\"bibr\" target=\"#b6\">[8]</ref> consists of images of urban driving scenes. We study three t"
        },
        {
            "pid": "53e99f28b7602d97027f7737",
            "content": "in the literature. While past theoretical work showed the hardness of multi-objective optimization <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>, we leverage this"
        },
        {
            "pid": "5da2f8aa3a55ac3402d8c10c",
            "content": "et=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b56\">58,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>, yet they remain brittle to adversarial examples. A large b"
        },
        {
            "pid": null,
            "content": "zation procedure. For instance, more training data -both labeled and unlabeled -improves robustness <ref type=\"bibr\" target=\"#b41\">[43,</ref><ref type=\"bibr\" target=\"#b50\">52]</ref>. It has been theor"
        }
    ],
    "5f03f3b611dc830562232090": [
        {
            "pid": "5aed148b17c44a44381550b0",
            "content": "s reported that these numerical errors brought about huge reputation risk, and even economic losses <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Since the documents disclosed by the firm usually have the significance testing, presented in the academic papers in major psychology journals. A recent study <ref type=\"bibr\" target=\"#b0\">[1]</ref> published a system called AutoDoc, and introduced the module uch more numerical facts in tables than textual paragraphs. Therefore, as an important extension to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose Automatic Numerical Cross-Checking over Tables ( ncial, and politic fields. It has attracted a lot of research interests in recent years. Cao et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> propose a system to cross-check numerical facts by extractin"
        },
        {
            "pid": "53e9b5bcb7602d97041036a5",
            "content": "cit hierarchical headers. There are some studies about recognizing this type of tables. Fang et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a Random Forest classification to identify the comp"
        },
        {
            "pid": "53e9b5bcb7602d97041036a5",
            "content": "cit hierarchical headers. There are some studies about recognizing this type of tables. Fang et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a Random Forest classification to identify the comp"
        },
        {
            "pid": null,
            "content": "elated evident paragraphs, and finally give a classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In academic field,"
        },
        {
            "pid": "5736977f6e3b12023e6658ca",
            "content": "ng text to table cells <ref type=\"bibr\" target=\"#b5\">[6]</ref>, table cell search for a given query <ref type=\"bibr\" target=\"#b14\">[15]</ref>, ad hoc search over tables <ref type=\"bibr\" target=\"#b17\">"
        },
        {
            "pid": "5843777eac44360f10841ac6",
            "content": "target=\"#b17\">[18]</ref>, transforming complex tables to the form that can be stored in a database <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Our task, cross-checking over numerical tables, is also a"
        },
        {
            "pid": null,
            "content": "l errors mean weak corporate governance or poor quality of financial reports. Fang, Huang, and Wang <ref type=\"bibr\" target=\"#b3\">[4]</ref> reveal that errors would affect the investors' reactions to"
        },
        {
            "pid": "56d81593dabfae2eee6f531d",
            "content": "structured information which is laborious when collecting the labelling dataset. Vlachos and Riedel <ref type=\"bibr\" target=\"#b16\">[17]</ref> propose a dataset to verify the claims made by public figu"
        },
        {
            "pid": "5aed14d617c44a4438159569",
            "content": "give a classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In academic field, Nuijten et al. <ref type=\"bibr\" target="
        },
        {
            "pid": "5aed14d617c44a4438159569",
            "content": "give a classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In academic field, Nuijten et al. <ref type=\"bibr\" target="
        },
        {
            "pid": "53e9b5bcb7602d97041036a5",
            "content": "cit hierarchical headers. There are some studies about recognizing this type of tables. Fang et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a Random Forest classification to identify the comp"
        }
    ],
    "5fe4094e9e795e14f30e634a": [
        {
            "pid": "599c7974601a182cd263f01c",
            "content": "a few fine-tuning updates. The proposed learning to pre-train can be deemed a form of meta-learning <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017)</ref>, also known as learning to lear ermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017)</ref>. Meta-learning aims to learn pr hods directly adjust the optimization algorithm to enable quick adaptation with just a few examples <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017;</ref><ref type=\"bibr\" target=\"#b43\">Y"
        },
        {
            "pid": "5e5e18eb93d709897ce3ce41",
            "content": "t=\"#b28\">(Navarin, Tran, and Sperduti 2018;</ref><ref type=\"bibr\" target=\"#b18\">Hu et al. 2019</ref><ref type=\"bibr\" target=\"#b17\">Hu et al. , 2020) )</ref> is to learn transferable prior knowledge fr t=\"#b18\">Hu et al. 2019)</ref>, or still require supervised information for graph-level pretraining <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref>). While at the node level, predicting links bet is to learn a generic initialization for model parameters using readily available graph structures <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref><ref type=\"bibr\" target=\"#b18\">(Hu et al. , 2019 atasets. We conduct experiments on data from two domains: biological function prediction in biology <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref>) and research field prediction in bibliography. ers with three unsupervised tasks to capture different aspects of a graph. More recently, Hu et al. <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> propose different strategies to pre-train grap subgraph is centered at a paper and contains the associated information of For biology data, as in <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref>, we use 306,925 unlabeled protein ego-networks that correspond to 40 binary classification tasks. We split the downstream data with species split <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref>, and evaluate the test performance with averag across the graph's patch representations; (3) Context Prediction strategy (denoted by ContextPred) <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> to explore graph structures and (4) Attribute 2020)</ref> to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> to learn the regularities of the node and edge which harms the generalization of the pre-trained GNNs. This finding confirms previous observations <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b31\">Rosenstein et al"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf312f1",
            "content": "ghborhoods on the graph, naturally capturing both graph structures as well as node or edge features <ref type=\"bibr\" target=\"#b47\">(Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b42\">Wu et bibr\" target=\"#b42\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b2\">Battaglia et al. 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zhou et"
        },
        {
            "pid": "57a4e91aac44365e35c97c6e",
            "content": "entations on graphs, this concept is extended to convolution neural networks using spectral methods <ref type=\"bibr\" target=\"#b6\">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "599c7960601a182cd2636137",
            "content": "errard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" target=\"#b5\">Bruna et al. 2014;</ref><ref type=\"bibr\" target=\"#b22\">Levie et al. 2019;</ref><ref type=\"bibr\" target=\"#b42\">Xu et al. 2019"
        },
        {
            "pid": null,
            "content": "odel-F), and consider three perspectives for comparison: Centered Kernel Alignment (CKA) similarity <ref type=\"bibr\" target=\"#b20\">(Kornblith et al. 2019)</ref> between the parameters of Model-P 0.6"
        },
        {
            "pid": "5b67b4b917c44aac1c867dbc",
            "content": "Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39\">Velickovic et al. 2018;</ref><ref type=\"bibr\" target=\"#b45\">Ying et al. 2018b;</ref><ref type=\"bibr\" target=\"#b15\">Hasanzadeh et t al. 2015)</ref> or more complex approaches <ref type=\"bibr\" target=\"#b5\">(Bruna et al. 2014;</ref><ref type=\"bibr\" target=\"#b45\">Ying et al. 2018b</ref>). We abstract READOUT as a parameterized func"
        },
        {
            "pid": "58437722ac44360f1082efeb",
            "content": "et al. 2020)</ref>. Various GNN architectures with different aggregation schemes have been proposed <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, hese GNNs have achieved impressive performance in many tasks, such as node and graph classification <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, et=\"#b42\">Xu et al. 2019a</ref>) and message passing architectures to aggregate neighbors' features <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\">Niepert, Ahmed, and Kut ted for different GNN architectures. We experiment with four popular GNN architectures, namely, GCN <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017)</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b1"
        },
        {
            "pid": "5f03f3b611dc830562232019",
            "content": "ype=\"bibr\" target=\"#b43\">Yao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Lee et al. 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lu, Fang, and Shi 2020)</ref>.</p><p>3 Learning to Pre-train:</p></di"
        },
        {
            "pid": "599c7980601a182cd2644cc9",
            "content": "target=\"#b30\">Qu, Bengio, and Tang 2019;</ref><ref type=\"bibr\" target=\"#b29\">Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b27\">Munkhdalai and Yu 2017)</ref>. Empirically, these GNNs have achieved ion over tasks, while model-based methods <ref type=\"bibr\" target=\"#b32\">(Santoro et al. 2016;</ref><ref type=\"bibr\" target=\"#b27\">Munkhdalai and Yu 2017)</ref> aim to design an architecture or traini"
        },
        {
            "pid": "5736977f6e3b12023e66632b",
            "content": ""
        },
        {
            "pid": "5cd7fa07ced107d4c65bf2af",
            "content": "ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017)</ref>, recommendation systems <ref type=\"bibr\" target=\"#b11\">(Fan et al. 2019;</ref><ref type=\"bibr\" target=\"#b44\">Ying et al. 201"
        },
        {
            "pid": "5f03f3b611dc830562232019",
            "content": "ype=\"bibr\" target=\"#b43\">Yao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Lee et al. 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lu, Fang, and Shi 2020)</ref>.</p><p>3 Learning to Pre-train:</p></di"
        },
        {
            "pid": null,
            "content": ")</ref>. Originally proposed <ref type=\"bibr\" target=\"#b26\">(Marco, Gabriele, and Franco 2005;</ref><ref type=\"bibr\" target=\"#b33\">Scarselli et al. 2008</ref>) as a framework of utilizing neural netwo"
        },
        {
            "pid": null,
            "content": "ge across similar learning tasks, so that the learned knowledge can be quickly adapted to new tasks <ref type=\"bibr\" target=\"#b41\">(Vilalta and Drissi 2002;</ref><ref type=\"bibr\" target=\"#b38\">Vanscho"
        },
        {
            "pid": "5db929de47c8f766461fb92c",
            "content": "\" target=\"#b39\">Velickovic et al. 2018;</ref><ref type=\"bibr\" target=\"#b45\">Ying et al. 2018b;</ref><ref type=\"bibr\" target=\"#b15\">Hasanzadeh et al. 2019;</ref><ref type=\"bibr\" target=\"#b30\">Qu, Bengi"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf312f1",
            "content": "ghborhoods on the graph, naturally capturing both graph structures as well as node or edge features <ref type=\"bibr\" target=\"#b47\">(Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b42\">Wu et bibr\" target=\"#b42\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b2\">Battaglia et al. 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zhou et"
        },
        {
            "pid": null,
            "content": "tation with just a few examples <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017;</ref><ref type=\"bibr\" target=\"#b43\">Yao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Lee et al. 2019;"
        },
        {
            "pid": "58d82fd2d649053542fd75d8",
            "content": "rk</head><p>GNNs have received significant attention due to the prevalence of graph-structured data <ref type=\"bibr\" target=\"#b4\">(Bronstein et al. 2017)</ref>. Originally proposed <ref type=\"bibr\" ta"
        },
        {
            "pid": "5a260c8117c44a4ba8a30f54",
            "content": "(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39\">Velickovic et al. 2018;</ref><ref type=\"bibr\" target=\"#b45\">Ying et a hmed, and Kutzkov 2016;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39\">Velickovic et al. 2018;</ref><ref type=\"bibr\" target=\"#b0\">Abu-El-Hai 017)</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b39\">(Velickovic et al. 2018)</ref> and <ref type=\"bibr\">GIN (Xu et al. 20"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "aggregation schemes have been proposed <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39 target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\">Niepert, Ahmed, and Kutzkov 2016;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39 , such as node and graph classification <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017)</ref>, recommendation systems <ref upervised objective of predicting the link between u and v <ref type=\"bibr\">(Tang et al. 2015;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017)</ref>, as follows.</p><formula xml r self-supervised or unsupervised baselines: (1) the original Edge Prediction (denoted by EdgePred) <ref type=\"bibr\" target=\"#b14\">(Hamilton, Ying, and Leskovec 2017)</ref> to predict the connectivity architectures, namely, GCN <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017)</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type=\"bibr\" targe"
        },
        {
            "pid": "5736977f6e3b12023e66632b",
            "content": ""
        }
    ],
    "5f1ff7ea91e011d50a621ab3": [
        {
            "pid": "5ecbc5829fced0a24b4e836e",
            "content": "o make utmost of the multi-scale features, the full-scale skip connections are designed in U-Net 3+ <ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, the design philosophy of full-scale skip connect"
        },
        {
            "pid": "5b8c9f5317c44af36f8b775c",
            "content": "ile feature maps generated by the decoder contain high-level and coarse-gained semantic information <ref type=\"bibr\" target=\"#b14\">[15]</ref>. And skip connections, which combine the low-level and hig ive method to boost the semantic extraction ability of encoder-decoder frameworks.</p><p>In U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>, plain skip connections are substituted by nested and dens =\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The major C. Duan is with the State Key Laboratory of Inf =\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+, the remaini"
        },
        {
            "pid": "5de0e36ddf1a9c0c415ca22b",
            "content": "ts of three convolutions, which just cause finite increasing in additional computational complexity <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The effectiveness of ACB has been verified in the fields [17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref> should be robust to rotation and renders consistent results in different rotations. As reported in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, different asymmetric convolutions are robust with differe network.</p><p>Based on above-mentioned insight, we modify the asymmetric convolutions proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> and design an asymmetric convolution block (ACB) to captur"
        },
        {
            "pid": "5550411745ce0a409eb38760",
            "content": "erarchical features from images, and have dramatically influenced the field of computer vision (CV) <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks"
        },
        {
            "pid": "53e9b97cb7602d970456d499",
            "content": "<ref type=\"bibr\" target=\"#b5\">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">[7]</ref> and random forest (RF) <ref type=\"bibr\" target=\"#b7\">[8]</re"
        },
        {
            "pid": "5550411745ce0a409eb38760",
            "content": "erarchical features from images, and have dramatically influenced the field of computer vision (CV) <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks"
        },
        {
            "pid": null,
            "content": "as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we design the channel attention block (CAB) to reweightin"
        },
        {
            "pid": "53e9a37ab7602d9702c8b25b",
            "content": "sign assorted classifiers from diverse perspectives, from orthodox methods such as distance measure <ref type=\"bibr\" target=\"#b5\">[6]</ref>, to advanced methods including support vector machine (SVM)"
        },
        {
            "pid": null,
            "content": "d economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote sensing community has tried to design a"
        },
        {
            "pid": null,
            "content": "he fields including image classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#"
        },
        {
            "pid": "58d82fced649053542fd6ea3",
            "content": "ibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref> =\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>"
        },
        {
            "pid": null,
            "content": "images, i.e., the assignment of assigning the precise category to every pixel contained in an image <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, plays a critical role"
        },
        {
            "pid": null,
            "content": "he fields including image classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#"
        },
        {
            "pid": null,
            "content": "images, i.e., the assignment of assigning the precise category to every pixel contained in an image <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, plays a critical role"
        },
        {
            "pid": null,
            "content": "source management, yield estimation, and economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote"
        },
        {
            "pid": null,
            "content": "as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we design the channel attention block (CAB) to reweightin"
        },
        {
            "pid": null,
            "content": "f application scenarios such as land resource management, yield estimation, and economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "cluding support vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">[7]</ref> and random forest (RF) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, the high dependency on hand-crafted visual feature"
        },
        {
            "pid": null,
            "content": "7]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In <ref type=\"bibr\" target=\"#b18\">[19]</ref>, only the co get=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In <ref type=\"bibr\" target=\"#b18\">[19]</ref>, only the convolutional layers of encoder are replaced by"
        },
        {
            "pid": "573696056e3b12023e51921c",
            "content": "\" target=\"#b9\">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Deep verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab rg/ns/1.0\"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab"
        },
        {
            "pid": null,
            "content": "ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r"
        },
        {
            "pid": null,
            "content": "images, i.e., the assignment of assigning the precise category to every pixel contained in an image <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, plays a critical role"
        },
        {
            "pid": "5a9cb66717c44a376ffb8b14",
            "content": "Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> have become the frequently-used schemes. Generally, feature =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">["
        },
        {
            "pid": "5e4588d393d709897c8f07dc",
            "content": "Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image Dataset (GID) <ref type=\"bibr\">[30]</ref>."
        },
        {
            "pid": null,
            "content": "source management, yield estimation, and economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote"
        },
        {
            "pid": "58d82fced649053542fd6ea3",
            "content": "ibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref> =\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>"
        },
        {
            "pid": "573698016e3b12023e6da477",
            "content": "ion, the encoder-decoder frameworks such as SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref the performance of proposed algorithm with SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Dee >To evaluate the effectiveness of MACU-Net, SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Dee"
        },
        {
            "pid": "5a9cb66717c44a376ffb8b14",
            "content": "Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> have become the frequently-used schemes. Generally, feature =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">["
        },
        {
            "pid": null,
            "content": "he fields including image classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#"
        },
        {
            "pid": null,
            "content": "source management, yield estimation, and economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote"
        },
        {
            "pid": null,
            "content": "d economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote sensing community has tried to design a"
        },
        {
            "pid": null,
            "content": "source management, yield estimation, and economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote"
        },
        {
            "pid": null,
            "content": "ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r"
        }
    ],
    "5fb24ee191e01186d3f5decc": [
        {
            "pid": "5e09a9d7df1a9c0c416afc7f",
            "content": "ss on non-autoregressive sequence generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein T maximum number of decoding steps is reached <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019)</ref>.<ref type=\"foot\" target=\"#foot_3\">5 n et al., 2018)</ref> or multi-pass decoding <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Gu et s by iteratively editing the outputs from previous iterations. Edit operations such as substitution <ref type=\"bibr\" target=\"#b20\">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type=\""
        },
        {
            "pid": "5ea8009091e0111d387ee87a",
            "content": "slation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints <ref type=\"bibr\" target=\"#b47\">(Susanto et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c. r\" target=\"#b16\">(Dinu et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Post and Vilar, 2018;</ref><ref type=\"bibr\" target=\"#b47\">Susanto et al., 2020)</ref>. Compared to <ref type=\"bibr\" target=\"#b3 constraints (Table <ref type=\"table\" target=\"#tab_6\">6</ref>). Consistent with previous findings by <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref>, incorporating soft constraints in LevT i s in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE. Enforcing hard constraints as in <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref> increases the term usage by +8-10% and im they help close the small gap to reach 100% term usage and do not 14 We use our implementations of <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref>'s technique for a more controlled compari b47\">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref> achieves higher BLEU than ours on the sma"
        },
        {
            "pid": "5a9cb65d17c44a376ffb80ef",
            "content": "h Repositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Stern et al., 2018)</ref> or multi-pass decoding <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad \"bibr\" target=\"#b22\">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad s widely used in nonautoregressive generation <ref type=\"bibr\" target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Gu et al., 2019"
        },
        {
            "pid": "5736960e6e3b12023e5216d4",
            "content": "ific content or style constraints <ref type=\"bibr\" target=\"#b0\">(Abu Sheikha and Inkpen, 2011;</ref><ref type=\"bibr\" target=\"#b29\">Mei et al., 2016)</ref>, or via segmentlevel \"side-constraints\" <ref"
        },
        {
            "pid": null,
            "content": "n as the KL divergence between the action distributions given by the model policy and by the oracle <ref type=\"bibr\" target=\"#b53\">(Welleck et al., 2019)</ref>:</p><formula xml:id=\"formula_8\">C(\u03c0 ; y,"
        },
        {
            "pid": "53e9b349b7602d9703e1dd00",
            "content": "e-level models that generate a bag of target words that is reordered to construct a target sentence <ref type=\"bibr\" target=\"#b4\">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref t"
        },
        {
            "pid": null,
            "content": "n addressed via partially parallel decoding <ref type=\"bibr\" target=\"#b51\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Stern et al., 2018)</ref> or multi-pass decoding <ref type=\"bibr\" tar"
        },
        {
            "pid": "57a4e91dac44365e35c9816f",
            "content": "orporate domain-specific knowledge and lexicons which is particularly helpful in low-resource cases <ref type=\"bibr\" target=\"#b2\">(Arthur et al., 2016;</ref><ref type=\"bibr\" target=\"#b48\">Tang et al.,"
        },
        {
            "pid": null,
            "content": "target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">van den Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Ma et al., 2019)</ref>. However, their output quality suffers due to y suffers due to the large decoding space and strong independence assumptions between target tokens <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Wang et al., 20"
        },
        {
            "pid": "53e9b0b2b7602d9703b235d4",
            "content": "in Gu et al. ( <ref type=\"formula\">2019</ref>))<ref type=\"foot\" target=\"#foot_7\">9</ref> and RIBES <ref type=\"bibr\" target=\"#b24\">(Isozaki et al., 2010)</ref>, which is more sensitive to word order d"
        },
        {
            "pid": "53e9a0a6b7602d970298fa6f",
            "content": "seful for interactive machine translation <ref type=\"bibr\" target=\"#b19\">(Foster et al., 2002;</ref><ref type=\"bibr\" target=\"#b5\">Barrachina et al., 2009)</ref> and domain adaptation <ref type=\"bibr\""
        },
        {
            "pid": "53e9b0b2b7602d9703b235d4",
            "content": "in Gu et al. ( <ref type=\"formula\">2019</ref>))<ref type=\"foot\" target=\"#foot_7\">9</ref> and RIBES <ref type=\"bibr\" target=\"#b24\">(Isozaki et al., 2010)</ref>, which is more sensitive to word order d"
        },
        {
            "pid": "5a73cb6a17c44a0b30358853",
            "content": "\" target=\"#b6\">(Bojar et al., 2014)</ref>, and English-Japanese (En-Ja) from WAT2017 Small-NMT Task <ref type=\"bibr\" target=\"#b30\">(Nakazawa et al., 2017)</ref>. We also evaluate EDITOR on the two En-"
        },
        {
            "pid": null,
            "content": "ps. <ref type=\"foot\" target=\"#foot_5\">7</ref>We select the best checkpoint based on validation BLEU <ref type=\"bibr\" target=\"#b35\">(Papineni et al., 2002)</ref>. All models are trained on 8 NVIDIA V10"
        },
        {
            "pid": "5843777eac44360f10841877",
            "content": "/ref><ref type=\"bibr\" target=\"#b29\">Mei et al., 2016)</ref>, or via segmentlevel \"side-constraints\" <ref type=\"bibr\" target=\"#b41\">(Sennrich et al., 2016a;</ref><ref type=\"bibr\" target=\"#b18\">Ficler a"
        },
        {
            "pid": "573696016e3b12023e5154b2",
            "content": ""
        },
        {
            "pid": "599c7ec3601a182cd28c525e",
            "content": "/ref>): Romanian-English (Ro-En) from WMT16 (Bojar et al., 2016), English-German (En-De) from WMT14 <ref type=\"bibr\" target=\"#b6\">(Bojar et al., 2014)</ref>, and English-Japanese (En-Ja) from WAT2017"
        },
        {
            "pid": null,
            "content": "standard for many sequence generation tasks <ref type=\"bibr\" target=\"#b12\">(Cho et al., 2014;</ref><ref type=\"bibr\" target=\"#b13\">Chorowski et al., 2015;</ref><ref type=\"bibr\" target=\"#b50\">Vinyals a"
        },
        {
            "pid": "599c7ec3601a182cd28c525e",
            "content": "/ref>): Romanian-English (Ro-En) from WMT16 (Bojar et al., 2016), English-German (En-De) from WMT14 <ref type=\"bibr\" target=\"#b6\">(Bojar et al., 2014)</ref>, and English-Japanese (En-Ja) from WAT2017"
        },
        {
            "pid": "5736960e6e3b12023e5216d4",
            "content": "ific content or style constraints <ref type=\"bibr\" target=\"#b0\">(Abu Sheikha and Inkpen, 2011;</ref><ref type=\"bibr\" target=\"#b29\">Mei et al., 2016)</ref>, or via segmentlevel \"side-constraints\" <ref"
        },
        {
            "pid": "57a4e91dac44365e35c9816f",
            "content": "orporate domain-specific knowledge and lexicons which is particularly helpful in low-resource cases <ref type=\"bibr\" target=\"#b2\">(Arthur et al., 2016;</ref><ref type=\"bibr\" target=\"#b48\">Tang et al.,"
        },
        {
            "pid": "53e9ac4eb7602d970361ef15",
            "content": "els and boldface the top scores among NAR models based on the paired bootstrap test with p &lt; 0.05<ref type=\"bibr\" target=\"#b14\">(Clark et al., 2011)</ref>. EDITOR decodes 6-7% faster than LevT on R"
        },
        {
            "pid": null,
            "content": "g, the reposition operation is defined to preserve the ability to use the Levenshtein edit distance <ref type=\"bibr\" target=\"#b27\">(Levenshtein, 1966)</ref> as an efficient oracle. We also introduce a nd insertion operations used in EDITOR are designed so that the Levenshtein edit distance algorithm <ref type=\"bibr\" target=\"#b27\">(Levenshtein, 1966)</ref> can be used as the oracle. The reposition o"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "58437722ac44360f1082ef6b",
            "content": ""
        },
        {
            "pid": null,
            "content": "nts better? We verify that this is not the case by computing the target word F1 binned by frequency <ref type=\"bibr\" target=\"#b31\">(Neubig et al., 2019)</ref>. Figure <ref type=\"figure\" target=\"#fig_3"
        },
        {
            "pid": null,
            "content": "ps. <ref type=\"foot\" target=\"#foot_5\">7</ref>We select the best checkpoint based on validation BLEU <ref type=\"bibr\" target=\"#b35\">(Papineni et al., 2002)</ref>. All models are trained on 8 NVIDIA V10"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "58437789ac44360f1084387e",
            "content": ""
        },
        {
            "pid": "599c794c601a182cd262dd0a",
            "content": "ions close to the cross-entropy loss are better suited to deep neural models than the squared error <ref type=\"bibr\" target=\"#b25\">(Leblond et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Cheng et a"
        },
        {
            "pid": "53e9ac4eb7602d970361ef15",
            "content": "els and boldface the top scores among NAR models based on the paired bootstrap test with p &lt; 0.05<ref type=\"bibr\" target=\"#b14\">(Clark et al., 2011)</ref>. EDITOR decodes 6-7% faster than LevT on R"
        },
        {
            "pid": null,
            "content": "tence <ref type=\"bibr\" target=\"#b4\">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref type=\"bibr\" target=\"#b17\">(Durrani et al., 2015;</ref><ref type=\"bibr\" target=\"#b44\">Stahlberg"
        },
        {
            "pid": "5736960e6e3b12023e5216d4",
            "content": "ific content or style constraints <ref type=\"bibr\" target=\"#b0\">(Abu Sheikha and Inkpen, 2011;</ref><ref type=\"bibr\" target=\"#b29\">Mei et al., 2016)</ref>, or via segmentlevel \"side-constraints\" <ref"
        }
    ],
    "5f1022a091e01168a7d6fc4f": [
        {
            "pid": "5c8dd8774895d9cbc6a78838",
            "content": "milar semantics.</p><p>Sharing a similar philosophy, there have been works on contrastive attention <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. MGCAM <ref type=\" attention <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. MGCAM <ref type=\"bibr\" target=\"#b30\">[31]</ref> uses the contrastive feature between persons and backgroun for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM <ref type=\"bibr\" target=\"#b30\">[31]</ref>, ACM automatically learns to focus on meaningful regions t"
        },
        {
            "pid": null,
            "content": "philosophy, there have been works on contrastive attention <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. MGCAM <ref type=\"bibr\" target=\"#b30\">[31]</ref> uses the c feature between persons and backgrounds, but it requires extra mask supervision for persons. C-MWP <ref type=\"bibr\" target=\"#b41\">[42]</ref> is a technique for generating more accurate localization m"
        },
        {
            "pid": null,
            "content": "est X-ray datasets <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, there has been a lon est X-ray datasets <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> showed that commonly"
        },
        {
            "pid": "5d0b00688607575390fc32b9",
            "content": "et=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": "5c8d7c6f4895d9cbc662c17c",
            "content": "chest X-ray tasks and natural image tasks.</p><p>Experimental Setting Following the previous study <ref type=\"bibr\" target=\"#b1\">[2]</ref> on multi-label classification with chest X-Rays, we mainly a"
        },
        {
            "pid": "5d0b00688607575390fc32b9",
            "content": "et=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "o incorporate group-wise operation. We replace all convolution operations with grouped convolutions <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, where the input a"
        },
        {
            "pid": null,
            "content": "est X-ray datasets <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, there has been a lon est X-ray datasets <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> showed that commonly"
        },
        {
            "pid": null,
            "content": "we model the way radiologists read X-rays? When radiologists read chest X-rays, they compare zones <ref type=\"bibr\" target=\"#b0\">[1]</ref>, paying close attention to any asymmetry between left and ri"
        },
        {
            "pid": null,
            "content": "opments of medical image recognition models have shown potentials for growth in diagnostic accuracy <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>With the recent presence of large-scale chest X-ray"
        },
        {
            "pid": null,
            "content": "ocalization tasks, we report the jackknife free-response receiver operating characteristic (JAFROC) <ref type=\"bibr\" target=\"#b4\">[5]</ref> for localization performances. JAFROC is a metric widely use"
        }
    ],
    "5f8d00a29e795ea21aee8001": [
        {
            "pid": "53e9ae11b7602d9703820c92",
            "content": "=\"#b3\">[5]</ref>, <ref type=\"bibr\" target=\"#b4\">[6]</ref>, <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr een these sub-problems. For example, coauthors, which are used as a strong evidence in many methods <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr Zhang\" in this example. More troubles may appear when multi-hop coauthorships are used as features <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b30\">[32]</ref>. For instance, \" icient, compared with the state-ofthe-art methods CE <ref type=\"bibr\" target=\"#b5\">[7]</ref>, GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref t d the redundant information, we only consider valid 2-hop coauthorship paths connecting two authors <ref type=\"bibr\" target=\"#b9\">[11]</ref>. Specifically, a valid 2-hop coauthorship path in G is an A fficiency of NDCC versus state-of-the-art methods CE <ref type=\"bibr\" target=\"#b5\">[7]</ref>, GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref t greedy agglomerative clustering method is used to merge the most similar clusters.</p><p>(2) GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref> is a graph-based method employing coauthorship only. Its si n baselines, only CE and GHOST analyze the time complexity <ref type=\"bibr\" target=\"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>. The time complexity of CE is O(|A (0) |k log |A (0) |), wh and unsupervised <ref type=\"bibr\" target=\"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr pe=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b37\">[39]</ref>, affinity propagation <ref type=\"bibr\" target=\"#b9\">[11]</ref> and Markov clustering <ref type=\"bibr\" target=\"#b39\">[41]</"
        },
        {
            "pid": "55323be745cec66b6f9dad3c",
            "content": "b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib scores are no less than t, and updates the network G (equally, the matrix W AP ) accordingly (lines <ref type=\"bibr\" target=\"#b13\">[15]</ref><ref type=\"bibr\" target=\"#b14\">[16]</ref>. It also needs to disambiguation can be divided into two classes: supervised <ref type=\"bibr\" target=\"#b2\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib >195,</ref><ref type=\"bibr\" target=\"#b17\">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, <ref type=\"bibr\" target=\"#b13\">(15,</ref><ref type=\"bibr\" target=\"#b6\">8)</ref> times faster than (C"
        },
        {
            "pid": null,
            "content": "to find more his/her publications. However, over 200 authors share the same name \"Wei Wang\" in DBLP <ref type=\"bibr\" target=\"#b17\">[19]</ref>, and the total number of their publications is over 2,000. \">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b27\">[29]</ref>, <ref type=\"bib ds CE <ref type=\"bibr\" target=\"#b5\">[7]</ref>, GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref type=\"bibr\" target=\"#b16\">[18]</ref>, and AM <re itional unigram model returns a low similarity score. In <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, the string level or character level tolerance is used whe thor, venue, title, and coauthor name). Given two authors i and j with the same name n, inspired by <ref type=\"bibr\" target=\"#b17\">[19]</ref>, we consider each pair and define the author similarity as number of authors of this name reaches the estimated one. Inspired by name ambiguity estimation in <ref type=\"bibr\" target=\"#b17\">[19]</ref>, we introduce a statistical method, which is based on the bility of a full name Algorithm 1: Collective Clustering is the joint probability of its components <ref type=\"bibr\" target=\"#b17\">[19]</ref>. Here we use the two-component names as an example to expl ds CE <ref type=\"bibr\" target=\"#b5\">[7]</ref>, GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref type=\"bibr\" target=\"#b16\">[18]</ref>, and AM <re CM (http://dl.acm.org) <ref type=\"bibr\" target=\"#b35\">[37]</ref> and DBLP (http://dblp.unitrier.de) <ref type=\"bibr\" target=\"#b17\">[19]</ref> for scholar name disambiguation. Different from previous w tering method is used to generate clusters of author references of the focused name.</p><p>(3) CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref> first groups the author references based on coauthorships \"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b26\">[28]</ref>, <ref type=\"bib sed methods use clustering, e.g., agglomerative clustering <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b37\">[39]</ref>, affinity propa <ref type=\"bibr\" target=\"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, web information <ref type=\"bibr\" target=\"#b17\">[19]</ref>, affiliation <ref type=\"bibr\" target=\"#b3\">[5]</ref>, and and applies to most digital libraries. It is also known that the usage of new evidence, e.g., wiki <ref type=\"bibr\" target=\"#b17\">[19]</ref>, abstracts <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref ctively. (b) NDCC is on average <ref type=\"bibr\" target=\"#b16\">(18,</ref><ref type=\"bibr\">195,</ref><ref type=\"bibr\" target=\"#b17\">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, <ref type=\" e baseline methods. (a) NDCC is <ref type=\"bibr\" target=\"#b16\">(18,</ref><ref type=\"bibr\">195,</ref><ref type=\"bibr\" target=\"#b17\">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, (15, 8) tim"
        },
        {
            "pid": "5736977a6e3b12023e6611f8",
            "content": "\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bib GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref type=\"bibr\" target=\"#b16\">[18]</ref>, and AM <ref type=\"bibr\" target=\"#b42\">[44]</ref>. Specifi o computer hardware. In this case, the traditional unigram model returns a low similarity score. In <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, the string lev GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref type=\"bibr\" target=\"#b16\">[18]</ref>, and AM <ref type=\"bibr\" target=\"#b42\">[44]</ref>, (2) the clusters. Then these clusters are merged by venue-based and title-based similarities.</p><p>(4) MIX <ref type=\"bibr\" target=\"#b16\">[18]</ref> is a supervised method. Random forests are used to calcula similarities, which works well when many features are available, such as abstract, and affiliation <ref type=\"bibr\" target=\"#b16\">[18]</ref>. While, in this study, we address the scholarly name disam 2\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bib type=\"bibr\" target=\"#b36\">[38]</ref> and random forests <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, which is then %) on ACM, and (13.11%, 23.31%, 8.47%, 50.37%, 9.86%) on DBLP, respectively. (b) NDCC is on average <ref type=\"bibr\" target=\"#b16\">(18,</ref><ref type=\"bibr\">195,</ref><ref type=\"bibr\" target=\"#b17\">1 ted here.</p><p>The results show that NDCC is more efficient than the baseline methods. (a) NDCC is <ref type=\"bibr\" target=\"#b16\">(18,</ref><ref type=\"bibr\">195,</ref><ref type=\"bibr\" target=\"#b17\">1"
        },
        {
            "pid": "53e9aa56b7602d97033c5d27",
            "content": "uation separately <ref type=\"bibr\" target=\"#b3\">[5]</ref>, <ref type=\"bibr\" target=\"#b4\">[6]</ref>, <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" <ref type=\"bibr\" target=\"#b42\">[44]</ref> and unsupervised <ref type=\"bibr\" target=\"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" the bibliography data is large. Unsupervised methods use clustering, e.g., agglomerative clustering <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\""
        },
        {
            "pid": "55323be745cec66b6f9dad3c",
            "content": "b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib scores are no less than t, and updates the network G (equally, the matrix W AP ) accordingly (lines <ref type=\"bibr\" target=\"#b13\">[15]</ref><ref type=\"bibr\" target=\"#b14\">[16]</ref>. It also needs to disambiguation can be divided into two classes: supervised <ref type=\"bibr\" target=\"#b2\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib >195,</ref><ref type=\"bibr\" target=\"#b17\">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, <ref type=\"bibr\" target=\"#b13\">(15,</ref><ref type=\"bibr\" target=\"#b6\">8)</ref> times faster than (C"
        },
        {
            "pid": "5d270c39275ded87f9541e29",
            "content": "stop condition, essentially a cluster estimation problem <ref type=\"bibr\" target=\"#b8\">[10]</ref>, <ref type=\"bibr\" target=\"#b24\">[26]</ref> Specifically, a name is considered as fully disambiguated"
        },
        {
            "pid": null,
            "content": "to find more his/her publications. However, over 200 authors share the same name \"Wei Wang\" in DBLP <ref type=\"bibr\" target=\"#b17\">[19]</ref>, and the total number of their publications is over 2,000. \">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b27\">[29]</ref>, <ref type=\"bib ds CE <ref type=\"bibr\" target=\"#b5\">[7]</ref>, GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref type=\"bibr\" target=\"#b16\">[18]</ref>, and AM <re itional unigram model returns a low similarity score. In <ref type=\"bibr\" target=\"#b16\">[18]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, the string level or character level tolerance is used whe thor, venue, title, and coauthor name). Given two authors i and j with the same name n, inspired by <ref type=\"bibr\" target=\"#b17\">[19]</ref>, we consider each pair and define the author similarity as number of authors of this name reaches the estimated one. Inspired by name ambiguity estimation in <ref type=\"bibr\" target=\"#b17\">[19]</ref>, we introduce a statistical method, which is based on the bility of a full name Algorithm 1: Collective Clustering is the joint probability of its components <ref type=\"bibr\" target=\"#b17\">[19]</ref>. Here we use the two-component names as an example to expl ds CE <ref type=\"bibr\" target=\"#b5\">[7]</ref>, GHOST <ref type=\"bibr\" target=\"#b9\">[11]</ref>, CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref>, MIX <ref type=\"bibr\" target=\"#b16\">[18]</ref>, and AM <re CM (http://dl.acm.org) <ref type=\"bibr\" target=\"#b35\">[37]</ref> and DBLP (http://dblp.unitrier.de) <ref type=\"bibr\" target=\"#b17\">[19]</ref> for scholar name disambiguation. Different from previous w tering method is used to generate clusters of author references of the focused name.</p><p>(3) CSLR <ref type=\"bibr\" target=\"#b17\">[19]</ref> first groups the author references based on coauthorships \"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b26\">[28]</ref>, <ref type=\"bib sed methods use clustering, e.g., agglomerative clustering <ref type=\"bibr\" target=\"#b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b37\">[39]</ref>, affinity propa <ref type=\"bibr\" target=\"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, web information <ref type=\"bibr\" target=\"#b17\">[19]</ref>, affiliation <ref type=\"bibr\" target=\"#b3\">[5]</ref>, and and applies to most digital libraries. It is also known that the usage of new evidence, e.g., wiki <ref type=\"bibr\" target=\"#b17\">[19]</ref>, abstracts <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref ctively. (b) NDCC is on average <ref type=\"bibr\" target=\"#b16\">(18,</ref><ref type=\"bibr\">195,</ref><ref type=\"bibr\" target=\"#b17\">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, <ref type=\" e baseline methods. (a) NDCC is <ref type=\"bibr\" target=\"#b16\">(18,</ref><ref type=\"bibr\">195,</ref><ref type=\"bibr\" target=\"#b17\">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, (15, 8) tim"
        },
        {
            "pid": null,
            "content": "\">[32]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bib guation methods <ref type=\"bibr\" target=\"#b5\">[7]</ref>, <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>. Note that using improper rules may include false positive \">[32]</ref>, <ref type=\"bibr\" target=\"#b31\">[33]</ref>, <ref type=\"bibr\" target=\"#b32\">[34]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref> , ACM (http://dl.acm.org) <ref type=\"bibr\" target=\"#b35\">[ bibr\" target=\"#b32\">[34]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref> , ACM (http://dl.acm.org) <ref type=\"bibr\" target=\"#b35\">[37]</ref> and DBLP (http://dblp.unitrier.de) <ref type=\"bibr\" target sambiguation, commonly used in name disambiguation tasks <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>. It contains 6,730 labeled papers of 110 author names. We we dismiss baselines relying on these external features <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>. Besides, some \">[29]</ref>, <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[34]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>, <ref type=\"bibr\" target=\"#b37\">[39]</ref>, <ref type=\"bib iki <ref type=\"bibr\" target=\"#b17\">[19]</ref>, abstracts <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>, and homepages <ref type=\"bibr\" target=\"#b35\">[37]</ref>, <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>, and homepages <ref type=\"bibr\" target=\"#b35\">[37]</ref>, usually improves the disambiguation performance. These me"
        },
        {
            "pid": null,
            "content": "b7\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b17\">[19]</ref>, <ref type=\"bibr\" target=\"#b26\">[28]</ref>, <ref type=\"bibr\" target=\"#b27\">[29]</ref>, <ref type=\"bib \"#b9\">[11]</ref> and Markov clustering <ref type=\"bibr\" target=\"#b39\">[41]</ref>, or topic modeling <ref type=\"bibr\" target=\"#b26\">[28]</ref>, <ref type=\"bibr\" target=\"#b27\">[29]</ref> to divide the s arget=\"#b17\">[19]</ref>, affiliation <ref type=\"bibr\" target=\"#b3\">[5]</ref>, and implicit evidence <ref type=\"bibr\" target=\"#b26\">[28]</ref>, <ref type=\"bibr\" target=\"#b27\">[29]</ref>. Citation infor"
        },
        {
            "pid": "599c78bd601a182cd25ebfd5",
            "content": "\">[29]</ref>, <ref type=\"bibr\" target=\"#b30\">[32]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>, <ref type=\"bibr\" target=\"#b35\">[37]</ref>, <ref type=\"bib"
        }
    ],
    "5f7d893591e011346ad27d16": [
        {
            "pid": "5dbab2523a55acea3c05b02b",
            "content": "bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Lewis et al., 2020</ref>), yet they are far from perfect. In generati eration framework, built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref>. As shown in Figure <ref type=\"figure\" tar et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al., 2019)</ref>. Our work uses BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref>, a state-of-the-art seq2seq model that off U card with 24 GB memory.</p><p>Model Sizes. Our generation model has the same architecture as BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref> with 406M parameters. The content planner"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "tone of many state-of-the-art models in various natural language understanding and generation tasks <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Liu et al., els to generate more rele-vant and coherent text. We first study a planning model trained from BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> to produce the initial content plan, which"
        },
        {
            "pid": null,
            "content": "This includes manipulating the syntax <ref type=\"bibr\" target=\"#b8\">(Du\u0161ek and Jur\u010d\u00ed\u010dek, 2016;</ref><ref type=\"bibr\" target=\"#b13\">Goyal and Durrett, 2020)</ref> and semantics <ref type=\"bibr\" target="
        },
        {
            "pid": null,
            "content": "ment has been studied in machine translation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Freitag et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Mansimov et"
        },
        {
            "pid": null,
            "content": "standing, it naturally benefits our method.</p><p>Decoding. We employ the nucleus sampling strategy <ref type=\"bibr\" target=\"#b16\">(Holtzman et al., 2019)</ref>, which is shown to yield superior outpu"
        },
        {
            "pid": null,
            "content": "arget=\"#b1\">Callaway, 2003)</ref> and have received dedicated research efforts in rulebased systems <ref type=\"bibr\" target=\"#b42\">(Reed et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Balakrishnan e"
        },
        {
            "pid": "53e9aa56b7602d97033c273c",
            "content": "tection and 71.0 for relation prediction on news articles from the annotated RST Discourse Treebank <ref type=\"bibr\" target=\"#b2\">(Carlson et al., 2001)</ref>. We run this trained model on our data fo"
        },
        {
            "pid": "5e09a9d7df1a9c0c416afc7f",
            "content": "s also used with masked language models to improve fluency of non-autoregressive generation outputs <ref type=\"bibr\" target=\"#b12\">(Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lawr fident tokens are replaced with <ref type=\"bibr\">[MASK]</ref>. Similar as the maskpredict algorithm <ref type=\"bibr\" target=\"#b12\">(Ghazvininejad et al., 2019)</ref>, we gradually reduce the number of"
        },
        {
            "pid": null,
            "content": "ases are identified as noun phrases and verb phrases that contain at least one topic signature word <ref type=\"bibr\" target=\"#b30\">(Lin and Hovy, 2000)</ref>, which is determined by a log-likelihood r"
        },
        {
            "pid": "5a260c8617c44a4ba8a31cdb",
            "content": "al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">See et al., 2019)</ref>, mention specified entities <ref type=\"bibr\" target=\"#b10\">(Fan et al., 2018)</ref>, or display a certain attribute <ref type=\"b"
        },
        {
            "pid": "53e9a17fb7602d9702a746db",
            "content": "r\" target=\"#b29\">(Lin, 2004)</ref>, measuring recall of the longest common subsequences; and METEOR <ref type=\"bibr\" target=\"#b24\">(Lavie and Agarwal, 2007)</ref>, which accounts for paraphrase. For o"
        },
        {
            "pid": "5a9cb65d17c44a376ffb80ef",
            "content": "-trained Transformer generators.</p><p>Iterative Refinement has been studied in machine translation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Freitag et al. inspired by iterative decoding designed for inference acceleration in non-autoregressive generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al"
        },
        {
            "pid": "53e9aa56b7602d97033c273c",
            "content": "tection and 71.0 for relation prediction on news articles from the annotated RST Discourse Treebank <ref type=\"bibr\" target=\"#b2\">(Carlson et al., 2001)</ref>. We run this trained model on our data fo"
        },
        {
            "pid": null,
            "content": "arget=\"#b22\">(Keskar et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Moryossef et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Hua and Wang, 2019)</ref>, yet those solutions require model architec <ref type=\"bibr\" target=\"#b35\">Moryossef et al., 2019;</ref><ref type=\"bibr\">Yao et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Hua and Wang, 2019)</ref>. However, it is still an open question to i m three distinct domains for multiparagraph-level text generation: (1) argument generation (ARGGEN) <ref type=\"bibr\" target=\"#b19\">(Hua et al., 2019)</ref>, to produce a counter-argument to refute a g rgument generation, based on a dataset collected from Reddit r/ChangeMyView (CMV) in our prior work <ref type=\"bibr\" target=\"#b19\">(Hua et al., 2019)</ref>. This dataset contains pairs of original pos"
        },
        {
            "pid": "5d9ed51b47c8f76646fc5d5d",
            "content": ", 2018)</ref>, or display a certain attribute <ref type=\"bibr\" target=\"#b18\">(Hu et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Luo et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Balakrishnan et"
        },
        {
            "pid": null,
            "content": "arget=\"#b22\">(Keskar et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Moryossef et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Hua and Wang, 2019)</ref>, yet those solutions require model architec <ref type=\"bibr\" target=\"#b35\">Moryossef et al., 2019;</ref><ref type=\"bibr\">Yao et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Hua and Wang, 2019)</ref>. However, it is still an open question to i m three distinct domains for multiparagraph-level text generation: (1) argument generation (ARGGEN) <ref type=\"bibr\" target=\"#b19\">(Hua et al., 2019)</ref>, to produce a counter-argument to refute a g rgument generation, based on a dataset collected from Reddit r/ChangeMyView (CMV) in our prior work <ref type=\"bibr\" target=\"#b19\">(Hua et al., 2019)</ref>. This dataset contains pairs of original pos"
        },
        {
            "pid": "5e15adcb3a55ac47ab5b09ee",
            "content": "/www.tei-c.org/ns/1.0\"><head n=\"4.2\">Implementation Details</head><p>Our code is written in PyTorch <ref type=\"bibr\" target=\"#b38\">(Paszke et al., 2019)</ref>. For fine-tuning, we adopt the standard l"
        },
        {
            "pid": null,
            "content": "standing, it naturally benefits our method.</p><p>Decoding. We employ the nucleus sampling strategy <ref type=\"bibr\" target=\"#b16\">(Holtzman et al., 2019)</ref>, which is shown to yield superior outpu"
        },
        {
            "pid": "5a260c8617c44a4ba8a31cdb",
            "content": "al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">See et al., 2019)</ref>, mention specified entities <ref type=\"bibr\" target=\"#b10\">(Fan et al., 2018)</ref>, or display a certain attribute <ref type=\"b"
        },
        {
            "pid": null,
            "content": "ourse markers are crucial for coherence <ref type=\"bibr\" target=\"#b14\">(Grote and Stede, 1998;</ref><ref type=\"bibr\" target=\"#b1\">Callaway, 2003)</ref> and have received dedicated research efforts in"
        },
        {
            "pid": null,
            "content": "ourse markers are crucial for coherence <ref type=\"bibr\" target=\"#b14\">(Grote and Stede, 1998;</ref><ref type=\"bibr\" target=\"#b1\">Callaway, 2003)</ref> and have received dedicated research efforts in"
        },
        {
            "pid": "5e09a9d7df1a9c0c416afc7f",
            "content": "s also used with masked language models to improve fluency of non-autoregressive generation outputs <ref type=\"bibr\" target=\"#b12\">(Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lawr fident tokens are replaced with <ref type=\"bibr\">[MASK]</ref>. Similar as the maskpredict algorithm <ref type=\"bibr\" target=\"#b12\">(Ghazvininejad et al., 2019)</ref>, we gradually reduce the number of"
        }
    ],
    "5f33bcf791e011861cfa0fd7": [
        {
            "pid": "5bdc316717c44a1f58a071ff",
            "content": "cal works include XLA <ref type=\"bibr\" target=\"#b8\">[9]</ref> (applicable to training as well), TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Glow <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Tensor Com gebra operations and calls into backend-specific libraries for execution on different backends. TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref> is an end-to-end compiler framework with Halide at the core"
        },
        {
            "pid": "53e9b0b2b7602d9703b20db9",
            "content": "wledge of hardware and associated parallel programming models. Typical DSL compilers include Halide <ref type=\"bibr\" target=\"#b17\">[18]</ref>, DLVM <ref type=\"bibr\" target=\"#b18\">[19]</ref>, Diesel <r"
        },
        {
            "pid": null,
            "content": "//www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Convolutional neural network (CNN) models <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": "5c8d14174895d9cbc6394f9c",
            "content": "\" target=\"#b10\">[11]</ref>, Tensor Comprehensions <ref type=\"bibr\" target=\"#b11\">[12]</ref>, nGraph <ref type=\"bibr\" target=\"#b12\">[13]</ref>, OpenVINO <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and T VM and aims to optimize CNN inference on CPUs by taking advantage of wide SIMD instructions. nGraph <ref type=\"bibr\" target=\"#b12\">[13]</ref> adopts a similar workflow to TVM, but was further extended"
        },
        {
            "pid": "5b3d98cc17c44a510f80212a",
            "content": "compiles to executable codes on a specific target device. This work was further enhanced by AutoTVM <ref type=\"bibr\" target=\"#b30\">[31]</ref> to enable automatic optimization of tensor operators. Comp"
        },
        {
            "pid": null,
            "content": "e gradient update per data sample, PPO enables training with mini-batch updates. In addition, RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> is used to implement our search algorithm (see Figure <ref ormula_9\">L V F t the square-error loss V \u03b8 (s t ) \u2212 V target t</formula><p>. Please refer to RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> for more implementation details.</p></div> <div xmlns=\"htt"
        },
        {
            "pid": null,
            "content": "or fusion ineffective at all. Taking GPU as an example, one effective approach is to write one CUDA <ref type=\"bibr\" target=\"#b22\">[23]</ref> kernel function for the fused operator and complete the wh s well. Nonetheless, our paper will merely investigate optimization techniques on CUDA-enabled GPUs <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p><p>Halide compiler Halide is a DSL compiler based on t"
        },
        {
            "pid": "5a260c8617c44a4ba8a32704",
            "content": "amming models. Typical DSL compilers include Halide <ref type=\"bibr\" target=\"#b17\">[18]</ref>, DLVM <ref type=\"bibr\" target=\"#b18\">[19]</ref>, Diesel <ref type=\"bibr\" target=\"#b19\">[20]</ref>, TIRAMIS"
        },
        {
            "pid": null,
            "content": "e gradient update per data sample, PPO enables training with mini-batch updates. In addition, RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> is used to implement our search algorithm (see Figure <ref ormula_9\">L V F t the square-error loss V \u03b8 (s t ) \u2212 V target t</formula><p>. Please refer to RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> for more implementation details.</p></div> <div xmlns=\"htt"
        },
        {
            "pid": null,
            "content": "target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref> usually have high computational cost subject to batch size,"
        },
        {
            "pid": "5736960a6e3b12023e51d64d",
            "content": "ion. PPO is a new family of policy gradient methods for RL. Unlike standard policy gradient methods <ref type=\"bibr\" target=\"#b26\">[27]</ref> performing one gradient update per data sample, PPO enable"
        }
    ],
    "5efcb8cd91e0115203245887": [
        {
            "pid": "58437722ac44360f1082efeb",
            "content": "get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> allow end-to-end differentable losses over data with arbitr for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type=\"bibr\" target=\"#b28\">[29]</ref>. For a complete introductions to the vast topic we refer i ider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type=\"bibr\" target=\"#b28\">[29]</ref> are simple yet effective <ref type=\"bibr\" target=\"#b50\">[5"
        },
        {
            "pid": "55a3e8f2c91b587b09666dae",
            "content": "br\" target=\"#b4\">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Let C \u2208 0, 1 n\u00d7k be the cluster assignment matrix and d b r iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type=\"bibr\" target=\"#b36\">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm"
        },
        {
            "pid": null,
            "content": "f the data as computational graph, allowing the information to propagate across the edges of graphs <ref type=\"bibr\" target=\"#b48\">[49]</ref>. When many real-wold systems are represented as graphs, th of research on graph neural networks and graph pooling methods.</p><p>Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "59ae3c262bbe271c4c71f46b",
            "content": "can be explained by the fact than a single node implicitly participates in many different clusters <ref type=\"bibr\" target=\"#b16\">[17]</ref>, e.g. a person in a social network is simultaneously conne graphs, as popular products are co-purchased with a lot of other items, so the effects discussed in <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> are prohibiting go"
        },
        {
            "pid": "5550453645ce0a409eb5495c",
            "content": "optimization process which does not allow to optimize the objective via gradient descent end-to-end <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Graclus <ref type=\"bibr\" target=\"#b12\">[13]</ref> DiffPoo"
        },
        {
            "pid": "5aed14a717c44a44381565cc",
            "content": "ised graph clustering is often an extremely useful end-goal in itself -whether for data exploration <ref type=\"bibr\" target=\"#b44\">[45]</ref>, visualization <ref type=\"bibr\" target=\"#b10\">[11,</ref><r"
        },
        {
            "pid": null,
            "content": "n <ref type=\"bibr\" target=\"#b44\">[45]</ref>, visualization <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, genomic feature discovery <ref type=\"bibr\" target=\"#b6\">[7"
        },
        {
            "pid": null,
            "content": "op-k <ref type=\"bibr\" target=\"#b19\">[20]</ref> SAG <ref type=\"bibr\" target=\"#b30\">[31]</ref> MinCut <ref type=\"bibr\" target=\"#b3\">[4]</ref> DMON</p><p>In this work, we take an ab initio approach to th ef> learn to sparsify the graph (select top-k edges for each node) with learned weights. MinCutPool <ref type=\"bibr\" target=\"#b3\">[4]</ref> pooling studies differentiable formulation of spectral clust ref type=\"bibr\" target=\"#b68\">[69]</ref> is not stable in terms of graph sparsity, while MinCutPool <ref type=\"bibr\" target=\"#b3\">[4]</ref> can not deal with uneven degree distribution.</p><p>Graph em work friends, forcing the algorithm to merge these communities together.</p><p>Recently, MinCutPool <ref type=\"bibr\" target=\"#b3\">[4]</ref> adapted the notion of the normalized cut to use as a regular s a trivial locally optimal solution that traps gradient-based optimization methods. MinCut pooling <ref type=\"bibr\" target=\"#b3\">[4]</ref> addresses this problem by adapting spectral orthogonality co tion with DGI and run k-means on the resulting representations.</p><p>\u2022 MinCutPool(graph, features) <ref type=\"bibr\" target=\"#b3\">[4]</ref> is a deep pooling method that we re-interpret as clustering."
        },
        {
            "pid": "5bdc31b817c44a1f58a0c628",
            "content": "resses this problem by adapting spectral orthogonality constraint in the form of soft-orthogonality <ref type=\"bibr\" target=\"#b1\">[2]</ref> regularization C C \u2212 I F . We notice that this term is overl"
        },
        {
            "pid": null,
            "content": "l optimization, and review some of their shortcomings.</p><p>Cut-based metrics. In his seminal work <ref type=\"bibr\" target=\"#b17\">[18]</ref>, Fiedler suggested that the second (Fiedler) eigenvector o"
        },
        {
            "pid": "573695ff6e3b12023e513303",
            "content": "11\">12]</ref>, genomic feature discovery <ref type=\"bibr\" target=\"#b6\">[7]</ref>, anomaly detection <ref type=\"bibr\" target=\"#b43\">[44]</ref>, or for many other use-cases discussed e.g. in <ref type=\""
        },
        {
            "pid": "53e9bd6ab7602d9704a09d4c",
            "content": "be balanced in terms of size. It is possible to get normalized partitions with the use of ratio cut <ref type=\"bibr\" target=\"#b62\">[63]</ref>, which normalizes the cut by the product of the number of"
        },
        {
            "pid": null,
            "content": "e structure. For graph-level metrics, we report average cluster conductance (as per definition from <ref type=\"bibr\" target=\"#b63\">[64]</ref>) and graph modularity <ref type=\"bibr\" target=\"#b37\">[38]<"
        },
        {
            "pid": "57a4e91dac44365e35c9830c",
            "content": "al Networks (GNNs) <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": null,
            "content": "s with the same degree distribution. While problems with the modularity metric have been identified <ref type=\"bibr\" target=\"#b21\">[22]</ref>, it remains one of the most commonly-used and eminently us"
        },
        {
            "pid": "59ae3c262bbe271c4c71f46b",
            "content": "can be explained by the fact than a single node implicitly participates in many different clusters <ref type=\"bibr\" target=\"#b16\">[17]</ref>, e.g. a person in a social network is simultaneously conne graphs, as popular products are co-purchased with a lot of other items, so the effects discussed in <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> are prohibiting go"
        },
        {
            "pid": null,
            "content": "arise from clusters <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b68\">69,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, for example through pooling or trainable attention over ed >[69]</ref>, or computing A 2 , like top-k pooling methods <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> .</p><p>\u2022 Node aggregation is crucial for our interpretatio fPool <ref type=\"bibr\" target=\"#b68\">[69]</ref> Top-k <ref type=\"bibr\" target=\"#b19\">[20]</ref> SAG <ref type=\"bibr\" target=\"#b30\">[31]</ref> MinCut <ref type=\"bibr\" target=\"#b3\">[4]</ref> DMON</p><p> loss to penalize soft assignments. Top-k <ref type=\"bibr\" target=\"#b19\">[20]</ref> and SAG pooling <ref type=\"bibr\" target=\"#b30\">[31]</ref> learn to sparsify the graph (select top-k edges for each n in terms of graph clustering. Both Top-k <ref type=\"bibr\" target=\"#b19\">[20]</ref> and SAG pooling <ref type=\"bibr\" target=\"#b30\">[31]</ref> only sparsify the graph and do not reduce the nodeset.</p>"
        },
        {
            "pid": "53e9b253b7602d9703cf4028",
            "content": "ialized deep learning architectures for dealing with graph-structured data, such as social networks <ref type=\"bibr\" target=\"#b46\">[47]</ref>, recommender graphs <ref type=\"bibr\" target=\"#b67\">[68]</r rary structure. They have been applied to an incredible range of applications, from social networks <ref type=\"bibr\" target=\"#b46\">[47]</ref>, to recommender systems <ref type=\"bibr\" target=\"#b67\">[68"
        },
        {
            "pid": null,
            "content": "n <ref type=\"bibr\" target=\"#b44\">[45]</ref>, visualization <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, genomic feature discovery <ref type=\"bibr\" target=\"#b6\">[7"
        },
        {
            "pid": null,
            "content": "shown to benefit from leveraging higher-order structural information that could arise from clusters <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b68\">69,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "53e99fd6b7602d97028b01ae",
            "content": "use the local Lloyd algorithm <ref type=\"bibr\" target=\"#b33\">[34]</ref> with the k-means++ seeding <ref type=\"bibr\" target=\"#b0\">[1]</ref> strategy.</p><p>\u2022 SBM <ref type=\"bibr\" target=\"#b41\">[42]</r"
        },
        {
            "pid": null,
            "content": "on synthetic graphs using an attributed, degree-corrected stochastic block model (ADC-SBM). The SBM <ref type=\"bibr\" target=\"#b52\">[53]</ref> plants a partition of clusters (\"blocks\") in a graph, and"
        }
    ],
    "5f7fdd328de39f0828397c88": [
        {
            "pid": "5e5e18a093d709897ce21291",
            "content": "where permutation equivariance is either learned from data <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> or obtained by design <ref type=\"bibr\" target=\"#b23\">[24]</ ]</ref>. With node features acting as identifiers, MPNN were shown to become universal in the limit <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which implies that they can solve the graph isomorphism t d evidence that the power of MPNN grows as a function of depth and width for certain graph problems <ref type=\"bibr\" target=\"#b22\">[23]</ref>, showing that (both anonymous and non-anonymous) MPNN cann ether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in <ref type=\"bibr\" target=\"#b22\">[23]</ref>) or with certain probability over the input distribution.< apacity is an effective generalization of the previously considered product between depth and width <ref type=\"bibr\" target=\"#b22\">[23]</ref>, being able to consolidate more involved properties, as we lower bounds rely on a new technique which renders them applicable not only to worst-case instances <ref type=\"bibr\" target=\"#b22\">[23]</ref>, but in expectation over the input distribution.</p><p>An previous theoretical findings that non-anonymous MPNN are universal and can solve graph isomorphism <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, as well as that the"
        },
        {
            "pid": null,
            "content": "(G a , G b , G c ) : G a \u2208 X a }|) log 2 s = c both fisom (B v,p ) log 2 s \u2212 log 2According to Otter<ref type=\"bibr\" target=\"#b50\">[51]</ref>, the number of unlabeled trees on v nodes grows liket(v) \u223c"
        },
        {
            "pid": "573696026e3b12023e5160cd",
            "content": "=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>Roughly tw"
        },
        {
            "pid": "5b67b4b917c44aac1c867dbc",
            "content": "(d) i : v i \u2208 V .</formula><p>For simplicity, it is here assumed that no graph pooling is employed <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, though the result"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b240",
            "content": "target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe mutation equivariant by design. Xu et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Morris et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> established the equivalence of anonymous MPNN to the 1st-ord en MPNN can also be analyzed by equivalence to the 1-WL test <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. For trees, the 1-WL test requires n iterations because there ng injective aggregation functions (i.e., of unbounded width <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>), the equivalence doe _0\">1c</ref>). Specifically, methods for isomorphism testing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> that compare graphs G a"
        },
        {
            "pid": null,
            "content": "=\"#b40\">[41]</ref><ref type=\"bibr\" target=\"#b41\">[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref><ref type=\"bibr\" target=\"#b43\">[44]</ref><ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p><p>Global sta"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b240",
            "content": "target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe mutation equivariant by design. Xu et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Morris et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> established the equivalence of anonymous MPNN to the 1st-ord en MPNN can also be analyzed by equivalence to the 1-WL test <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. For trees, the 1-WL test requires n iterations because there ng injective aggregation functions (i.e., of unbounded width <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>), the equivalence doe _0\">1c</ref>). Specifically, methods for isomorphism testing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> that compare graphs G a"
        },
        {
            "pid": "5ac1824c17c44a1fda913a60",
            "content": "studied subcases of isomorphism, such as subgraph freeness <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> or those focused on anonymous networks <ref type=\"bibr\">[3-"
        },
        {
            "pid": "5e5e18e693d709897ce3abeb",
            "content": "e=\"bibr\" target=\"#b8\">[9]</ref> and there is some evidence that it can facilitate logical reasoning <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Here, I will suppose that x ( ) 0 belongs to set S \u03b3 . Ad"
        },
        {
            "pid": null,
            "content": "/ref>, as well as the analysis of the power of particular architectures to compute graph properties <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and to distinguish"
        },
        {
            "pid": "5c8a11324895d9cbc6121c34",
            "content": "by Scarselli et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref>, MPNN has been extended to include edge <ref type=\"bibr\" target=\"#b7\">[8]</ref> and global features <ref type=\"bibr\" target=\"#b8\">[9]</ref>. ing able to consolidate more involved properties, as well as to characterize MPNN with global state <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target"
        }
    ],
    "5fe31b9491e01125d4b5b744": [
        {
            "pid": "5d06e47bda562926acc4020c",
            "content": "machine translation <ref type=\"bibr\" target=\"#b10\">(Gu et al. 2018)</ref>, task-oriented dialogues <ref type=\"bibr\" target=\"#b24\">(Qian and Yu 2019;</ref><ref type=\"bibr\" target=\"#b20\">Mi et al. 2019"
        },
        {
            "pid": "56d8b2f0dabfae2eeef0bac2",
            "content": ""
        },
        {
            "pid": null,
            "content": "pe=\"bibr\" target=\"#b16\">(Lin et al. 2019;</ref><ref type=\"bibr\" target=\"#b33\">Wei et al. 2018;</ref><ref type=\"bibr\" target=\"#b35\">Xu et al. 2019)</ref>. It has a significant potential to simplify the ialogue policy learning <ref type=\"bibr\" target=\"#b33\">(Wei et al. 2018)</ref>, dialogue management <ref type=\"bibr\" target=\"#b35\">(Xu et al. 2019)</ref>, and make promising progress to build a satisf ei et al. (2018)</ref> proposed to learn dialogue policy with RL to facilitate automatic diagnosis. <ref type=\"bibr\" target=\"#b35\">Xu et al. (2019)</ref> incorporated the knowledge inference into dial"
        },
        {
            "pid": "5b1642388fbcbf6e5a9b55a1",
            "content": "ial to simplify the diagnostic process and relieve the cost of collecting information from patients <ref type=\"bibr\" target=\"#b12\">(Kao, Tang, and Chang 2018)</ref>. Moreover, preliminary diagnosis re"
        },
        {
            "pid": "619b93ed1c45e57ce9c7d1f2",
            "content": "high-resource diseases to others of data scarcity. Besides, existing knowledge-grounded approaches <ref type=\"bibr\" target=\"#b17\">(Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b15\">Lian et al. 201 nes. We first compare our base dialogue model MGR with two knowledge-grounded dialogue systems, NKD <ref type=\"bibr\" target=\"#b17\">(Liu et al. 2018)</ref> and POKS <ref type=\"bibr\" target=\"#b15\">(Lian test it directly on target diseases. We test the above three base models and denote them as PT-NKD <ref type=\"bibr\" target=\"#b17\">(Liu et al. 2018)</ref>, PT-POKS <ref type=\"bibr\" target=\"#b15\">(Lian"
        },
        {
            "pid": null,
            "content": "\"bibr\" target=\"#b38\">(Zhou et al. 2018;</ref><ref type=\"bibr\" target=\"#b36\">Zhang et al. 2020;</ref><ref type=\"bibr\" target=\"#b21\">Moon et al. 2019)</ref> or retrieved from unstructured documents <ref"
        },
        {
            "pid": null,
            "content": "e-layer LSTM <ref type=\"bibr\" target=\"#b11\">(Hochreiter and Schmidhuber 1997)</ref>, and use pkuseg <ref type=\"bibr\" target=\"#b19\">(Luo et al. 2019</ref>) toolkit to segment Chinese words. We set both"
        },
        {
            "pid": null,
            "content": "br\" target=\"#b3\">(Devlin et al. 2019;</ref><ref type=\"bibr\" target=\"#b25\">Radford et al. 2019;</ref><ref type=\"bibr\" target=\"#b29\">Song et al. 2019</ref>) over unsupervised corpora have achieved signi"
        },
        {
            "pid": null,
            "content": "e-layer LSTM <ref type=\"bibr\" target=\"#b11\">(Hochreiter and Schmidhuber 1997)</ref>, and use pkuseg <ref type=\"bibr\" target=\"#b19\">(Luo et al. 2019</ref>) toolkit to segment Chinese words. We set both"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "enario directly in absence of medical knowledge. Recently, large-scale pre-training language models <ref type=\"bibr\" target=\"#b3\">(Devlin et al. 2019;</ref><ref type=\"bibr\" target=\"#b25\">Radford et al"
        },
        {
            "pid": null,
            "content": "br\" target=\"#b3\">(Devlin et al. 2019;</ref><ref type=\"bibr\" target=\"#b25\">Radford et al. 2019;</ref><ref type=\"bibr\" target=\"#b29\">Song et al. 2019</ref>) over unsupervised corpora have achieved signi"
        }
    ],
    "5eff040a91e011ea6db8de11": [
        {
            "pid": "5daaea4247c8f766460f928f",
            "content": "of a single device.</p><p>Recently, pipeline parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> has been proposed as a promising approach for training lar een those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This manner inserts mini-batches into pipeline continuous lism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type=\"bibr\" target=\"#b11\">[12]</ref> is not able to be applied to synchronous training effectiv evice assignment affects communication efficiency and computing resource utilization. Previous work <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses hierarchical planning and works well for asynchronous D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type=\"bibr\" target=\"#b11\">[12]</ref> (for asynchronous training) and torchgpipe <ref type=\"bibr it the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type=\"bibr\" target=\"#b11\">[12]</ref> (Fig. <ref type=\"figure\" target=\"#fig_6\">8(b)</ref>) is no"
        },
        {
            "pid": "5e09ab80df1a9c0c416de108",
            "content": "optimizing pipeline parallelism for synchronous training <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. This approach requires necessary gradients synchronizatio llelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib"
        },
        {
            "pid": "5550415945ce0a409eb3a820",
            "content": "pe=\"bibr\" target=\"#b26\">[27]</ref>, SQuAD2.0 <ref type=\"bibr\" target=\"#b27\">[28]</ref> and ImageNet <ref type=\"bibr\" target=\"#b28\">[29]</ref>, respectively. Hardware Configurations.   We train GNMT-16"
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": "While the performance issue can be alleviated by optimizations such as local gradients accumulation <ref type=\"bibr\" target=\"#b4\">[5]</ref>- <ref type=\"bibr\" target=\"#b6\">[7]</ref> or computation and II. MOTIVATION AND DAPPLE OVERVIEW</head><p>We consider pipelines training only if DP optimizations <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t ref>   data parallelism. As a commonly used performance optimization method, gradients accumulation <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t"
        },
        {
            "pid": "5aed14d617c44a4438158cff",
            "content": "LP <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Internet scale E-commerce search/recommendation systems <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>) have billions of p"
        },
        {
            "pid": "5aed14d617c44a4438158cff",
            "content": "LP <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Internet scale E-commerce search/recommendation systems <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>) have billions of p"
        },
        {
            "pid": "5b8c9f4a17c44af36f8b71fa",
            "content": "ategy to assign operations in a DNN to different devices <ref type=\"bibr\" target=\"#b55\">[57]</ref>- <ref type=\"bibr\" target=\"#b57\">[59]</ref> to further improve system efficiency.</p></div> <div xmlns"
        },
        {
            "pid": "5843774bac44360f108397c4",
            "content": ">, Internet scale E-commerce search/recommendation systems <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>) have billions of parameters, demanding tens to hundreds of"
        },
        {
            "pid": "5c04967517c44a2c74708b7e",
            "content": "cale of the model exceeds the memory limit of a single device.</p><p>Recently, pipeline parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> has been propose rally falls into two categories. One is on optimizing pipeline parallelism for synchronous training <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. This approach r rbates the already critical memory consumption issue. As for synchronous training, current approach <ref type=\"bibr\" target=\"#b9\">[10]</ref> still requires notable memory consumption, because no backw rresponding BW's usage later) while the devices are busy with FW of some other micro-batches. GPipe <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposes to discard some intermediate results to free the m et=\"#b11\">[12]</ref> is not able to be applied to synchronous training effectively. Some other work <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> relies on empiri common approach to keep all stages busy is to split the training batch into multiple micro-batches <ref type=\"bibr\" target=\"#b9\">[10]</ref>. These micro-batches are scheduled in the pipelined manner iciency as average GPU utilization of all devices in the pipeline. The pipeline efficiency is 1 1+P <ref type=\"bibr\" target=\"#b9\">[10]</ref>, where P = (1+\u03b1) (S\u22121)</p></div> <div xmlns=\"http://www.tei xed \u03b1 and M ,.</p><p>There are efforts to improve synchronous pipelines with micro-batch scheduling <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which suffer from two issues.</p><p>(1) Extra memory consu 1.0\"><head>A. Limitations of GPipe Schedule</head><p>To improve pipeline training efficiency, GPipe <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposes to split global batch into multiple micro-batches ning) and torchgpipe <ref type=\"bibr\" target=\"#b22\">[23]</ref>, a community implementation of GPipe <ref type=\"bibr\" target=\"#b9\">[10]</ref> which uses \"Block Partitioning of Sequences\" <ref type=\"bib odels among GPUs to mitigate communication overhead and memory bottlenecks for distributed training <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr tion between layers, namely, pipeline parallelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr bibr\" target=\"#b53\">[55]</ref> has been recently proposed to train DNN in a pipelined manner. GPipe <ref type=\"bibr\" target=\"#b9\">[10]</ref> explores synchronous pipeline approach to train large model"
        },
        {
            "pid": "5e09ab80df1a9c0c416de108",
            "content": "optimizing pipeline parallelism for synchronous training <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. This approach requires necessary gradients synchronizatio llelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib"
        },
        {
            "pid": "5b8c9f4a17c44af36f8b71fa",
            "content": "ategy to assign operations in a DNN to different devices <ref type=\"bibr\" target=\"#b55\">[57]</ref>- <ref type=\"bibr\" target=\"#b57\">[59]</ref> to further improve system efficiency.</p></div> <div xmlns"
        }
    ],
    "5f6c6a6391e0119671e8583c": [
        {
            "pid": "5daaea4147c8f766460f9278",
            "content": "eam generates about 1.2TB intermediate data to count 4-motif on the MiCo graph with 1 million edges <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>Recently, specialized systems have been developed f thms in these specialized pattern matching systems can be described with nested loops, and AutoMine <ref type=\"bibr\" target=\"#b17\">[18]</ref> and GraphZero <ref type=\"bibr\" target=\"#b11\">[12]</ref> re ate-of-the-art singlemachine pattern matching systems. GraphZero is an upgraded version of AutoMine <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and it outperforms AutoMine by up to 40\u00d7. Fractal is a JV ef>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Automine <ref type=\"bibr\" target=\"#b17\">[18]</ref> is built upon a set-based representation and uses compilat"
        },
        {
            "pid": "53e9b839b7602d97043f8430",
            "content": "ODUCTION</head><p>Graph data and algorithms are widely used in many fields, such as social networks <ref type=\"bibr\" target=\"#b0\">[1]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and there are n! possible relative magnitudes of n vertices in an embedding (e.g., when n = 5, they are <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target"
        },
        {
            "pid": null,
            "content": "\">[19]</ref>- <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Automine <ref"
        },
        {
            "pid": "58437707ac44360f1082b439",
            "content": "rms several JVM-based specialized algorithms (MRSUB <ref type=\"bibr\" target=\"#b34\">[35]</ref>, SEED <ref type=\"bibr\" target=\"#b35\">[36]</ref> and QKCount <ref type=\"bibr\" target=\"#b36\">[37]</ref>) and"
        },
        {
            "pid": "53e9b44ab7602d9703f424a7",
            "content": "r even several days to mine a pattern with a size of 6 on an unlabeled graph with millions of edges <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>Recently, researchers have proposed several general M Co-authorship Patents <ref type=\"bibr\" target=\"#b26\">[27]</ref> 3.8M 16.5M US Patents LiveJournal <ref type=\"bibr\" target=\"#b12\">[13]</ref> 4.0M 34.7M Social network Orkut <ref type=\"bibr\" target=\"#"
        },
        {
            "pid": "58437707ac44360f1082b439",
            "content": "rms several JVM-based specialized algorithms (MRSUB <ref type=\"bibr\" target=\"#b34\">[35]</ref>, SEED <ref type=\"bibr\" target=\"#b35\">[36]</ref> and QKCount <ref type=\"bibr\" target=\"#b36\">[37]</ref>) and"
        },
        {
            "pid": null,
            "content": "ialized systems have been developed for pattern matching <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>- <ref type=\"bibr\" target=\"#b20\">[21]</ref>, approximate pa thms, their performance is relatively poor. Specialized pattern matching systems have been proposed <ref type=\"bibr\" target=\"#b18\">[19]</ref>- <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib"
        },
        {
            "pid": "53e9ac82b7602d970365a664",
            "content": ""
        },
        {
            "pid": "5e8da0bf91e011f2de583740",
            "content": "up theory to break the inherent symmetry in patterns and eliminate redundant computation. Peregrine <ref type=\"bibr\" target=\"#b40\">[41]</ref> is another DFS-based system which provides a pattern-based"
        },
        {
            "pid": "53e99f56b7602d9702824ecc",
            "content": "Description Wiki-Vote <ref type=\"bibr\" target=\"#b30\">[31]</ref> 7.1K 100.8K Wiki Editor Voting MiCo <ref type=\"bibr\" target=\"#b31\">[32]</ref> 96.6K 1.1M Co-authorship Patents <ref type=\"bibr\" target=\""
        },
        {
            "pid": "58437785ac44360f10842e1b",
            "content": ">) and general-purpose systems (Arabesque <ref type=\"bibr\" target=\"#b13\">[14]</ref> and GraphFrames <ref type=\"bibr\" target=\"#b37\">[38]</ref>) by orders of magnitudes. Since GraphZero is not released,"
        }
    ],
    "5f2e715791e011ecdac9c1bc": [
        {
            "pid": "5b67b45517c44aac1c8607cb",
            "content": "=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> or for member profiles (personalization) <ref type=\"bibr\" target=\"#b9\">[10]</ref>. It requires a huge amount of hard disk space to store the"
        },
        {
            "pid": "53e9affab7602d9703a4f291",
            "content": "ortunity to understand the deep semantics of natural language data through embedding representation <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Moreover, to enhance contextual modeling, contextual embe dels. Representation based models learn independent embeddings for the query and the document. DSSM <ref type=\"bibr\" target=\"#b12\">[13]</ref> averages the word embeddings as the query/document embeddi t BERT-based ranking model for industry use cases, we propose to use representation based structure <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Instead of applyi ents. Both source and target could have multiple fields, which is different from most previous work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta embedding layer. It is worth noting that we use word tokens instead of triletters as in prior work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, since the latter"
        },
        {
            "pid": null,
            "content": "iple target scores as input. DeText provides the flexibility of pointwise, pairwise or listwise LTR <ref type=\"bibr\" target=\"#b2\">[3]</ref>, as well as Lambda rank <ref type=\"bibr\" target=\"#b3\">[4]</r"
        },
        {
            "pid": "53e9affab7602d9703a4f291",
            "content": "ortunity to understand the deep semantics of natural language data through embedding representation <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Moreover, to enhance contextual modeling, contextual embe dels. Representation based models learn independent embeddings for the query and the document. DSSM <ref type=\"bibr\" target=\"#b12\">[13]</ref> averages the word embeddings as the query/document embeddi t BERT-based ranking model for industry use cases, we propose to use representation based structure <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Instead of applyi ents. Both source and target could have multiple fields, which is different from most previous work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta embedding layer. It is worth noting that we use word tokens instead of triletters as in prior work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, since the latter"
        },
        {
            "pid": "573696046e3b12023e517cb1",
            "content": "rg/ns/1.0\"><head n=\"6.1.3\">Baseline Models.</head><p>The production models are trained with XGBoost <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The hyper-parameters (pairwise vs listwise, number of trees"
        },
        {
            "pid": "5550412045ce0a409eb38b4c",
            "content": "r lifts the computation (by an order of the character length of words). We follow the previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref> to generate the text embedding from word embedding matrix"
        },
        {
            "pid": "5550489045ce0a409eb6f76a",
            "content": "se cases, we propose to use representation based structure <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Instead of applying BERT to a concatenated string of query ry/document embeddings. Following this work, CLSM/LSTM-RNN <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> encodes word order information using CNN <ref type=\"bibr\" t most previous work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, where only two fields (query and document) are available. we use word tokens instead of triletters as in prior work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, since the latter lifts the computation (by an order of the res are excluded. Note that DeText-CNN with a single target field is a special version of CLSM model<ref type=\"bibr\" target=\"#b25\">[26]</ref> that operates on words. To better understand the trade-off"
        },
        {
            "pid": "5c8c9fa54895d9cbc61354c1",
            "content": "side, and the document score is the cosine similarity score of the query/document embedding. NRM-F <ref type=\"bibr\" target=\"#b29\">[30]</ref> adds more fields in the document side and achieves better"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "query and document texts. BERT is then fine tuned with ranking loss. The inherent transformer layer <ref type=\"bibr\" target=\"#b26\">[27]</ref> in BERT allows direct context sharing between query words ince the query string and document string are concatenated as one sentence, where transformer layer <ref type=\"bibr\" target=\"#b26\">[27]</ref> compares every word pair in that sentence.</p><p>In experi"
        },
        {
            "pid": "5a9cb60d17c44a376ffb3c4c",
            "content": "rate many matching signals in documents. This approach, in the category of interaction based models <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ wise word similarity score histogram. K-NRM <ref type=\"bibr\" target=\"#b27\">[28]</ref> and Conv-KNRM <ref type=\"bibr\" target=\"#b6\">[7]</ref> extended DRMM by kernel pooling and pairwise ngram similarit"
        },
        {
            "pid": "57aa28de0a3ac518da98974b",
            "content": "ng work uses embedding pre-computing, either for documents <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> or for member profiles (personalization) <ref type=\"bibr\" t"
        }
    ],
    "5f02f25491e011ee5e0258e0": [
        {
            "pid": "5de0e035df1a9c0c415c6700",
            "content": "<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or feature matrix <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>) are not compatibl denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplac ing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes a symmetric graph convolutional autoencoder recov"
        },
        {
            "pid": null,
            "content": "profileDesc> \t</teiHeader> \t<text xml:lang=\"en\"> \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"> <ref type=\"bibr\" target=\"#b14\">[15]</ref><p>. The components we argue about are marked in red blocks methods, most of them are based on graph autoencoder (GAE) and variational graph autoencoder (VGAE) <ref type=\"bibr\" target=\"#b14\">[15]</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_0\">1</ localized neighborhood structure. Graph autoencoder (GAE) and variational graph autoencoder (VGAE) <ref type=\"bibr\" target=\"#b14\">[15]</ref> learn node embeddings by using GCN as the encoder, then de tributed graph embedding methods, we include 5 baseline algorithms in our comparisons: GAE and VGAE <ref type=\"bibr\" target=\"#b14\">[15]</ref> combine graph convolutional networks with the (variational ig_0\"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The architecture of graph autoencoder<ref type=\"bibr\" target=\"#b14\">[15]</ref>. The components we argue about are marked in red blocks: E"
        },
        {
            "pid": "5550401245ce0a409eb32060",
            "content": "he learned node embeddings, we visualize the node representations in 2D space using t-SNE algorithm <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The figures are shown in Figure <ref type=\"figure\" target"
        },
        {
            "pid": "5d9edc8347c8f76646042a37",
            "content": "gure <ref type=\"figure\" target=\"#fig_0\">1</ref>) and an activation function. However, previous work <ref type=\"bibr\" target=\"#b34\">[35]</ref> demonstrates that the entanglement of the filters and weig"
        },
        {
            "pid": "53e9ac18b7602d97035d9131",
            "content": "tering and link prediction experiments on four widely used network datasets (Cora, Citeseer, Pubmed <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Wiki <ref type=\"bibr\" target=\"#b35\">[36]</ref>). Featu"
        },
        {
            "pid": "573698636e3b12023e728f15",
            "content": "rget=\"#b16\">[17]</ref>, assuming that similar nodes tend to cooccur in same sequences. Other models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "f type=\"bibr\" target=\"#b20\">[21]</ref>, matrix factorization <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar ere are several works make adjustments to encode structural and content information simultaneously. <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "53e998f6b7602d9702130ab8",
            "content": "on extensions that add feature-related regularization terms. <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> model features as latent variables in Bayesian networks.</p><"
        },
        {
            "pid": "5a260c8117c44a4ba8a30adf",
            "content": ">27,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> can be concluded by an encoder-decoder framework <ref type=\"bibr\" target=\"#b10\">[11]</ref>, while they differ from model structure and training objec"
        },
        {
            "pid": "573697086e3b12023e602359",
            "content": "rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, and random walks <ref type=\"bibr\" target=\"#b9\">[10,</ref>< on simultaneously. <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> are matrix factorization extensions that add feature-relate ly used network datasets (Cora, Citeseer, Pubmed <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Wiki <ref type=\"bibr\" target=\"#b35\">[36]</ref>). Features in Cora and Citeseer are binary word vectors, w ram on generated random walk paths on graphs.</p><p>(3) Methods using both features and graph. TADW <ref type=\"bibr\" target=\"#b35\">[36]</ref> interprets DeepWalk as matrix factorization and incorporat"
        },
        {
            "pid": "5736977f6e3b12023e66632b",
            "content": "imilar nodes tend to cooccur in same sequences. Other models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> can be concluded by"
        }
    ],
    "5f8d6be69fced0a24bbab01e": [
        {
            "pid": "599c7987601a182cd2648373",
            "content": "ts decrease along with neighbor hops increasing. Inspired by the architecture design of Transformer <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we leverage attention mechanism to learn the weight of so"
        },
        {
            "pid": "5c75711af56def9798751adb",
            "content": "c data, such as images <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, texts <ref type=\"bibr\" target=\"#b15\">[16]</ref> and user b"
        },
        {
            "pid": null,
            "content": "he disentangled embeddings in separate two stages. Inspired by the capsule neural network (CapsNet) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, some works replace scalar-valued neurons with vector-valu"
        },
        {
            "pid": "5d3c233f3a55acd386d4dde4",
            "content": "omogeneous network <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. Our work focus on learning disentangled representation in"
        },
        {
            "pid": "5d3ed25a275ded87f97deaab",
            "content": "Ns framework to obtain better user and item representations by aggregating neighborhood information <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" targ sponding semantic relations. Recently, some HIN based GNNs are proposed for information propagation <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "5db92a1147c8f766461ff79d",
            "content": "=\"bibr\" target=\"#b12\">13]</ref>, texts <ref type=\"bibr\" target=\"#b15\">[16]</ref> and user behaviors <ref type=\"bibr\" target=\"#b21\">[22]</ref>. For graph-structure data, GAT performs multi-head attenti"
        },
        {
            "pid": "58437722ac44360f1082efeb",
            "content": "neural network (GGNN) <ref type=\"bibr\" target=\"#b18\">[19]</ref>, graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, graph inductive representation learning (GraphSAGE) <ref ad><label></label><figDesc>GC-MC<ref type=\"bibr\" target=\"#b29\">[30]</ref> (C): This model adopts GCN<ref type=\"bibr\" target=\"#b16\">[17]</ref> encoders in useritem bipartite graph to generate the repre"
        },
        {
            "pid": null,
            "content": "ce equally, and can be seen as a special case of disentangled representation learning. PolyDeepwalk <ref type=\"bibr\" target=\"#b19\">[20]</ref> notices multiple aspects of nodes, but learns the disentan"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "GCN) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, graph inductive representation learning (GraphSAGE) <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph attention network (GAT) <ref type=\"bibr\" target=\"# on, so the padding would lead to both high space and time complexity. Followed by the previous work <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we sample fixed number of source nodes and pad zeros when s"
        },
        {
            "pid": "57aa28de0a3ac518da98974f",
            "content": "paths between users and items. Cao et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Zhang et al. <ref type=\"bibr\" target=\"#b41\">[42]</ref> jointly learn latent representations in CF as well as item"
        },
        {
            "pid": null,
            "content": "rpretable which can directly find applications in various fields with semantic data, such as images <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target"
        }
    ],
    "5f9a9af391e0114d7e7813ed": [
        {
            "pid": "5ede0553e06a4c1b26a8419c",
            "content": "icit data augmentation. Moreover, to supplement the input graph with more global information, MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposes to augment the input graph using graph diffusion. asure MI between input and representations of both nodes and edges without data augmentation; MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposes to learn both node-level and graph-level represen <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> in Table <ref type=\"table\" target=\"#tab_0\">1</ref>, where MI) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Furthermore, we report the performance obtained using a l"
        },
        {
            "pid": "5e3a92413a55ac054d0cdbf7",
            "content": "posed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposes two node-level contrastive objectives to directly the agreement of node embeddings across two corrupted views of the graph.</p><p>Following DGI, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref> employs two discriminators to directly measure MI between summary, we provide a brief comparison between the  <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> in Ta ax (DGI) <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref"
        },
        {
            "pid": "5d0b00dd8607575390fe8b2e",
            "content": "and structural features. However, existing GNN models are mostly established in a supervised manner <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta tional methods. Among them, considerable literature has grown up around the theme of supervised GNN <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "53e9ba85b7602d97046aa61d",
            "content": "es about computer science and edges are hyperlinks between the articles. Nodes are labeled with ten <ref type=\"bibr\" target=\"#b9\">(10)</ref> classes each representing a branch of the field. Node featu ad>A.2 Hyperparameter Specifications</head><p>All models are initialized with Glorot initialization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and trained using Adam SGD optimizer <ref type=\"bibr\" targ"
        },
        {
            "pid": "5a260c8117c44a4ba8a30a57",
            "content": "uivalent to factorizing some forms of graph proximity (e.g., transformation of the adjacent matrix) <ref type=\"bibr\" target=\"#b34\">[35]</ref>, which overly emphasize on the structural information enco"
        },
        {
            "pid": "5a260c8117c44a4ba8a30a57",
            "content": "uivalent to factorizing some forms of graph proximity (e.g., transformation of the adjacent matrix) <ref type=\"bibr\" target=\"#b34\">[35]</ref>, which overly emphasize on the structural information enco"
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b38\">39]</ref> and natural language processing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. These CL methods see"
        },
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "an be generated using a multiple-stage augmentation pipeline <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, consisting of color ji ploys a memory bank for storing negative samples. Other work <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> explores in-batch neg cosine similarity and \ud835\udc54(\u2022) is a nonlinear projection to enhance the expression power of the critic <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. The projection func"
        },
        {
            "pid": "53e9ab69b7602d970350aaa0",
            "content": "al language processing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. These CL methods seek to maximize the Mutual Information ("
        },
        {
            "pid": null,
            "content": "br\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and natural language processing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target"
        },
        {
            "pid": null,
            "content": "ikely to correspond to influential papers.</p><p>Eigenvector centrality. The eigenvector centrality <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> of a node is calcula"
        },
        {
            "pid": null,
            "content": "rget=\"#b23\">[24]</ref>, achieves great success in many fields, e.g., visual representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ es. For visual data, negative samples can be generated using a multiple-stage augmentation pipeline <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target type=\"bibr\" target=\"#b46\">47]</ref> employs a memory bank for storing negative samples. Other work <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ative samples. For an image patch as the anchor, these methods usually find a global summary vector <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> or patches in neighb mbeddings in the two views, which has been widely applied in the representation learning literature <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "arget=\"#b17\">[18]</ref>.</p><p>Theoretical analysis sheds light on the reasons behind their success <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Objectives used in these methods can be seen as maximizin lower bound of the InfoNCE objective <ref type=\"bibr\" target=\"#b41\">[42]</ref>, which is defined as <ref type=\"bibr\" target=\"#b32\">[33]</ref>. According to van den Oord et al. <ref type=\"bibr\" target= ive J and the InfoNCE objective <ref type=\"bibr\" target=\"#b41\">[42]</ref> , which can be defined as <ref type=\"bibr\" target=\"#b32\">[33]</ref> \ud835\udc3c NCE (U; V) \u225c E   </p><p>Thus, we arrive at 2J \u2264 \ud835\udc3c (U; V) -c.org/ns/1.0\" xml:id=\"fig_7\"><head>( 17 )</head><label>17</label><figDesc>According to Poole et al.<ref type=\"bibr\" target=\"#b32\">[33]</ref>, the InfoNCE estimator is a lower bound of the true MI, i. een widely applied in the representation learning literature <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "57aa28de0a3ac518da9896d5",
            "content": "ional methods on unsupervised graph representation learning employ the contrastive paradigm as well <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta ar embeddings. Positive samples under this circumstance are nodes appearing in the same random walk <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. For example, the datasets. Also, these methods are known to be error-prone with inappropriate hyperparameter tuning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>.</p><p>Recent work , (1) traditional methods including DeepWalk <ref type=\"bibr\" target=\"#b31\">[32]</ref> and node2vec <ref type=\"bibr\" target=\"#b10\">[11]</ref> and (2) deep learning methods including Graph Autoencoders"
        },
        {
            "pid": "5cede10ada562983788ea933",
            "content": "ILS A.1 Computing Infrastructures</head><p>All models are implemented using PyTorch Geometric 1.6.1 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, PyTorch 1.6.0 <ref type=\"bibr\" target=\"#b28\">[29]</ref>, an"
        },
        {
            "pid": "5cede10ada562983788ea933",
            "content": "ILS A.1 Computing Infrastructures</head><p>All models are implemented using PyTorch Geometric 1.6.1 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, PyTorch 1.6.0 <ref type=\"bibr\" target=\"#b28\">[29]</ref>, an"
        },
        {
            "pid": "53e9b281b7602d9703d27ebe",
            "content": "uctural augmentation schemes, we calculate edge centrality scores of the famous Karate club dataset <ref type=\"bibr\" target=\"#b49\">[50]</ref>, containing two groups of students leading by two coaches"
        },
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "an be generated using a multiple-stage augmentation pipeline <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, consisting of color ji ploys a memory bank for storing negative samples. Other work <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> explores in-batch neg cosine similarity and \ud835\udc54(\u2022) is a nonlinear projection to enhance the expression power of the critic <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. The projection func"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b4e9",
            "content": "with negative-sampled counterparts.</p><p>Inspired by previous CL methods, Deep Graph InfoMax (DGI) <ref type=\"bibr\" target=\"#b43\">[44]</ref> marries the power of GNN into InfoMax-based methods. DGI f gmentation in either the structural domain or the attribute domain, such as feature shifting in DGI <ref type=\"bibr\" target=\"#b43\">[44]</ref>, is not sufficient for generating diverse neighborhoods (i type=\"bibr\" target=\"#b13\">[14]</ref>, which incorporates DeepWalk-like objectives. Recent work DGI <ref type=\"bibr\" target=\"#b43\">[44]</ref> marries the power of GNN and contrastive learning, which f related graph contrastive learning methods. In summary, we provide a brief comparison between the  <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and MVGRL ><p>For every experiment, we follow the linear evaluation scheme as introduced in Veli\u010dkovi\u0107 et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref>, where each model is firstly trained in an unsupervised ma Graph Autoencoders (GAE, VGAE) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type"
        },
        {
            "pid": null,
            "content": "features and their representations <ref type=\"bibr\" target=\"#b23\">[24]</ref>. However, recent work <ref type=\"bibr\" target=\"#b40\">[41]</ref> reveals that downstream performance in evaluating the qual ear projection to enhance the expression power of the critic <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. The projection function \ud835\udc54 in our method is implemented wit rget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. MI quantifies the amount of information obtained about one r downstream performance on visual representation learning <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which further highlights the importance of the design of d"
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b38\">39]</ref> and natural language processing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. These CL methods see"
        },
        {
            "pid": "5a260c8117c44a4ba8a30f54",
            "content": "supervised manner <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, which require abundant labeled nodes for training. Recentl of supervised GNN <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>, which requires lab utional Networks (GCN) <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Graph Attention Networks (GAT) <ref type=\"bibr\" target=\"#b42\">[43]</ref>, where they are trained in an endto-end fashion. For all b"
        }
    ],
    "5f0d85c69fced0a24be4f03a": [
        {
            "pid": "53e9b56cb7602d97040a8546",
            "content": "as a single table, but starting with z15 it exploits a variation of the TAGE algorithm based off of <ref type=\"bibr\" target=\"#b8\">[8]</ref>. Two TAGE PHT tables are employed in z15-a short and a long wever, a weak TAGE PHT prediction can sometimes be detrimental, particularly after a context switch <ref type=\"bibr\" target=\"#b8\">[8]</ref>. As such, weak filtering is employed. Before allowing a weak"
        },
        {
            "pid": "557dee9ef6678c77ea21ea17",
            "content": "to various papers describing ways to reduce branch prediction latency. The Alpha EV8 line predictor <ref type=\"bibr\" target=\"#b11\">[11]</ref> fetches and predicts up to 16 conditional branches every c"
        },
        {
            "pid": "557dee9ef6678c77ea21ea17",
            "content": "to various papers describing ways to reduce branch prediction latency. The Alpha EV8 line predictor <ref type=\"bibr\" target=\"#b11\">[11]</ref> fetches and predicts up to 16 conditional branches every c"
        },
        {
            "pid": "5bdc319c17c44a1f58a09c03",
            "content": "direction prediction was introduced to help augment the patternbased auxiliary direction prediction <ref type=\"bibr\" target=\"#b9\">[9]</ref> <ref type=\"bibr\" target=\"#b18\">[18]</ref>. The z15 perceptro br\" target=\"#b18\">[18]</ref>. The z15 perceptron carries this design forward.</p><p>As described in <ref type=\"bibr\" target=\"#b9\">[9]</ref> and in patents <ref type=\"bibr\" target=\"#b13\">[13]</ref>[14] imilar components -detection and prediction. The z14 introduced a basic call/return stack predictor <ref type=\"bibr\" target=\"#b9\">[9]</ref>, which was further enhanced on z15.</p><p>A mechanism at the"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "53e9b586b7602d97040c6e97",
            "content": "ly had significant focus from academia over the decades <ref type=\"bibr\" target=\"#b7\">[7]</ref> [8] <ref type=\"bibr\" target=\"#b19\">[19]</ref>. As mentioned earlier, the impact from a wrong direction a"
        },
        {
            "pid": "55c33d17683a451f09d272a5",
            "content": "allows the IDU to know when branch prediction has fallen behind. Starting with the IBM z13 machine <ref type=\"bibr\" target=\"#b17\">[17]</ref>, the branch predictor employs such strict synchronization."
        },
        {
            "pid": "55c33d17683a451f09d272a5",
            "content": "allows the IDU to know when branch prediction has fallen behind. Starting with the IBM z13 machine <ref type=\"bibr\" target=\"#b17\">[17]</ref>, the branch predictor employs such strict synchronization."
        },
        {
            "pid": null,
            "content": "a Pattern History Table (PHT). The tagged PHT has been used on z branch predictions since the z196 <ref type=\"bibr\" target=\"#b15\">[15]</ref> as a single table, but starting with z15 it exploits a var"
        },
        {
            "pid": null,
            "content": "a Pattern History Table (PHT). The tagged PHT has been used on z branch predictions since the z196 <ref type=\"bibr\" target=\"#b15\">[15]</ref> as a single table, but starting with z15 it exploits a var"
        }
    ],
    "5f5f378a91e0117a861e8942": [
        {
            "pid": "5bdc31b417c44a1f58a0b4e9",
            "content": "orhood aggregation function), GVAE with GCN encoder Kipf &amp; Welling (2016), DGI with GIN encoder <ref type=\"bibr\" target=\"#b45\">Velickovic et al. (2019)</ref>, and EGI with GIN encoder. We train GV small), our t-tests have shown the improvements of EGI to be significant. 55.56% ? 6.83% DGI (GIN) <ref type=\"bibr\" target=\"#b45\">Velickovic et al. (2019)</ref> 57.75% ? 4.47% 62.44% ? 4.46% 68.15% ?"
        },
        {
            "pid": null,
            "content": "r focus on the structural information g i . Specifically, we use the Jensen-Shannon MI estimator in <ref type=\"bibr\" target=\"#b18\">Hjelm et al. (2019)</ref>,</p><formula xml:id=\"formula_2\">I (JSD) (G,"
        },
        {
            "pid": "5bdc31b817c44a1f58a0c039",
            "content": "rget=\"#b7\">Chen et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b31\">Oono &amp; Suzuki (2020)</ref>; <ref type=\"bibr\" target=\"#b21\">Huang et al. (2018)</ref>, due to their established performance towar"
        },
        {
            "pid": "53e9a515b7602d9702e37973",
            "content": "ntifying structural equivalent nodes. We randomly generate 40 graphs with the Forest-fire model (F) <ref type=\"bibr\" target=\"#b26\">Leskovec et al. (2005)</ref> and 40 graphs with the Barabasi model (B &amp; Albert (1999)</ref> and ( <ref type=\"formula\" target=\"#formula_4\">2</ref>) forest-fire graph <ref type=\"bibr\" target=\"#b26\">Leskovec et al. (2005)</ref>. We generate 40 graphs each with 100 nod"
        },
        {
            "pid": "599c797b601a182cd2642a37",
            "content": "n log-log scale. The class labels are between 0 to 3 reflecting the level of the airport activities <ref type=\"bibr\" target=\"#b37\">Ribeiro et al. (2017)</ref>. For the Gene dataset, we matched the gen bel></label><figDesc>We use two real-world network datasets with role-based node labels: (1) Airport<ref type=\"bibr\" target=\"#b37\">Ribeiro et al. (2017)</ref> contains three networks different regions"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "53e9b991b7602d970457fe44",
            "content": "_7\">8</ref>, the source graph we use to pre-train GNNs is the full graph cleaned from the YAGO dump <ref type=\"bibr\" target=\"#b40\">Suchanek et al. (2007)</ref>, where we assume the relations among ent"
        },
        {
            "pid": "53e9b253b7602d9703cf4028",
            "content": "l embeddings Chung &amp; Graham (1997)</ref>, and many pre-computed unsupervised network embeddings <ref type=\"bibr\" target=\"#b34\">Perozzi et al. (2014)</ref>; <ref type=\"bibr\" target=\"#b42\">Tang et a o show the performance of different transferable and non-transferable features, i.e. node embedding <ref type=\"bibr\" target=\"#b34\">Perozzi et al. (2014)</ref> and random feature vectors. The observati"
        },
        {
            "pid": "5a9cb66717c44a376ffb8667",
            "content": "an still be costly regarding both memory and computation resources on real-world large-scale graphs <ref type=\"bibr\" target=\"#b6\">Chen et al. (2018)</ref>; <ref type=\"bibr\">Ying et al. (2018a)</ref>. n et al. (2017)</ref></p>;</p><ref type=\"bibr\" target=\"#b44\">Velickovic et al. (2018)</ref></p>;</p><ref type=\"bibr\" target=\"#b6\">Chen et al. (2018)</ref></p>, and their usage is limited to single gra"
        },
        {
            "pid": "599c7974601a182cd263f01c",
            "content": "s and even domains in settings like few-shot learning <ref type=\"bibr\">Vinyals et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b11\">Finn et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b36\">Ravi &amp; L"
        },
        {
            "pid": "5c8eff704895d9cbc6043836",
            "content": "le\" target=\"#tab_7\">8</ref>. On the target graph, we also have the access to 24 different relations <ref type=\"bibr\" target=\"#b39\">Shi et al. (2018)</ref> such as isAdvisedBy, isMarriedTo and so on. S"
        }
    ],
    "5fd8acf991e0119b22c1f38d": [
        {
            "pid": "5e09a806df1a9c0c41680baa",
            "content": "m, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in <ref type=\"bibr\" target=\"#b28\">(Tsai et al. 2019)</ref>, the i-th query's attention is defined as a"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "d architecture efficient, as well as maintain higher prediction capacity?</p><p>Vanilla Transformer <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017</ref>) has three significant limitations when so tei-c.org/ns/1.0\"><head>Efficient Self-attention Mechanism</head><p>The canonical self-attention in <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017</ref>) is defined on receiving the tuple input ( ing long sequential outputs through one forward procedure</p><p>We use a standard decoder structure <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017)</ref> in Fig.</p><p>(2), and it is composed of"
        },
        {
            "pid": "5a260c5717c44a4ba8a294ce",
            "content": "e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type=\"bibr\" target=\"#b27\">(Taylor and Letham 2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">("
        },
        {
            "pid": "5a260c5717c44a4ba8a294ce",
            "content": "e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type=\"bibr\" target=\"#b27\">(Taylor and Letham 2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">("
        },
        {
            "pid": "599c798a601a182cd26495fe",
            "content": "2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type=\"bibr\" target=\"#b14\">(Lai et al. 2018</ref>) and DeepAR <ref type=\"bibr\" target=\"#b10\">(Fl"
        },
        {
            "pid": null,
            "content": "ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\""
        },
        {
            "pid": "5a260c5717c44a4ba8a294ce",
            "content": "e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type=\"bibr\" target=\"#b27\">(Taylor and Letham 2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">("
        },
        {
            "pid": null,
            "content": "t=\"#b19\">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, economics and finance <ref type=\"bibr\" target=\"#b33\">(Zhu and Shasha 2002)</ref>, and disease propagation analysis <ref ty"
        },
        {
            "pid": null,
            "content": "ef type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu"
        },
        {
            "pid": "5a260c5717c44a4ba8a294ce",
            "content": "e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type=\"bibr\" target=\"#b27\">(Taylor and Letham 2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">("
        },
        {
            "pid": "5a260c5717c44a4ba8a294ce",
            "content": "e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type=\"bibr\" target=\"#b27\">(Taylor and Letham 2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">("
        },
        {
            "pid": "599c797f601a182cd26443b8",
            "content": "on blocks in Fig. <ref type=\"figure\" target=\"#fig_3\">(3</ref>). Inspired by the dilated convolution <ref type=\"bibr\" target=\"#b32\">(Yu, Koltun, and Funkhouser 2017;</ref><ref type=\"bibr\" target=\"#b11\""
        },
        {
            "pid": "5550453645ce0a409eb54932",
            "content": "inance <ref type=\"bibr\" target=\"#b33\">(Zhu and Shasha 2002)</ref>, and disease propagation analysis <ref type=\"bibr\" target=\"#b18\">(Matsubara et al. 2014)</ref>. In these scenarios, we can leverage a \">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, disease propagation analysis <ref type=\"bibr\" target=\"#b18\">(Matsubara et al. 2014)</ref>, economics and finance forecasting (Zhu"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "599c797f601a182cd26443b8",
            "content": "on blocks in Fig. <ref type=\"figure\" target=\"#fig_3\">(3</ref>). Inspired by the dilated convolution <ref type=\"bibr\" target=\"#b32\">(Yu, Koltun, and Funkhouser 2017;</ref><ref type=\"bibr\" target=\"#b11\""
        },
        {
            "pid": null,
            "content": "reliable workhorse for time-series forecasting <ref type=\"bibr\" target=\"#b3\">(Box et al. 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</re"
        },
        {
            "pid": "5550410f45ce0a409eb384f8",
            "content": ""
        },
        {
            "pid": null,
            "content": "ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\""
        },
        {
            "pid": null,
            "content": "reliable workhorse for time-series forecasting <ref type=\"bibr\" target=\"#b3\">(Box et al. 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</re"
        },
        {
            "pid": "5a260c5717c44a4ba8a294ce",
            "content": "e-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type=\"bibr\" target=\"#b27\">(Taylor and Letham 2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">("
        },
        {
            "pid": null,
            "content": "ef type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu"
        },
        {
            "pid": "599c798a601a182cd26495fe",
            "content": "2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type=\"bibr\" target=\"#b14\">(Lai et al. 2018</ref>) and DeepAR <ref type=\"bibr\" target=\"#b10\">(Fl"
        }
    ],
    "5f3268fb91e011bc1612aeab": [
        {
            "pid": "5aed14d617c44a4438158d5d",
            "content": "are not available <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t #b4\">[5]</ref>. The system first predicts 68 face landmarks from speech using an LSTM-based network <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and then predicts a few talking face images from the condit"
        },
        {
            "pid": null,
            "content": "cial features such as wrinkled eyes and head motion to generate facial expressions. Sadoughi et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> extended the conditional-GAN-based model to take the targe ad><p>Instead of inferring emotion from the input speech <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, in this work, we propose to use emotions as a condition i"
        },
        {
            "pid": null,
            "content": "ic, lowering the workers' confidence in rating the ground-truth videos. A Wilcoxon signed-rank test <ref type=\"bibr\" target=\"#b37\">[38]</ref> shows that the median difference between our ratings and t"
        },
        {
            "pid": "599e92739c05cae4992b4ee0",
            "content": "ach is to first convert the speech input to face landmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" estimate video frames using the predicted face landmarks. In Suwajanakorn et al.'s two-stage system <ref type=\"bibr\" target=\"#b15\">[16]</ref>, an LSTM network first predicts the principal component an"
        },
        {
            "pid": "5aed14d617c44a4438158f74",
            "content": "CNN which is trained on pairs of artificially blurred images and their clear originals. Chen et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> proposed another method that predicts video frames of the"
        },
        {
            "pid": "5ecbc5959fced0a24b4e9327",
            "content": "ity between generated frames, and the synchronization between audio and visual data. Eskimez et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposed an end-to-end talking face generation system that ial network (GAN) framework. Our generator network architecture is built based on our previous work <ref type=\"bibr\" target=\"#b20\">[21]</ref>, with a modification to accept the emotion condition input the input speech waveform and outputs a speech embedding. It follows the original implementation of <ref type=\"bibr\" target=\"#b20\">[21]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head condition face image. The architecture follows the original implementation without any modification <ref type=\"bibr\" target=\"#b20\">[21]</ref>. It contains six layers of 2-D convolutional layers with t image, and the emotion condition.</p><p>5) Video Decoder: We modify the video decoder described in <ref type=\"bibr\" target=\"#b20\">[21]</ref> to accept the additional emotion embedding. We concatenate that focus on different aspects of the generated videos: a mouth region mask (MRM) loss proposed in <ref type=\"bibr\" target=\"#b20\">[21]</ref> to improve mouth-audio synchronization, a perceptual loss rames. To measure the audiovisual synchronization, we used the normalized landmarks distance (NLMD) <ref type=\"bibr\" target=\"#b20\">[21]</ref> between landmarks extracted from the generated and ground-"
        },
        {
            "pid": "5ecbc5959fced0a24b4e9327",
            "content": "ity between generated frames, and the synchronization between audio and visual data. Eskimez et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposed an end-to-end talking face generation system that ial network (GAN) framework. Our generator network architecture is built based on our previous work <ref type=\"bibr\" target=\"#b20\">[21]</ref>, with a modification to accept the emotion condition input the input speech waveform and outputs a speech embedding. It follows the original implementation of <ref type=\"bibr\" target=\"#b20\">[21]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head condition face image. The architecture follows the original implementation without any modification <ref type=\"bibr\" target=\"#b20\">[21]</ref>. It contains six layers of 2-D convolutional layers with t image, and the emotion condition.</p><p>5) Video Decoder: We modify the video decoder described in <ref type=\"bibr\" target=\"#b20\">[21]</ref> to accept the additional emotion embedding. We concatenate that focus on different aspects of the generated videos: a mouth region mask (MRM) loss proposed in <ref type=\"bibr\" target=\"#b20\">[21]</ref> to improve mouth-audio synchronization, a perceptual loss rames. To measure the audiovisual synchronization, we used the normalized landmarks distance (NLMD) <ref type=\"bibr\" target=\"#b20\">[21]</ref> between landmarks extracted from the generated and ground-"
        },
        {
            "pid": "5cf48a30da56291d58292a2f",
            "content": "ndmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a style-based landmark-to-image conversion method"
        },
        {
            "pid": "5cf48a3cda56291d5829eb69",
            "content": "generate talking faces from speech in order to provide the visual cues when they are not available <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t 6\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref> and then estimate r, this system works only for a single speaker. Another two-stage system is proposed by Chen et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The system first predicts 68 face landmarks from speech usi"
        },
        {
            "pid": null,
            "content": "=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr 8\">[9]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" erate more realistic image sequences. They further improved their methods with three discriminators <ref type=\"bibr\" target=\"#b9\">[10]</ref> that focus on improving the realness of video frames, the c etworks, and objective and subjective evaluations. We choose the temporal GAN approach described in <ref type=\"bibr\" target=\"#b9\">[10]</ref> as our baseline since it is the closest to our method. We u PS and audio to 8 kHz. We followed the same train (70%), validation (15%), and test (15%) splits as <ref type=\"bibr\" target=\"#b9\">[10]</ref>. We used the same files for these splits to ensure a fair c NLMD), even though our method does not use a discriminator calculating a synchronization loss as in <ref type=\"bibr\" target=\"#b9\">[10]</ref>, the improvement is as high as 8.9%, showing the effectiven ical emotion inputs. We evaluated our network against the ground-truth videos and a baseline system <ref type=\"bibr\" target=\"#b9\">[10]</ref> and validated that our method can generate emotional expres"
        },
        {
            "pid": "5b67b4b417c44aac1c86717b",
            "content": "b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. These systems can increase the accessibility of abundantl b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref> and then estimate video frames using the predicted face la method is limited to only generating the lip region instead of the entire talking face. Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed a GAN-based method that models the whole face and"
        }
    ],
    "5fc4cfdf91e011abfa2faf94": [
        {
            "pid": "5db929f247c8f766461fd4f1",
            "content": ""
        },
        {
            "pid": "60337a9391e011e54d039ada",
            "content": "ion probabilistic modeling (DDPM) <ref type=\"bibr\" target=\"#b39\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Ho et al., 2020)</ref> trains a sequence of probabilistic models to r at generation of images <ref type=\"bibr\">(Song &amp; Ermon, 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" target=\"#b17\">Ho et al., 2020)</ref>, audio <ref type=\"bibr\" target=\"#b6\">(Chen et s of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> has reported higher sample quality than <ref t Eq. ( <ref type=\"formula\" target=\"#formula_3\">3</ref>) described here is equivalent to L simple in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, but we re-write it in a slightly different fo rget=\"#b16\">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we obtain better bits/dim compared to the upp e=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, the gaps can be reduced, but score-based mode data pxq. In our experiments, we let \u03b2min \" 0.1 and \u03b2max \" 20, which correspond to the settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formu arget=\"#fig_4\">4</ref>, we use a DDPM model trained on 256 \u02c6256 CelebA-HQ with the same settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. We use the RK45 ODE solver <ref type=\"bibr\" t on and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns tially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP S \u00b4\u03c32 i\u00b41 qIq, i \" 1, 2, \u00a8\u00a8\u00a8, N.</formula><p>Here we assume \u03c3 0 \" 0 to simplify notations. Following <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we can compute</p><formula xml:id=\"formula_55 \" x i `p\u03c3 2 i \u00b4\u03c32 i\u00b41 qs \u03b8 px i , iq,</formula><p>where s \u03b8 px i , iq is to estimate z{\u03c3 i . As in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we let \u03c4 i \"</p><formula xml:id=\"formula_58\"> DDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS</head><p>Training We use the same architecture in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for cond amples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for optimization, including the learning rate, mula\" target=\"#formula_0\">1</ref>) and use a batch size of 128. Our architecture is mostly based on <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. We additionally search over the following com pe=\"bibr\" target=\"#b20\">Karras et al. (2018)</ref>; <ref type=\"bibr\">Song &amp; Ermon (2019)</ref>; <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, the FID value here is the lowest over the cou odel upon conditioning on continuous time variables, we change positional embeddings, the layers in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for conditioning on discrete time steps, to ra al., 2018)</ref> 3.40 -Flow++ <ref type=\"bibr\" target=\"#b16\">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.70 13.51 DDPM (Lsimple) <ref type=\"bibr\" > 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.70 13.51 DDPM (Lsimple) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.75 3.17   <ref type=\"bibr\">(Song &amp; Er )</ref> 25.32 8.87 \u02d8.12 NCSNv2 <ref type=\"bibr\">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 \u02d8.07 DDPM <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> 3.17 9.46 \u02d8.11 Exact likelihood computation L s on CIFAR-10 is 10.23 <ref type=\"bibr\">(Song &amp; Ermon, 2020)</ref>, whereas for DDPM it is 3.17 <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref>. With PC samplers and the same model architec \"#formula_52\">41</ref>)) reverse diffusion samplers.</p><p>Note that the ancestral sampling of DDPM <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> (Eq. ( <ref type=\"formula\" target=\"#formula_4"
        },
        {
            "pid": "5f3b99b791e0110589e89921",
            "content": "ong et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b31\">(Niu et al., 2020)</ref>, and shapes <ref type=\"bibr\" target=\"#b5\">(Cai et al., 2020)</ref>. However, the \u02daWork done during an internship"
        },
        {
            "pid": null,
            "content": "oaches improve results and enable more efficient sampling, they remain slower at sampling than GANs <ref type=\"bibr\" target=\"#b12\">(Goodfellow et al., 2014)</ref> on the same datasets. Identifying way"
        },
        {
            "pid": null,
            "content": "noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE <ref type=\"bibr\" target=\"#b1\">(Anderson, 1982)</ref>, which can be derived from the forward SDE give : R d \u00d1 R d\u02c6d . We follow the It\u00f4 interpretation of SDEs throughout this paper.</p><p>According to <ref type=\"bibr\" target=\"#b1\">(Anderson, 1982)</ref>, the reverse-time SDE is given by (cf ., Eq. ( xpT q \" p T and reversing the process, we can obtain samples xp0q \" p 0 . A remarkable result from <ref type=\"bibr\" target=\"#b1\">Anderson (1982)</ref> states that the reverse of a diffusion process i is p 0 pxp0q | yq. The density at time t is p t pxptq | yq when conditioned on y. Therefore, using <ref type=\"bibr\" target=\"#b1\">Anderson (1982)</ref>, the reverse-time SDE is given by dx \" tf px, tq"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5550411745ce0a409eb38760",
            "content": "be open sourced.   <ref type=\"bibr\" target=\"#b27\">Krizhevsky et al., 2009)</ref> and 64 \u02c664 CelebA <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2015)</ref>, which is pre-processed following <ref type="
        },
        {
            "pid": null,
            "content": "samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC <ref type=\"bibr\" target=\"#b34\">(Parisi, 1981;</ref><ref type=\"bibr\" target=\"#b15\">Grenander &amp; Mi del s \u03b8 \u02dapx, tq \u00ab \u2207 x log p t pxq, we can employ score-based MCMC approaches, such as Langevin MCMC <ref type=\"bibr\" target=\"#b34\">(Parisi, 1981;</ref><ref type=\"bibr\" target=\"#b15\">Grenander &amp; Mi"
        },
        {
            "pid": null,
            "content": "et=\"#b48\">(Zhang, 2019)</ref>. We follow the same implementation and hyper-parameters in StyleGAN-2 <ref type=\"bibr\" target=\"#b23\">(Karras et al., 2020b)</ref>. \u2022 Rescaling all skip connections by 1 {"
        },
        {
            "pid": null,
            "content": "</ref> 3.28 46.37 FFJORD <ref type=\"bibr\" target=\"#b14\">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type=\"bibr\" target=\"#b16\">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\" are to models evaluated in the same way (excluding models evaluated with variational dequantization <ref type=\"bibr\" target=\"#b16\">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the"
        },
        {
            "pid": "5a260c0c17c44a4ba8a1e141",
            "content": "generative models, and related techniques <ref type=\"bibr\" target=\"#b3\">(Bordes et al., 2017;</ref><ref type=\"bibr\" target=\"#b13\">Goyal et al., 2017)</ref>, have proven effective at generation of ima"
        },
        {
            "pid": "5f3b99b791e0110589e89921",
            "content": "ong et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b31\">(Niu et al., 2020)</ref>, and shapes <ref type=\"bibr\" target=\"#b5\">(Cai et al., 2020)</ref>. However, the \u02daWork done during an internship"
        },
        {
            "pid": null,
            "content": "et=\"#fig_4\">4</ref>). RealNVP <ref type=\"bibr\" target=\"#b9\">(Dinh et al., 2016)</ref> 3.49 -iResNet <ref type=\"bibr\" target=\"#b2\">(Behrmann et al., 2019)</ref> 3.45 -Glow <ref type=\"bibr\" target=\"#b24"
        },
        {
            "pid": "5d04e8ddda56295d08db32ed",
            "content": "ras et al., 2018)</ref>, a task that was previously only achievable by some GAN models and VQ-VAE-2 <ref type=\"bibr\" target=\"#b35\">(Razavi et al., 2019)</ref>. We used a batch size of 8, increased the"
        },
        {
            "pid": "5f5f404d91e0117a861e8a51",
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5e3a928fdf1a9c0c41ebe39e",
            "content": "orks, including ProgressiveGAN <ref type=\"bibr\" target=\"#b20\">(Karras et al., 2018)</ref>, StyleGAN <ref type=\"bibr\" target=\"#b21\">(Karras et al., 2019)</ref> and <ref type=\"bibr\">StyleGAN-2 (Karras e models like Progressive-GAN <ref type=\"bibr\" target=\"#b20\">(Karras et al., 2018)</ref> and StyleGAN <ref type=\"bibr\" target=\"#b21\">(Karras et al., 2019)</ref>. However, we found it harmful at an early"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC <ref type=\"bibr\" target=\"#b34\">(Parisi, 1981;</ref><ref type=\"bibr\" target=\"#b15\">Grenander &amp; Mi del s \u03b8 \u02dapx, tq \u00ab \u2207 x log p t pxq, we can employ score-based MCMC approaches, such as Langevin MCMC <ref type=\"bibr\" target=\"#b34\">(Parisi, 1981;</ref><ref type=\"bibr\" target=\"#b15\">Grenander &amp; Mi"
        },
        {
            "pid": "5a260c8117c44a4ba8a30d9b",
            "content": "ections by 1 { ? 2. This has been demonstrated effective in several works, including ProgressiveGAN <ref type=\"bibr\" target=\"#b20\">(Karras et al., 2018)</ref>, StyleGAN <ref type=\"bibr\" target=\"#b21\"> e also tested equalized learning rates, a trick used in very successful models like Progressive-GAN <ref type=\"bibr\" target=\"#b20\">(Karras et al., 2018)</ref> and StyleGAN <ref type=\"bibr\" target=\"#b2 f>). Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 1024 \u02c61024 CelebA-HQ <ref type=\"bibr\" target=\"#b20\">(Karras et al., 2018)</ref>, a task that was previously only achievab residual blocks achieves an FID of 2.45 on CIFAR-10. Here in order to match the convention used in <ref type=\"bibr\" target=\"#b20\">Karras et al. (2018)</ref>; <ref type=\"bibr\">Song &amp; Ermon (2019)<"
        }
    ],
    "5fbe5cf091e011e6e11b3cf5": [
        {
            "pid": "5e4672c93a55ac14f595d8b5",
            "content": "earning has been closing the gap with, and in some cases even surpassing its supervised counterpart <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ air), creating contradicting objectives.</p><p>While recent efforts focus on improved architectures <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ef type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and data augmentation <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, relatively little w get=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>.</p><p>In contrastive learning, the embedding space is govern different images, regardless of their semantic information <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Figure <ref type=\"fi s this problem by maintaining a momentum encoder and a limited queue of previous samples. SimCLR v1 <ref type=\"bibr\" target=\"#b8\">[9]</ref> eschews a momentum encoder in favor of a large batch size, a \"bibr\" target=\"#b33\">[34]</ref> 63.6 -PCL <ref type=\"bibr\" target=\"#b31\">[32]</ref> 65.9 -SimCLR v1 <ref type=\"bibr\" target=\"#b8\">[9]</ref> 69.3 89.0 MoCo v2 <ref type=\"bibr\" target=\"#b10\">[11]</ref>"
        },
        {
            "pid": null,
            "content": "et=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar r\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, and contrastive multiview coding <ref type=\"bibr\" target=\"#b41\">[42]</ref>. Recognizing that contrastive loss requires a large set of"
        },
        {
            "pid": "53e9abc4b7602d9703576623",
            "content": "t <ref type=\"bibr\" target=\"#b32\">[33]</ref>, VOC2007 <ref type=\"bibr\" target=\"#b16\">[17]</ref>, DTD <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Pets <ref type=\"bibr\" target=\"#b36\">[37]</ref>, Caltech-1"
        },
        {
            "pid": "5aed14e217c44a4438159ac5",
            "content": "in visual representations, there has been a recent surge in self-supervised representation learning <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta mploys proxy tasks to guide the learned embeddings, such as predicting the angle of a rotated image <ref type=\"bibr\" target=\"#b18\">[19]</ref>, the relative location of patches <ref type=\"bibr\" target="
        },
        {
            "pid": null,
            "content": "et=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. While conventional approaches use labeled data to pretrain"
        },
        {
            "pid": null,
            "content": "et=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. While conventional approaches use labeled data to pretrain"
        },
        {
            "pid": "573696136e3b12023e5258e2",
            "content": "<ref type=\"bibr\" target=\"#b49\">[50]</ref> or colorization <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. While these approa"
        },
        {
            "pid": "58d82fced649053542fd6e59",
            "content": "arget=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. In fact, selfsuper rget=\"#b7\">8]</ref> or generating one view of an image from another, e.g., split-brain auto-encoder <ref type=\"bibr\" target=\"#b49\">[50]</ref> or colorization <ref type=\"bibr\" target=\"#b30\">[31,</ref><"
        },
        {
            "pid": null,
            "content": "et=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. While conventional approaches use labeled data to pretrain"
        },
        {
            "pid": "58d82fced649053542fd6e59",
            "content": "arget=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. In fact, selfsuper rget=\"#b7\">8]</ref> or generating one view of an image from another, e.g., split-brain auto-encoder <ref type=\"bibr\" target=\"#b49\">[50]</ref> or colorization <ref type=\"bibr\" target=\"#b30\">[31,</ref><"
        },
        {
            "pid": "53e9abc4b7602d9703576623",
            "content": "t <ref type=\"bibr\" target=\"#b32\">[33]</ref>, VOC2007 <ref type=\"bibr\" target=\"#b16\">[17]</ref>, DTD <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Pets <ref type=\"bibr\" target=\"#b36\">[37]</ref>, Caltech-1"
        }
    ],
    "5f7aeb7691e011983cc81e80": [
        {
            "pid": "5b1643ba8fbcbf6e5a9bc5b8",
            "content": ""
        },
        {
            "pid": "5b8c9f4a17c44af36f8b72cf",
            "content": "reported in Appendix A.1.</p><p>End-to-End Entity Linking (EL) For EL, we reproduce the setting of <ref type=\"bibr\" target=\"#b27\">Kolitsas et al. (2018)</ref> using the same in-domain and out-of-doma ntity mentions (a span contained in d j ) and e i \u2208 E its corresponding entity in the KB. Following <ref type=\"bibr\" target=\"#b27\">Kolitsas et al. (2018)</ref>, we considered only mentions that have e tics for 10k steps and we do model selection on the validation set. Again, following previous works <ref type=\"bibr\" target=\"#b27\">(Kolitsas et al., 2018)</ref>, we considered only mentions that have"
        },
        {
            "pid": "5c8dc31f4895d9cbc69e72ee",
            "content": ""
        },
        {
            "pid": "5550410f45ce0a409eb384f8",
            "content": "pensive when E is very large (e.g., Wikipedia has \u223c6M entities). Hence, we exploit Beam Search (BS, <ref type=\"bibr\" target=\"#b60\">Sutskever et al., 2014)</ref>, an established approximate decoding st"
        },
        {
            "pid": "5bdc315017c44a1f58a05c8a",
            "content": "g using Natural Questions <ref type=\"bibr\" target=\"#b29\">(Kwiatkowski et al., 2019)</ref>, HotpotQA <ref type=\"bibr\" target=\"#b70\">(Yang et al., 2018c)</ref>, TriviaQA <ref type=\"bibr\" target=\"#b24\">("
        },
        {
            "pid": "53e99cb5b7602d9702567499",
            "content": "a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing <ref type=\"bibr\" target=\"#b59\">(Sutskever et al., 2011;</ref><ref type=\"bibr\">2014)</ref> and regula"
        },
        {
            "pid": "5e09a81edf1a9c0c41683038",
            "content": "rget=\"#b30\">Le &amp; Titov, 2018;</ref><ref type=\"bibr\" target=\"#b37\">Logeswaran et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Broscheit, 2019;</ref><ref type=\"bibr\" target=\"#b64\">Wu et al., 2019</"
        },
        {
            "pid": null,
            "content": "generate entity names. This architecture has been shown to retain factual knowledge to some extent <ref type=\"bibr\" target=\"#b45\">(Petroni et al., 2019)</ref> and language translation skills <ref typ"
        },
        {
            "pid": "5aed14d617c44a4438159569",
            "content": "ists of five tasks that use the same Wikipedia dump as a knowledge source: fact checking with FEVER <ref type=\"bibr\" target=\"#b62\">(Thorne et al., 2018)</ref>; open domain question answering using Nat"
        },
        {
            "pid": null,
            "content": "nowledge Bases (KBs) given a textual input is a fundamental building block for several applications <ref type=\"bibr\" target=\"#b14\">(Ferrucci, 2012;</ref><ref type=\"bibr\" target=\"#b56\">Slawski, 2015;</ (e.g., Wikipedia articles) to find knowledge for sustaining a conversation or answering a question <ref type=\"bibr\" target=\"#b14\">(Ferrucci, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2017"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "the public leaderboard: DPR <ref type=\"bibr\" target=\"#b25\">(Karpukhin et al., 2020)</ref>, DPR+BERT <ref type=\"bibr\" target=\"#b9\">(Devlin et al., 2019)</ref>, DPR+BART, tf-idf <ref type=\"bibr\" target="
        },
        {
            "pid": null,
            "content": "RIMENTAL DETAILS</head><p>We implemented, trained, and evaluate our model using the fariseq library <ref type=\"bibr\" target=\"#b44\">(Ott et al., 2019)</ref>. We trained GENRE for every task using Adam"
        },
        {
            "pid": "599c796d601a182cd263c5ac",
            "content": "discourse representation structure parsing <ref type=\"bibr\" target=\"#b36\">(Liu et al., 2018)</ref>  <ref type=\"bibr\" target=\"#b28\">(Konstas et al., 2017)</ref>. In these works a structured representat"
        },
        {
            "pid": "53e99cb5b7602d9702567499",
            "content": "a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing <ref type=\"bibr\" target=\"#b59\">(Sutskever et al., 2011;</ref><ref type=\"bibr\">2014)</ref> and regula"
        },
        {
            "pid": null,
            "content": "generate entity names. This architecture has been shown to retain factual knowledge to some extent <ref type=\"bibr\" target=\"#b45\">(Petroni et al., 2019)</ref> and language translation skills <ref typ"
        },
        {
            "pid": "58d82fcbd649053542fd65e4",
            "content": "Liu, 2017;</ref><ref type=\"bibr\" target=\"#b49\">Post &amp; Vilar, 2018)</ref>, and image captioning <ref type=\"bibr\" target=\"#b1\">(Anderson et al., 2017)</ref>. To the best of our knowledge, we are th"
        },
        {
            "pid": "5b67b47917c44aac1c863dde",
            "content": ""
        },
        {
            "pid": "53e9b02fb7602d9703a8d4c2",
            "content": ""
        },
        {
            "pid": null,
            "content": "tings (both in and out-ofdomain); (ii) end-to-end entity linking, with the GERBIL benchmarking tool <ref type=\"bibr\" target=\"#b52\">(R\u00f6der et al., 2018)</ref>, by using a novel dynamically markup-const nd out-of-domain datasets as well as evaluating the InKB micro-F 1 on the GERBIL benchmark platform <ref type=\"bibr\" target=\"#b52\">(R\u00f6der et al., 2018)</ref>. Similarly to the ED setting, we first pre"
        },
        {
            "pid": null,
            "content": "nowledge Bases (KBs) given a textual input is a fundamental building block for several applications <ref type=\"bibr\" target=\"#b14\">(Ferrucci, 2012;</ref><ref type=\"bibr\" target=\"#b56\">Slawski, 2015;</ (e.g., Wikipedia articles) to find knowledge for sustaining a conversation or answering a question <ref type=\"bibr\" target=\"#b14\">(Ferrucci, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2017"
        },
        {
            "pid": null,
            "content": "generate entity names. This architecture has been shown to retain factual knowledge to some extent <ref type=\"bibr\" target=\"#b45\">(Petroni et al., 2019)</ref> and language translation skills <ref typ"
        },
        {
            "pid": "58d82fcbd649053542fd65e4",
            "content": "Liu, 2017;</ref><ref type=\"bibr\" target=\"#b49\">Post &amp; Vilar, 2018)</ref>, and image captioning <ref type=\"bibr\" target=\"#b1\">(Anderson et al., 2017)</ref>. To the best of our knowledge, we are th"
        }
    ],
    "5ef96b048806af6ef27720ed": [
        {
            "pid": "599c7968601a182cd263a485",
            "content": "nformation retrieval to capture the exact and soft matches between a query and a candidate document <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref>. Specifically, we apply the basic BERT uni ors are disordered and independent from each other. Thus we adopt a RBF kernel aggregation function <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref> to extract features about the accumulation"
        },
        {
            "pid": null,
            "content": "itional KG embedding models such as <ref type=\"bibr\">MTransE [Chen et al., 2017]</ref> and IPTransE <ref type=\"bibr\" target=\"#b15\">[Zhu et al., 2017]</ref> to recent emergent graph neural networks suc"
        },
        {
            "pid": null,
            "content": "#b15\">[Zhu et al., 2017]</ref> to recent emergent graph neural networks such as attention-based GCN <ref type=\"bibr\" target=\"#b11\">[Xu et al., 2019]</ref>, highway <ref type=\"bibr\">GCN [Wu et al., 201 ings <ref type=\"bibr\">[Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Wu et al., 2019a;</ref><ref type=\"bibr\" target=\"#b11\">Xu et al., 2019]</ref>. However, since different KGs are highly heter pe=\"bibr\" target=\"#b0\">[Cao et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Wu et al., 2019a;</ref><ref type=\"bibr\" target=\"#b11\">Xu et al., 2019]</ref>, essentially, the GCNlike models still mix the of all their neighbors as existing works did <ref type=\"bibr\" target=\"#b9\">[Wu et al., 2019a;</ref><ref type=\"bibr\" target=\"#b11\">Xu et al., 2019]</ref>. This similar idea is widely used in informati"
        },
        {
            "pid": null,
            "content": "N [Wu et al., 2019b]</ref>, relation-aware <ref type=\"bibr\">GCN [Wu et al., 2019a]</ref> and VR-GCN <ref type=\"bibr\" target=\"#b12\">[Ye et al., 2019]</ref>.</p><p>Despite much effort taken on graph str"
        },
        {
            "pid": null,
            "content": "a node embedding by aggregating all neighbors' embeddings <ref type=\"bibr\">[Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Wu et al., 2019a;</ref><ref type=\"bibr\" target=\"#b11\">Xu et al., 2019] nguish the influence from different neighbors <ref type=\"bibr\" target=\"#b0\">[Cao et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Wu et al., 2019a;</ref><ref type=\"bibr\" target=\"#b11\">Xu et al., 2019] ion for e or e \u2032 by aggregating the names/descriptions of all their neighbors as existing works did <ref type=\"bibr\" target=\"#b9\">[Wu et al., 2019a;</ref><ref type=\"bibr\" target=\"#b11\">Xu et al., 2019 he assumption that two relations are more similar if they associate to more similar head-tail pairs <ref type=\"bibr\" target=\"#b9\">[Wu et al., 2019a]</ref>. Specifically, we average C(e) of all the ass lable results or codes. Some methods such as <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Trsedya et al., 2019]</ref> are not compared due to the code implement"
        },
        {
            "pid": null,
            "content": "dge graphs (KGs), which can benefit many applications such as question answering and recommendation <ref type=\"bibr\" target=\"#b8\">[Tong et al., 2019]</ref>. However, a single KG is far from complete t"
        },
        {
            "pid": null,
            "content": "N [Wu et al., 2019b]</ref>, relation-aware <ref type=\"bibr\">GCN [Wu et al., 2019a]</ref> and VR-GCN <ref type=\"bibr\" target=\"#b12\">[Ye et al., 2019]</ref>.</p><p>Despite much effort taken on graph str"
        },
        {
            "pid": null,
            "content": "\">[Yang et al., 2019]</ref>. Although some works distinguish the influence from different neighbors <ref type=\"bibr\" target=\"#b0\">[Cao et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Wu et al., 2019a"
        },
        {
            "pid": null,
            "content": "itional KG embedding models such as <ref type=\"bibr\">MTransE [Chen et al., 2017]</ref> and IPTransE <ref type=\"bibr\" target=\"#b15\">[Zhu et al., 2017]</ref> to recent emergent graph neural networks suc"
        },
        {
            "pid": null,
            "content": "een C(e) and C(e \u2032 ). Negative pairs are sampled according to the cosine similarity of two entities <ref type=\"bibr\" target=\"#b7\">[Sun et al., 2018]</ref>.</p><p>For the input of BERT, we give priorit et DBP15K and the mono-lingual dataset DWY100K and use HitRatio@K (K=1,10) and MRR to evaluate (Cf. <ref type=\"bibr\" target=\"#b7\">[Sun et al., 2018]</ref> for details). The dimension of the BERT CLS e the one-hop neighbors take the most important role in aligning two entities. However, as claimed by <ref type=\"bibr\" target=\"#b7\">[Sun et al., 2020]</ref>, the multi-hop neighbors also impact the alig"
        },
        {
            "pid": null,
            "content": "ural embeddings have low * Contact Author  <ref type=\"bibr\">Yang et al., 2019)</ref> expressiveness <ref type=\"bibr\" target=\"#b5\">[Guo et al., 2019]</ref>. For example, in DBpedia-ZH<ref type=\"foot\" t"
        }
    ],
    "5f6f14c49fced0a24bb647ec": [
        {
            "pid": "599c782b601a182cd25a765e",
            "content": "also trigger side effects, adverse reactions, and even serious toxicity, leading patients in danger <ref type=\"bibr\" target=\"#b1\">[2]</ref>. As there exists increasing needs of multi-drug treatments, unannotated DDIs, and cannot give alerts to potential DDIs before a combinational treatment is made <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In contrast, machine learning-based methods provide a promi presents drugs in a form of feature vector according to drug properties, such as chemical structure <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe 2]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, targets <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe \"bibr\" target=\"#b11\">[12]</ref>, SVM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, logistic regression <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target s, then deduces potential DDIs with the well-trained model. Most methods utilize a single predictor <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": "5cf65cc53a55ac562ab1af46",
            "content": "#b16\">[17]</ref>. Recently, the GCN has been applied to the field of drug development and discovery <ref type=\"bibr\" target=\"#b19\">[20]</ref>, such as molecular activity prediction <ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": "extracted the approved small molecular drugs and their interaction relationships from DrugBank 4.0 <ref type=\"bibr\" target=\"#b31\">[32]</ref> to build the DB1 dataset which contains 1562 drugs and 180"
        },
        {
            "pid": null,
            "content": "target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe [13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, targets <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe ef type=\"bibr\" target=\"#b10\">[11]</ref>, Anatomical Therapeutic Chemical classification (ATC) codes <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe rug network structure <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref>, label propagation <ref type=\"bibr\" target=\"#b12\">[13]</ref> target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" tar /ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, side effects <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target ef type=\"bibr\" target=\"#b11\">[12]</ref>, logistic regression <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, decision tree <ref ty"
        },
        {
            "pid": "5cf65cc53a55ac562ab1af46",
            "content": "#b16\">[17]</ref>. Recently, the GCN has been applied to the field of drug development and discovery <ref type=\"bibr\" target=\"#b19\">[20]</ref>, such as molecular activity prediction <ref type=\"bibr\" ta"
        },
        {
            "pid": "5bdc31af17c44a1f58a0a5af",
            "content": "=\"bibr\" target=\"#b3\">4]</ref>, insurance claim databases and the FDA Adverse Event Reporting System <ref type=\"bibr\" target=\"#b4\">[5]</ref>. They are quite useful in building DDI-related databases. Ho well-trained model. Most methods utilize a single predictor <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, targets <ref typ f><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref>, label propagation <ref type=\"bibr\" target=\"#b12\">[13]</ref>, random walk <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t =\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, label propagationbased method (named as LP) <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Zhang's method (named as CE) <ref type=\"bibr\" target=\" ion profile fingerprints (IPFs) to measure similarity for predicting DDIs. Label propagation method <ref type=\"bibr\" target=\"#b12\">[13]</ref> applies label propagation to assign labels from known DDIs target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, medication and/or"
        },
        {
            "pid": "5cadfb2de1cd8e3b60c0de9f",
            "content": "=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>, while some of them integrate multiple predictors <ref typ into a one-hot code with 118 bits.</p><p>We also used drug-binding protein (DBP) data collected by <ref type=\"bibr\" target=\"#b15\">[16]</ref>, including 899 drug targets and 222 non-target proteins. S"
        },
        {
            "pid": "5cf65cc53a55ac562ab1af46",
            "content": "#b16\">[17]</ref>. Recently, the GCN has been applied to the field of drug development and discovery <ref type=\"bibr\" target=\"#b19\">[20]</ref>, such as molecular activity prediction <ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": "get=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" t f>.</p><p>The supervised predictor is usually implemented by classification algorithms, such as KNN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, SVM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, logistic r mplemented by classification algorithms, such as KNN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, SVM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, logistic regression <ref type=\"bibr\" target=\"#b1\">[2,</re arget=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, side effects <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref t 16]</ref>, while some of them integrate multiple predictors <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>.</p><p>In general, the performance of existing approaches h"
        },
        {
            "pid": "5cf65cc53a55ac562ab1af46",
            "content": "#b16\">[17]</ref>. Recently, the GCN has been applied to the field of drug development and discovery <ref type=\"bibr\" target=\"#b19\">[20]</ref>, such as molecular activity prediction <ref type=\"bibr\" ta"
        }
    ],
    "5f50ba4291e01182e69239cb": [
        {
            "pid": "5bdc315817c44a1f58a05e9d",
            "content": "\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and typically require pixel-level correspondences of sens better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b13\">[14]</ref> to build dense LiDAR BEV feature maps and do point-wise fe"
        },
        {
            "pid": "58d82fcbd649053542fd640a",
            "content": "nformation, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bib -based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type=\"bibr\" target=\"#b10\">[11]</ref> and AVOD <ref type=\"bibr\" target=\"#b11\">[12]</ref> project"
        },
        {
            "pid": "5a73cbcc17c44a0b3035f319",
            "content": "data alignment, often involve complicated architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib that could only be observed from 3D space. MV3D <ref type=\"bibr\" target=\"#b10\">[11]</ref> and AVOD <ref type=\"bibr\" target=\"#b11\">[12]</ref> project the raw point cloud into bird's eye view (BEV) to"
        },
        {
            "pid": "56d90a73dabfae2eee136349",
            "content": "mpared to 2D object detection, which has been well-studied <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t"
        },
        {
            "pid": "5da2f8aa3a55ac3402d8c202",
            "content": "eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b1"
        },
        {
            "pid": null,
            "content": "architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and typically etup for self-driving cars. Frustum PointNet <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Pointfusion <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Frustum ConvNet <ref type=\"bibr\" target=\"#b24\">[25]</r"
        },
        {
            "pid": "5736986b6e3b12023e730129",
            "content": "g for object avoidance and navigation. Compared to 2D object detection, which has been well-studied <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" t"
        },
        {
            "pid": "53e9a508b7602d9702e2bcf5",
            "content": "sor T. Note that for the first three convolution layers, after each convolution layer applied, ReLU <ref type=\"bibr\" target=\"#b26\">[27]</ref> is used. Since we have saved the indices of these non-empt"
        },
        {
            "pid": "573697826e3b12023e669875",
            "content": "ween 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D object information by calculating the similari"
        },
        {
            "pid": null,
            "content": "ref> leverage the geometric constraints between 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D obj"
        },
        {
            "pid": null,
            "content": "in this section since this is the most common sensor setup for self-driving cars. Frustum PointNet <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Pointfusion <ref type=\"bibr\" target=\"#b12\">[13]</ref> and"
        },
        {
            "pid": null,
            "content": "=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, 3D object detection is more challenging with more output pa ead><p>We use a cross entropy entropy loss for target classification, modified by the focal loss in <ref type=\"bibr\" target=\"#b3\">[4]</ref> with parameters \u03b1 = 0.25 and \u03b3 = 2 to address the large clas"
        },
        {
            "pid": "5da2f8aa3a55ac3402d8c202",
            "content": "eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b1"
        },
        {
            "pid": "5da2f8aa3a55ac3402d8c202",
            "content": "eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b1"
        },
        {
            "pid": null,
            "content": "ref> leverage the geometric constraints between 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D obj"
        },
        {
            "pid": "5c04965717c44a2c74707d12",
            "content": "bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are 70400 (200 \u00d7 176 \u00d7 2) predictions in each frame. scade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Point official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> pub"
        },
        {
            "pid": "573697826e3b12023e669875",
            "content": "ween 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D object information by calculating the similari"
        },
        {
            "pid": "573697826e3b12023e669875",
            "content": "ween 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D object information by calculating the similari"
        },
        {
            "pid": null,
            "content": "erate 3D proposals in a bottomup manner and then refines these proposals in a second stage. PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> leverages the advantages of both 3D voxel CNN and PointNet f type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>. While not the top performers within the KITTI leaderboard as, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cascade R-CNN, as CLOCs PVCas. All the other combinati"
        },
        {
            "pid": "5da2f8aa3a55ac3402d8c202",
            "content": "eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b1"
        },
        {
            "pid": "58d82fced649053542fd7243",
            "content": "on, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t e inference time significantly. PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> uses PointNets <ref type=\"bibr\" target=\"#b6\">[7]</ref> in an encoder that represents point clouds organized in vert"
        },
        {
            "pid": null,
            "content": "ype=\"bibr\" target=\"#b7\">[8]</ref>, Fast PointRCNN <ref type=\"bibr\" target=\"#b19\">[20]</ref> and STD <ref type=\"bibr\" target=\"#b20\">[21]</ref> applies a two-stage architecture that first generate 3D pr"
        },
        {
            "pid": "5736986b6e3b12023e72f6f2",
            "content": "submit the detection results to KITTI server. For experimental studies, we follow the convention in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to split the original training samples into 3712 training"
        },
        {
            "pid": null,
            "content": "ref> leverage the geometric constraints between 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D obj"
        },
        {
            "pid": null,
            "content": "target=\"#b23\">[24]</ref>, Pointfusion <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Frustum ConvNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> are the representatives of 2D driven 3D detectors, which e"
        },
        {
            "pid": null,
            "content": "ype=\"bibr\" target=\"#b7\">[8]</ref>, Fast PointRCNN <ref type=\"bibr\" target=\"#b19\">[20]</ref> and STD <ref type=\"bibr\" target=\"#b20\">[21]</ref> applies a two-stage architecture that first generate 3D pr"
        },
        {
            "pid": null,
            "content": "=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> are hampered by typ etection; it enables inference at 62 Hz; Compared with one-stage methods discussed above, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Fast PointRCNN <ref type=\"bibr\" target=\"#b19\">[20]</ref> an type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>. While ef> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, written as CLOCs SecCas, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type=\"bib"
        },
        {
            "pid": null,
            "content": "CNN and PointNet-based set abstraction to learn more discriminative features. Besides, Part-A 2 in <ref type=\"bibr\" target=\"#b22\">[23]</ref> explores predicting intra-object part locations (lower lef"
        },
        {
            "pid": "58d82fc8d649053542fd5963",
            "content": "xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> leverage the geometric constraints between 2D and 3D bound"
        },
        {
            "pid": null,
            "content": "CNN and PointNet-based set abstraction to learn more discriminative features. Besides, Part-A 2 in <ref type=\"bibr\" target=\"#b22\">[23]</ref> explores predicting intra-object part locations (lower lef"
        },
        {
            "pid": "56d90a73dabfae2eee136349",
            "content": "mpared to 2D object detection, which has been well-studied <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t"
        },
        {
            "pid": "5736986b6e3b12023e72f6f2",
            "content": "submit the detection results to KITTI server. For experimental studies, we follow the convention in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to split the original training samples into 3712 training"
        },
        {
            "pid": "5da2f8aa3a55ac3402d8c202",
            "content": "eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b1"
        }
    ],
    "5f91548b91e011126509bd5a": [
        {
            "pid": "5e3d353b3a55ac4de4104f40",
            "content": "ghbors into node representation learning and achieve state-ofthe-art performance for recommendation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta pervision Signal. Most models approach the recommendation task under a supervised learning paradigm <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta n users and items, where \ud835\udc66 \ud835\udc62\ud835\udc56 indicates that user \ud835\udc62 has adopted item \ud835\udc56 before. Most existing models <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta rimental Settings</head><p>We conduct experiments on three widely used benchmark datasets: Yelp2018 <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref 2018 <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, and Alibaba-iFash shion <ref type=\"bibr\" target=\"#b5\">[6]</ref> <ref type=\"foot\" target=\"#foot_0\">1</ref> . Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, we use the same 1 et=\"#b35\">[36]</ref>, and PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> since the previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta type=\"bibr\" target=\"#b1\">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta embedding and item embedding. Here we implement it on a state-of-the-art GCN-based model, LightGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Experimental studies on three benchmark datasets demonstr ibr\" target=\"#b47\">48]</ref>, concatenation <ref type=\"bibr\" target=\"#b43\">[44]</ref>, or summation <ref type=\"bibr\" target=\"#b16\">[17]</ref> over the representations of all layers.</p><p>Supervised L rization coefficient \ud835\udf06 2 and the number of GCN layers within the suggested ranges.</p><p>\u2022 LightGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>. This is the state-of-the-art graph-based CF method which between LightGCN and SGL-ED.trainable parameters, the space complexity remains the same as LightGCN<ref type=\"bibr\" target=\"#b16\">[17]</ref>. The time complexity of model inference is also the same,"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": ""
        },
        {
            "pid": "5b67b45517c44aac1c860876",
            "content": ". This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> and LightGCN [17]. Despite effectiveness, we argue that th type=\"bibr\" target=\"#b18\">[19]</ref>, GC-MC <ref type=\"bibr\" target=\"#b35\">[36]</ref>, and PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> since the previous work <ref type=\"bibr\" target=\"#b16\">[17 et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Despite effectiveness, current GCN-based recommendat p>which can be simply set as the last-layer representation <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, concatenation <ref type=\"bibr\" target=\"#b43\">[44]</ref>, o et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Recently, attention mechanism is introduced into GCN-based"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": ""
        },
        {
            "pid": null,
            "content": "arget=\"#b37\">38]</ref> and natural language processing (NLP) <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, SSL is relatively less explored in recommendation. The ide"
        },
        {
            "pid": "53e9ba85b7602d97046aa61d",
            "content": "r fair comparison, all models are trained from scratch which are initialized with the Xavier method <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The models are optimized by the Adam optimizer with  <ref"
        },
        {
            "pid": null,
            "content": "s: generative models <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and contrastive models <ref type=\"bibr\" target=\"#b4\">[5,</r"
        },
        {
            "pid": null,
            "content": "tion data is the theme of collaborative recommendation. Earlier work like matrix factorization (MF) <ref type=\"bibr\" target=\"#b31\">[32]</ref> projects single ID of each user (or item) into an embeddin er intensively used loss in recommendation is the pairwise Bayesian Personalized Ranking (BPR) loss <ref type=\"bibr\" target=\"#b31\">[32]</ref>, which enforces the prediction of an observed interaction he suggested model setting and tune the dropout ratio and \ud835\udefd. We discard potential baselines like MF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, NeuMF <ref type=\"bibr\" target=\"#b18\">[19]</ref>, GC-MC <r learning paradigm <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, where the supervision signal comes from the observed user-"
        },
        {
            "pid": null,
            "content": "ntations. \u2022 Skewed Data Distribution. Observed interactions usually follow a power-law distribution <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, where the long tail"
        },
        {
            "pid": "5736977f6e3b12023e666369",
            "content": "get=\"#b44\">45]</ref> is infeasible for graph-based recommendation, due to specific characteristics: <ref type=\"bibr\" target=\"#b0\">(1)</ref> The features of users and items are discrete, like one-hot I ing for side information apart from user-item interactions, which ranges from user social relations <ref type=\"bibr\" target=\"#b0\">[1]</ref>, item co-occurrence <ref type=\"bibr\" target=\"#b1\">[2]</ref>,"
        },
        {
            "pid": "5cf48a48da56291d582ab75a",
            "content": "for recommendation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Despite effe ngs over the graph <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Recently, attentio st existing models <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> construct a bipartite graph G = (V, E), where the node set eriments on three widely used benchmark datasets: Yelp2018 <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref /ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, and Alibaba-iFashion <ref type=\"bibr\" target=\"#b5\">[6]</re ref> <ref type=\"foot\" target=\"#foot_0\">1</ref> . Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, we use the same 10-core setting for Yelp2018 and Amazon-Bo the previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> has validated the superiority over the compared ones. Upon on <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, concatenation <ref type=\"bibr\" target=\"#b43\">[44]</ref>, or summation <ref type=\"bibr\" target=\"#b16\">[17]</ref> ov arized in Table <ref type=\"table\" target=\"#tab_2\">2</ref>. We follow the same strategy described in <ref type=\"bibr\" target=\"#b43\">[44]</ref> to split the interactions into training, validation, and t sting with a ratio of 7:1:2.</p><p>For users in the testing set, we follow the all-ranking protocol <ref type=\"bibr\" target=\"#b43\">[44]</ref> to evaluate the top-\ud835\udc3e recommendation performance and repor p><p>4.1.1 Compared Methods. We compare the proposed SGL with the following CF models:</p><p>\u2022 NGCF <ref type=\"bibr\" target=\"#b43\">[44]</ref>. This is a graph-based CF method largely follows the stand"
        }
    ],
    "5f0d85c69fced0a24be4f04c": [
        {
            "pid": "53e9b5edb7602d97041417c7",
            "content": "ave tried to predicate only those instances of H2P branches which have low confidence of prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>- <ref type=\"bibr\" >[11]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Policies like Diverge Merge Processor (DMP) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref> use careful compi control flow convergence using generic patterns of convergence. This is unlike previous approaches <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" paths can converge to some later point in the program (using the same convergence criterion as DMP <ref type=\"bibr\" target=\"#b6\">[7]</ref>). Loops are naturally converging and contribute to another 1 namically applied predication only on branch instances having low confidence from branch prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" ofitably) and fetches both the directions of the hammock in hardware. Diverge Merge Processor (DMP) <ref type=\"bibr\" target=\"#b6\">[7]</ref> improves upon both Wish Branches and DHP. DMP uses compiler ce register or flags (like stores or branches), instantly releases its resources.</p><p>Prior works <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> have relied on se er transparency without resorting to complex RAT recovery mechanisms or re-execution as proposed in <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>3) Predica evaluate this trade-off for ACB.  In this section, we compare against Diverge-Merge Processor (DMP) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, which relies on changes to the compiler, ISA and micro-arch e=\"bibr\" target=\"#b17\">[18]</ref> but due to large overheads, the realistic benefits are diminished <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Wish Branches <r target=\"#b33\">[34]</ref>- <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Diverge-Merge Processor (DMP) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref> uses branch predi"
        },
        {
            "pid": "53e9a130b7602d9702a1eea0",
            "content": "and stores are stalled in the OOO-IQ until ACB resolves its direction. Memory disambiguation logic <ref type=\"bibr\" target=\"#b19\">[20]</ref> stalls on stores since their addresses are not computed ye"
        },
        {
            "pid": null,
            "content": "s. ACB is applicable on top of any baseline branch predictor, including SLB.</p><p>Rotenberg et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> proposed a hardware to detect only forward convergence sce"
        },
        {
            "pid": "558a6900e4b031bae1f777fc",
            "content": "ly in the past <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Popular ISAs support static predication <ref type=\"bibr\""
        },
        {
            "pid": "5b1642388fbcbf6e5a9b5373",
            "content": "ck, nab, roms, perlbench, gcc, mcf, omnetpp, xalancbmk, x264, deepsjeng, leela, exchange, xz SPEC17 <ref type=\"bibr\" target=\"#b21\">[22]</ref> winzip, photoshop, sketchup, premiere SYSmark <ref type=\"b"
        },
        {
            "pid": "53e9a130b7602d9702a1eea0",
            "content": "and stores are stalled in the OOO-IQ until ACB resolves its direction. Memory disambiguation logic <ref type=\"bibr\" target=\"#b19\">[20]</ref> stalls on stores since their addresses are not computed ye"
        },
        {
            "pid": "53e9ab07b7602d97034878ec",
            "content": "es of H2P branches which have low confidence of prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Policies like lly. However, this approach increases the compiled code footprint. Dynamic Hammock Predication (DHP <ref type=\"bibr\" target=\"#b10\">[11]</ref>) uses the compiler to identify simple, short hammocks whic ches), instantly releases its resources.</p><p>Prior works <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> have relied on select-micro-op based approaches to handle lex RAT recovery mechanisms or re-execution as proposed in <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>3) Predicated-False Path Loads/Stores: All ACB body egory D, and reduces mispredictions over baseline. A similar observation was made by Klauser et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> for branch history update and dynamic predication. Interes tling mechanism like Dynamo is needed for such cases.</p><p>Comparison against DHP: Unlike DMP, DHP <ref type=\"bibr\" target=\"#b10\">[11]</ref> performs predication only on simple and short hammocks, ta but applies predication dynamically only on less predictable instances. Dynamic Hammock Predication <ref type=\"bibr\" target=\"#b10\">[11]</ref> targets only small, simple hammocks. Hyperblock predicatio"
        },
        {
            "pid": "53e9aca8b7602d97036868cf",
            "content": "DUCTION</head><p>High accuracy of modern branch predictors <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref> has allowed Out-of-Order (OOO) processors to speculate aggre comes of a branch <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Decades of research have made them very accurate. However,"
        },
        {
            "pid": null,
            "content": "ion information. DMP outperformed previous schemes and was the focus of our comparison. Joao et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> extended dynamic predication to indirect branches. Stephen"
        },
        {
            "pid": "558c6c66e4b02b9f07a703d0",
            "content": "bound by the problem of mis-speculation.</p><p>predictors <ref type=\"bibr\" target=\"#b5\">[6]</ref>- <ref type=\"bibr\" target=\"#b7\">[8]</ref>. These branches cost not only performance but also significa it is limited in application only to consistently behaving branches. Control Flow Decoupling (CFD) <ref type=\"bibr\" target=\"#b7\">[8]</ref> is a branch pre-computation based solution which modifies th"
        },
        {
            "pid": null,
            "content": "tion of the control flow removing the need for branch prediction. Store-Load-Branch (SLB) Predictor <ref type=\"bibr\" target=\"#b42\">[43]</ref> is an adjunct branch predictor which improves accuracy by"
        }
    ],
    "5f76f20a91e011f31b98056c": [
        {
            "pid": "5b1642d68fbcbf6e5a9b7e77",
            "content": "e=\"bibr\" target=\"#b7\">Garg et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">May et al., 2010;</ref><ref type=\"bibr\" target=\"#b38\">Zhao et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Rudinger et al"
        },
        {
            "pid": "53e9b35ab7602d9703e36598",
            "content": "ibr\" target=\"#b3\">Caliskan et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Garg et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">May et al., 2010;</ref><ref type=\"bibr\" target=\"#b38\">Zhao et al., 20"
        },
        {
            "pid": "5a73cb7117c44a0b303593e9",
            "content": "=\"bibr\" target=\"#b19\">May et al., 2010;</ref><ref type=\"bibr\" target=\"#b38\">Zhao et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Rudinger et al., 2017)</ref>. Models that have learnt representations"
        },
        {
            "pid": "5ff686c6d4150a363cc36a3f",
            "content": "e of a learnt social bias.</p><p>To approximate p(U |M, \u03b8), we adapt pseudolog-likehood MLM scoring <ref type=\"bibr\" target=\"#b31\">(Wang and Cho, 2019;</ref><ref type=\"bibr\" target=\"#b30\">Salazar et a"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b9f0",
            "content": "m tasks, such as coreference resolution <ref type=\"bibr\" target=\"#b28\">(Rudinger et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Webster et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Dinan et al."
        },
        {
            "pid": null,
            "content": "ontribution. learn and use these biases <ref type=\"bibr\" target=\"#b2\">(Bolukbasi et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Caliskan et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Garg et al., </head><p>Measuring Bias Bias in natural language processing has gained visibility in recent years. <ref type=\"bibr\" target=\"#b3\">Caliskan et al. (2017)</ref> introduce a dataset for evaluating gender"
        },
        {
            "pid": "5e5e195993d709897ce66b01",
            "content": "that workers be in the United States and have a &gt; 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a pay rate of at least $15/hou eted at least 5,000 HITs, and to have greater than a 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a minimum of $15 hourly wage.<"
        },
        {
            "pid": "5550456245ce0a409eb55cee",
            "content": "introduce a dataset for evaluating gender bias in word embeddings. They find that GloVe embeddings <ref type=\"bibr\" target=\"#b23\">(Pennington et al., 2014)</ref> reflect historical gender biases and"
        },
        {
            "pid": null,
            "content": "bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Lan et al., 2020)</ref>. However, these models are trained on minimal 19)</ref>, RoBERTa Large <ref type=\"bibr\" target=\"#b16\">(Liu et al., 2019)</ref>, and ALBERT XXL-v2 <ref type=\"bibr\" target=\"#b14\">(Lan et al., 2020)</ref>. These models have shown good performance on"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b9f0",
            "content": "m tasks, such as coreference resolution <ref type=\"bibr\" target=\"#b28\">(Rudinger et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Webster et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Dinan et al."
        },
        {
            "pid": "599c7978601a182cd2641b24",
            "content": "i et al., 2019)</ref>. The prompts are either premise sentences taken from MultiNLI's fiction genre <ref type=\"bibr\" target=\"#b36\">(Williams et al., 2018)</ref> or 2-3 sentence story openings taken fr , a crowdworker wrote standalone sentences inspired by a prompt that was drawn from either MultiNLI <ref type=\"bibr\" target=\"#b36\">(Williams et al., 2018)</ref> or ROCStories <ref type=\"bibr\" target=\""
        }
    ],
    "5f4f6ec291e0111f07b30a2b": [
        {
            "pid": null,
            "content": "\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref> to automatically designed neural architectures <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t es human experts to frequently try and evaluate numerous different operation and connection options <ref type=\"bibr\" target=\"#b3\">[4]</ref>. In contrast to architectures that are manually designed, th NAS-generated architectures have shown promising results in many domains, such as image recognition <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t rates a total search space of itive training procedure of each selected architecture can be avoided <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> so that researche"
        },
        {
            "pid": "58d82fc8d649053542fd59b8",
            "content": ". They brought great advancements in many applications of neural network, such as visual perception <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib"
        },
        {
            "pid": "5e63725991e011ae97a69d6a",
            "content": "is inspired by transformable architecture search methods <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>. In the size se"
        },
        {
            "pid": "5cede0feda562983788dd6e5",
            "content": "\">[20]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>. It is essentia \">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>. It is essentially not clear if the reported improvements cy. NAS has been dominated by multi-fidelity based methods <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bib rch (RANDOM) <ref type=\"bibr\" target=\"#b46\">[47]</ref>, random search with parameter sharing (RSPS) <ref type=\"bibr\" target=\"#b26\">[27]</ref>. (II) ES methods, e.g., REA <ref type=\"bibr\" target=\"#b5\">"
        },
        {
            "pid": "53e99859b7602d970209567a",
            "content": "we evaluate some typical NAS algorithms: (I) Random Search algorithms, e.g., random search (RANDOM) <ref type=\"bibr\" target=\"#b46\">[47]</ref>, random search with parameter sharing (RSPS) <ref type=\"bi"
        },
        {
            "pid": "5b1642388fbcbf6e5a9b5847",
            "content": "#b11\">[12]</ref>, accuracy prediction <ref type=\"bibr\" target=\"#b37\">[38]</ref>, mutation-based NAS <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, etc.</p><p>Arc Other methods mutate an architecture to become another one <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. With NATS-Bench, researchers could directly use the off-t"
        },
        {
            "pid": "5eccb534e06a4c1b26a83a32",
            "content": "ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <ref type=\"bibr\" target=\"#b43\">[44]</ref>, TuNAS <ref type=\"bibr\" target=\"#b48\">[49]</ref>. (V) HPO methods, e.g., BOHB <ref type=\"bibr\" target=\"#b35 f type=\"bibr\" target=\"#b43\">[44]</ref>. (3) TuNAS samples masks based on the learnable distribution <ref type=\"bibr\" target=\"#b48\">[49]</ref>. TAS and FBNetV2 optimize the architecture parameters in a"
        },
        {
            "pid": "5c04967517c44a2c747089ff",
            "content": "e employed to train this architecture and report the performance, e.g., different data augmentation <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, different regu"
        },
        {
            "pid": "5a73cbc317c44a0b3035ec55",
            "content": "t scheduler <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and different selections of hyper-parameters <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. <ref type=\"bib ing procedure of each selected architecture can be avoided <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> so that researchers can target on the essence of NAS, i.e."
        },
        {
            "pid": "5a9cb65d17c44a376ffb820b",
            "content": "ral architectures <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t image recognition <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeling <ref type=\"bibr\" target=\"#b4\">[5]</ref generate parameters of an architecture. Other methods mutate an architecture to become another one <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. With NATS-Bench, spective of both performance and efficiency. NAS has been dominated by multi-fidelity based methods <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" with parameter sharing (RSPS) <ref type=\"bibr\" target=\"#b26\">[27]</ref>. (II) ES methods, e.g., REA <ref type=\"bibr\" target=\"#b5\">[6]</ref>. (III) RL algorithms, e.g., REINFORCE <ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "\">[18]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bib"
        },
        {
            "pid": null,
            "content": "[15]</ref>, and different selections of hyper-parameters <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. <ref type=\"bibr\" target=\"#b2\">(3)</ref> The validation se =\"#b7\">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <"
        },
        {
            "pid": null,
            "content": "<ref type=\"bibr\" target=\"#b0\">[1]</ref>, Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>, VGGNet <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and Transformer <ref type=\"bibr\" target=\"#b9\">[10]</ref>. H"
        },
        {
            "pid": null,
            "content": "=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. In its early stage f type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Recently, a f type=\"bibr\" target=\"#b23\">[24]</ref>, language modelling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, etc. Despite their efines operation candidates on the node, whereas we associate operations on the edge as inspired by <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t y Search Space S t . The topology search space is inspired by the popular cell-based NAS algorithms <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t e API. The implementation difference between DARTS <ref type=\"bibr\" target=\"#b7\">[8]</ref> and GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref> is only less than 20 lines of code. Our library reduces the order DARTS (DARTS-V1) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref ty cy. Such strategy is more robust than using the arg max over the learned architecture parameters in <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. <ref type=\"bibr\" t"
        },
        {
            "pid": "5b1642388fbcbf6e5a9b5847",
            "content": "#b11\">[12]</ref>, accuracy prediction <ref type=\"bibr\" target=\"#b37\">[38]</ref>, mutation-based NAS <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, etc.</p><p>Arc Other methods mutate an architecture to become another one <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. With NATS-Bench, researchers could directly use the off-t"
        },
        {
            "pid": null,
            "content": "[15]</ref>, and different selections of hyper-parameters <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. <ref type=\"bibr\" target=\"#b2\">(3)</ref> The validation se =\"#b7\">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <"
        },
        {
            "pid": "5eccb534e06a4c1b26a83a32",
            "content": "ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <ref type=\"bibr\" target=\"#b43\">[44]</ref>, TuNAS <ref type=\"bibr\" target=\"#b48\">[49]</ref>. (V) HPO methods, e.g., BOHB <ref type=\"bibr\" target=\"#b35 f type=\"bibr\" target=\"#b43\">[44]</ref>. (3) TuNAS samples masks based on the learnable distribution <ref type=\"bibr\" target=\"#b48\">[49]</ref>. TAS and FBNetV2 optimize the architecture parameters in a"
        },
        {
            "pid": null,
            "content": "=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. In its early stage f type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Recently, a f type=\"bibr\" target=\"#b23\">[24]</ref>, language modelling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, etc. Despite their efines operation candidates on the node, whereas we associate operations on the edge as inspired by <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t y Search Space S t . The topology search space is inspired by the popular cell-based NAS algorithms <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t e API. The implementation difference between DARTS <ref type=\"bibr\" target=\"#b7\">[8]</ref> and GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref> is only less than 20 lines of code. Our library reduces the order DARTS (DARTS-V1) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref ty cy. Such strategy is more robust than using the arg max over the learned architecture parameters in <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. <ref type=\"bibr\" t"
        },
        {
            "pid": "5a260c8417c44a4ba8a31511",
            "content": "eport the performance, e.g., different data augmentation <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, different regularization <ref type=\"bibr\" target=\"#b10\">["
        },
        {
            "pid": "573696026e3b12023e515eec",
            "content": "><p>T HE deep learning community is undergoing a transition from hand-designed neural architectures <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" t ss of deep learning was promoted by the introductions of novel neural architectures, such as ResNet <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>, VGGNet < s the same topology. The intermediate residual block is the basic residual block with a stride of 2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, which serves to down-sample the spatial size and double the ur NATS-Bench, we follow previous literature to set up the hyper-parameters and training strategies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" iginal ImageNet to 16\u00d716 pixels to form ImageNet16\u00d716, from which we select all images with label \u2208 <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\">120]</ref> to construct ImageNet-16-120. In"
        },
        {
            "pid": null,
            "content": "[15]</ref>, and different selections of hyper-parameters <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. <ref type=\"bibr\" target=\"#b2\">(3)</ref> The validation se =\"#b7\">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <"
        },
        {
            "pid": "5c757217f56def97987db58b",
            "content": "lgorithms, such as platformaware NAS <ref type=\"bibr\" target=\"#b11\">[12]</ref>, accuracy prediction <ref type=\"bibr\" target=\"#b37\">[38]</ref>, mutation-based NAS <ref type=\"bibr\" target=\"#b38\">[39]</r to predict the final accuracy of an architecture based on the results of few early training epochs <ref type=\"bibr\" target=\"#b37\">[38]</ref>. These algorithms can be trained faster and the performanc 5\">[6]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, which learn to search based on an approximation of the pe"
        }
    ],
    "5f02f17c91e011ee5e0258c8": [
        {
            "pid": "5ce2d032ced107d4c635260c",
            "content": "directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Intuitively, propagation based on personalized PageRank c here the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>'s approach does not easily scale to large graphs since it ng that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> suggest decoupling the feature transformation from the pro instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> used K = 10 to achieve a good approximation) is prohibitiv ency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which experimentally shows higher classification performan . In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which we build upon. The results are summarized in Table < ich experimentally shows higher classification performance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine ba"
        },
        {
            "pid": "5b16426b8fbcbf6e5a9b5cd3",
            "content": "get=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Random walk sampling <ref type=\"bibr\" target=\"#b18\"> type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. For example Wei et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> propose the TopPPR algorithm combining the strengths of ra"
        },
        {
            "pid": "5b67b45517c44aac1c8607aa",
            "content": "et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar h, e.g. based on different importance scores for the nodes <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar ibr\" target=\"#b53\">54]</ref>. <ref type=\"foot\" target=\"#foot_1\">3</ref> Beyond sampling, Gao et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> collect the representations from a node's neighborhood int"
        },
        {
            "pid": "53e9afb4b7602d9703a0458c",
            "content": "eRank. Luckily, given the broad applicability of PageRank, many such algorithms have been developed <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> and backward search <ref type=\"bibr\" target=\"#b3\">[4]</ref> can be viewed as deterministic variants of the random walk s"
        },
        {
            "pid": "5b67b45517c44aac1c8607aa",
            "content": "et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar h, e.g. based on different importance scores for the nodes <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar ibr\" target=\"#b53\">54]</ref>. <ref type=\"foot\" target=\"#foot_1\">3</ref> Beyond sampling, Gao et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> collect the representations from a node's neighborhood int"
        },
        {
            "pid": "53e9bc4eb7602d97048c2071",
            "content": "target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar Rank vector up to a given precision using a filterand-refine paradigm. Another family of approaches <ref type=\"bibr\" target=\"#b19\">[20]</ref> are based on the idea of maintaining upper and lower bound"
        },
        {
            "pid": "57aa28dd0a3ac518da9896a6",
            "content": "ing.</p><p>Their applications occur across all media types and power many different Google products <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5b67b45517c44aac1c8607aa",
            "content": "et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar h, e.g. based on different importance scores for the nodes <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar ibr\" target=\"#b53\">54]</ref>. <ref type=\"foot\" target=\"#foot_1\">3</ref> Beyond sampling, Gao et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> collect the representations from a node's neighborhood int"
        },
        {
            "pid": "5b67b45517c44aac1c8607aa",
            "content": "et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar h, e.g. based on different importance scores for the nodes <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar ibr\" target=\"#b53\">54]</ref>. <ref type=\"foot\" target=\"#foot_1\">3</ref> Beyond sampling, Gao et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> collect the representations from a node's neighborhood int"
        },
        {
            "pid": "5b16426b8fbcbf6e5a9b5cd3",
            "content": "get=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Random walk sampling <ref type=\"bibr\" target=\"#b18\"> type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. For example Wei et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> propose the TopPPR algorithm combining the strengths of ra"
        },
        {
            "pid": "57aa28dd0a3ac518da9896a6",
            "content": "ing.</p><p>Their applications occur across all media types and power many different Google products <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta"
        }
    ],
    "5f0d8b6891e011047aff993b": [
        {
            "pid": "5ce3af9aced107d4c65f6b80",
            "content": "\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The main concept behind these approaches is to interpret s successfully transferred from NLP to protein sequences <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, with the excep f their surrounding context (residues next to it). As previously established for another protein LM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, the t-SNE projections (e.g. ProtBert Fig. <ref type=\"figu \">[20]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, we might expect an upper limit for what protein LMs can l"
        },
        {
            "pid": null,
            "content": "\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bib"
        },
        {
            "pid": "5eeb1b339e795e28ab4bc05e",
            "content": "ein sequences <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, with the exception of the number of layers that was incre"
        },
        {
            "pid": null,
            "content": "dels (including Bert and Albert). With the average length of an English sentence around 15-30 words <ref type=\"bibr\" target=\"#b50\">[51]</ref>, an upper sentence length limit is no problem for sentence"
        },
        {
            "pid": "53e9b042b7602d9703aa05f4",
            "content": "dvanced libraries <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t"
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": "=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> enable the training of ever more complex models on bigger da TPU Pod <ref type=\"bibr\" target=\"#b1\">[2]</ref>, combined with optimized libraries such as IBM DDL <ref type=\"bibr\" target=\"#b6\">[7]</ref> and Horovod <ref type=\"bibr\" target=\"#b5\">[6]</ref> set the"
        },
        {
            "pid": "5550456245ce0a409eb55cee",
            "content": "Q3) and over non-contextualized word2vec-type approaches <ref type=\"bibr\" target=\"#b63\">[64]</ref>, <ref type=\"bibr\" target=\"#b64\">[65]</ref>, <ref type=\"bibr\" target=\"#b65\">[66]</ref> (12-17 percenta"
        },
        {
            "pid": null,
            "content": "hine learning <ref type=\"bibr\" target=\"#b33\">[34]</ref>, <ref type=\"bibr\" target=\"#b68\">[69]</ref>, <ref type=\"bibr\" target=\"#b69\">[70]</ref>, <ref type=\"bibr\" target=\"#b70\">[71]</ref>. The gain in in"
        },
        {
            "pid": null,
            "content": "ects of protein function. Using the same proteins as for SCOPe but different annotations (ECnumbers <ref type=\"bibr\" target=\"#b59\">[60]</ref>), we assessed whether the LM embeddings captured aspects o"
        },
        {
            "pid": "5dfb4a97df1a9c0c4164fbf3",
            "content": "anguage of life through advanced LMs trained on proteins <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bib s through the performance of the supervised tasks. The supervised models have been described before <ref type=\"bibr\" target=\"#b16\">[17]</ref>. To briefly summarize: we applied tasks on two different l zed (ProtVec <ref type=\"bibr\" target=\"#b60\">[61]</ref>) as well as existing, LSTM-based LMs (SeqVec <ref type=\"bibr\" target=\"#b16\">[17]</ref>), all LMs trained here still fall short compared to method in prediction of localization and membrane/non-membrane). Overall, the supervised results confirmed <ref type=\"bibr\" target=\"#b16\">[17]</ref> that evolutionary information scientifically and statistic input to those approaches. Newer contextual models improved both over previous LM-based approaches <ref type=\"bibr\" target=\"#b16\">[17]</ref> (3-5 percentage points in Q3) and over non-contextualized nse increase, the highest performance increase remained rather limited with respect to existing LMs <ref type=\"bibr\" target=\"#b16\">[17]</ref> (\u2206Q3=Q3(ProtBert-BFD)-Q3(SeqVec)=4.7%) despite a significa for much slower are amiss. Nevertheless, given the experiments described here and in previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bib"
        },
        {
            "pid": null,
            "content": "2]</ref>), TS115 (115 proteins; <ref type=\"bibr\" target=\"#b42\">[43]</ref>) and CASP12 (21 proteins; <ref type=\"bibr\" target=\"#b43\">[44]</ref>).</p><p>Per-protein prediction: For the prediction of feat"
        }
    ],
    "5f69cfbb91e011a2f02706bb": [
        {
            "pid": "53e9a12ab7602d9702a1d01f",
            "content": "the support of better branch prediction, which could potentially offer more IPC gains. Prior works <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref> have tried to a hose address is very predictable. Moreover, we do not make any modifications to the ISA. Gao et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a closely related work. They correlate the branch"
        },
        {
            "pid": "539087a120f70186a0d4773a",
            "content": "ised the bar for the prediction accuracy. Yeh and Patt came up with the two-level branch predictors <ref type=\"bibr\" target=\"#b24\">[25]</ref>. McFarling <ref type=\"bibr\" target=\"#b25\">[26]</ref> propo"
        },
        {
            "pid": null,
            "content": "level of accuracy.</p><p>Energy Per Access (EPA) for IMLI and LDBP were calculated using CACTI 6.0 <ref type=\"bibr\" target=\"#b22\">[23]</ref>. For IMLI, we model an ideal structure with a single port."
        },
        {
            "pid": "5d04eeba8607575390f83f44",
            "content": "but the loads in the speculative path can still leak unless speculative loads are protected like in <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The result is that LDBP is not a new source of speculativ"
        },
        {
            "pid": "53e9ba11b7602d9704614c21",
            "content": "pture the history of such branches competently, even with an unusually large predictor. Prior works <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> show that using"
        },
        {
            "pid": "53e999cab7602d970220f327",
            "content": "tween the outcome of the current branch and the history of previous branch outcomes.</p><p>PPM-like <ref type=\"bibr\" target=\"#b26\">[27]</ref> and TAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref> achieve h"
        },
        {
            "pid": "53e99d8eb7602d97026490bf",
            "content": "CPU designs use either perceptron-based predictors <ref type=\"bibr\" target=\"#b0\">[1]</ref> [2] [3] <ref type=\"bibr\" target=\"#b3\">[4]</ref> or TAGEbased predictors <ref type=\"bibr\" target=\"#b4\">[5]</r"
        },
        {
            "pid": "539087a120f70186a0d4773a",
            "content": "ised the bar for the prediction accuracy. Yeh and Patt came up with the two-level branch predictors <ref type=\"bibr\" target=\"#b24\">[25]</ref>. McFarling <ref type=\"bibr\" target=\"#b25\">[26]</ref> propo"
        },
        {
            "pid": "557c906af66765fbb46b6f2d",
            "content": "0]</ref> proposed prediction mechanisms to tackle loop-termination branches. The Wormhole predictor <ref type=\"bibr\" target=\"#b30\">[31]</ref> improved on earlier loop-based predictors to handle branch"
        },
        {
            "pid": "5736982b6e3b12023e6fd328",
            "content": "10]</ref> using a 256-Kbit TAGE-based predictor. For this work, we use the 256-Kbit TAGE-GSC + IMLI <ref type=\"bibr\" target=\"#b10\">[11]</ref>, which combines the global history components of the TAGE- random to capture.</p><p>Statistical correlator <ref type=\"bibr\" target=\"#b27\">[28]</ref> and IMLI <ref type=\"bibr\" target=\"#b10\">[11]</ref> components are augmented to TAGE to mitigate some of the m"
        },
        {
            "pid": "53e9ba11b7602d9704614c21",
            "content": "pture the history of such branches competently, even with an unusually large predictor. Prior works <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> show that using"
        }
    ],
    "5f3cf98391e011c89f2f178c": [
        {
            "pid": "5d4d46fb3a55acff992fde49",
            "content": "equential recommenders <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It has been demonstrated that contextual information is im ich is able to achieve the same effect as previous methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Besides, the pre-trained data representations can be also them by the interaction timestamps ascendingly. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we only keep the 5-core datasets, and filter unpopular ite te the performance, which are widely used in related works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Since HR@1 is equal to NDCG@1, we report results on HR@{1, ation Machines to incorporate arbitrary real-valued features to the sequential recommendation. FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> employed a feature-level self-attention block to leverage ttribute-aware sequential models such as TransFM <ref type=\"bibr\" target=\"#b15\">[16]</ref> and FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> leverage the contextual features to improve the sequential and attribute as the input to the model. ( <ref type=\"formula\" target=\"#formula_16\">11</ref>) FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> constructs a feature sequence and uses a featurelevel self"
        },
        {
            "pid": "5cede10dda562983788ed645",
            "content": "tc. There are also studies that leverage other architectures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> for sequential reco xt from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n owing previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we apply the leave-oneout strategy for evaluation. Concret model, which uses the multi-head attention mechanism to recommend the next item.</p><p>(7) BERT4Rec <ref type=\"bibr\" target=\"#b22\">[23]</ref> uses a Cloze objective loss for sequential recommendation"
        },
        {
            "pid": null,
            "content": "ive loss for sequential recommendation by the bidirectional self-attention mechanism.</p><p>(8) HGN <ref type=\"bibr\" target=\"#b12\">[13]</ref> is recently proposed and adopts hierarchical gating networ"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "not been well captured in data representations. As shown in increasing evidence from various fields <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> is a newly emerging tp://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Self-supervised Learning</head><p>Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target guide the visual feature learning <ref type=\"bibr\" target=\"#b4\">[5]</ref>. As for language modeling <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmln or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref> and natural language understanding <ref type=\"bibr\" target=\" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, at the pre-training stage, we remove the mask mechanism to deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose to model the bidirectional information in item s"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf31719",
            "content": "t=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">"
        },
        {
            "pid": null,
            "content": "@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we apply the leave"
        },
        {
            "pid": null,
            "content": "@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we apply the leave"
        },
        {
            "pid": null,
            "content": "mization (MIM) method <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. It has been shown ormation maximization <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> is a special branch of the self-supervised learning. It is </ref> is a special branch of the self-supervised learning. It is inspired by the InfoMax principle <ref type=\"bibr\" target=\"#b10\">[11]</ref> and has made important progress in several domains such as"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf31719",
            "content": "t=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">"
        },
        {
            "pid": null,
            "content": "died in the literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar Early works on sequential recommendation are based on the Markov Chain assumption. MC-based methods <ref type=\"bibr\" target=\"#b19\">[20]</ref> estimated an item-item transition probability matrix and u ork. We make a brief discussion below.</p><p>Feature-based approaches such as Factorization Machine <ref type=\"bibr\" target=\"#b19\">[20]</ref> and AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> main ethod that ranks items according to popularity measured by the number of interactions.</p><p>(2) FM <ref type=\"bibr\" target=\"#b19\">[20]</ref> characterizes the pairwise interactions between variables"
        },
        {
            "pid": null,
            "content": "mization (MIM) method <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. It has been shown ormation maximization <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> is a special branch of the self-supervised learning. It is </ref> is a special branch of the self-supervised learning. It is inspired by the InfoMax principle <ref type=\"bibr\" target=\"#b10\">[11]</ref> and has made important progress in several domains such as"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "not been well captured in data representations. As shown in increasing evidence from various fields <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> is a newly emerging tp://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Self-supervised Learning</head><p>Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target guide the visual feature learning <ref type=\"bibr\" target=\"#b4\">[5]</ref>. As for language modeling <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmln or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref> and natural language understanding <ref type=\"bibr\" target=\" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, at the pre-training stage, we remove the mask mechanism to deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose to model the bidirectional information in item s"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf31719",
            "content": "t=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">"
        },
        {
            "pid": "573696c56e3b12023e5c58be",
            "content": ") Amazon Beauty, Sports, and Toys: these three datasets are obtained from Amazon review datasets in <ref type=\"bibr\" target=\"#b13\">[14]</ref>. In this work, we select three subcategories: \"Beauty\", \"S"
        },
        {
            "pid": "5ee3526a91e011cb3bff739b",
            "content": "16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">[27]</ref>, etc. There are also studies that leverage other architect"
        },
        {
            "pid": null,
            "content": "died in the literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar Early works on sequential recommendation are based on the Markov Chain assumption. MC-based methods <ref type=\"bibr\" target=\"#b19\">[20]</ref> estimated an item-item transition probability matrix and u ork. We make a brief discussion below.</p><p>Feature-based approaches such as Factorization Machine <ref type=\"bibr\" target=\"#b19\">[20]</ref> and AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> main ethod that ranks items according to popularity measured by the number of interactions.</p><p>(2) FM <ref type=\"bibr\" target=\"#b19\">[20]</ref> characterizes the pairwise interactions between variables"
        },
        {
            "pid": "5bdc31b817c44a1f58a0bee9",
            "content": "on way is easy to suffer from issues such as data sparsity <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>a ased approaches such as Factorization Machine <ref type=\"bibr\" target=\"#b19\">[20]</ref> and AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> mainly learn data representations through the interaction characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> utilizes the multi-head self-attentive neural network to l"
        },
        {
            "pid": "5bbacbad17c44aecc4eb00ee",
            "content": "ial recommendation has been widely studied in the literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targe 4]</ref>.</p><p>Typically, sequential recommendation methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" targe s of works follow this line and extend it for high-order MCs <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. With the development ad><p>Existing studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> mainly emphasize the d is a parameter matrix to learn. Note that existing methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> seldom directly model nal neural networks (CNNs) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and self-attention mechanisms <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been proposed to learn good representations of user pre commendation with MIM, which is called S 3 -Rec. Based on a self-attentive recommender architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>, we propose to first pre-train the sequential recommender wi introduce the base model of our proposed approach that is developed on the Transformer architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Then, we will describe how we utilize the correlation signa tions.</p><p>Sequential models such as GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> mainly focus on modeling the sequential dependencies between ng horizontal and vertical convolutional operations for sequential recommendation.</p><p>(6) SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> is a self-attention based sequential recommendation model, w heads as 2. The dimension of the embedding is 64, and the maximum sequence length is 50 (following <ref type=\"bibr\" target=\"#b7\">[8]</ref>). Note that our training phase contains two stages (i.e., pr type=\"bibr\" target=\"#b26\">[27]</ref>, etc. There are also studies that leverage other architectures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ tem prediction loss L M I P in Eq. 12 has a similar effect to capture sequential dependencies as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> except that it can a qual to NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ ems as candidates for testing. Following the common strategy <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, we pair the ground-truth item with 99 randomly sampled negat"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf31719",
            "content": "t=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">"
        },
        {
            "pid": "573696c56e3b12023e5c58be",
            "content": ") Amazon Beauty, Sports, and Toys: these three datasets are obtained from Amazon review datasets in <ref type=\"bibr\" target=\"#b13\">[14]</ref>. In this work, we select three subcategories: \"Beauty\", \"S"
        },
        {
            "pid": "53e997ddb7602d9701fd1efa",
            "content": "target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>Typically, s recommendation methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> capture useful sequ It has been found that such an optimization way is easy to suffer from issues such as data sparsity <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>.</p></div> <div xm the interaction records by users and sort them by the interaction timestamps ascendingly. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we only keep the and Mean Reciprocal Rank (MRR) to evaluate the performance, which are widely used in related works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Since HR@1 is equ enhance data representations instead of making predictions.</p><p>Sequential models such as GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> mainly as a similar effect to capture sequential dependencies as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> except that it can also utilize bidirectional sequential in"
        },
        {
            "pid": "5c5c55bfe1cd8e03e716895e",
            "content": "(such as item attributes) to neural sequential recommenders <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It has been demonstr el by introducing pair-wise loss functions <ref type=\"bibr\" target=\"#b3\">[4]</ref>, memory networks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, hierarchical structur"
        }
    ],
    "5fe30a2291e01125d4b5b5e3": [
        {
            "pid": "5e09a9a8df1a9c0c416aa5ea",
            "content": "g loops of neural networks, blurring different dimensions of the problem. By contrast -similarly to <ref type=\"bibr\" target=\"#b35\">Parshakova et al. (2019a;</ref><ref type=\"bibr\">b)</ref> in a differe =\"bibr\" target=\"#b23\">(Kim &amp; Bengio, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Owen, 2013;</ref><ref type=\"bibr\" target=\"#b35\">Parshakova et al., 2019a</ref>) SNIS consists in computing:</p><formu et=\"#b4\">Belanger &amp; McCallum, 2016)</ref>. Some current applications to text generation include <ref type=\"bibr\" target=\"#b35\">Parshakova et al. (2019a)</ref> and <ref type=\"bibr\" target=\"#b14\">De"
        },
        {
            "pid": null,
            "content": "ential model towards optimizing some global reward over the generated text. This includes REINFORCE <ref type=\"bibr\" target=\"#b55\">(Williams, 1992a)</ref> for Machine translation (MT) Ranzato et al. ("
        },
        {
            "pid": "53e9b929b7602d9704512348",
            "content": ""
        },
        {
            "pid": null,
            "content": "www.tei-c.org/ns/1.0\"><head>A.3.1 TRANSITIVITY PROPERTY OF GENERALIZED MAXENT</head><p>According to <ref type=\"bibr\" target=\"#b11\">(Csisz\u00e1r, 1996)</ref>, the Generalized MaxEnt of sections \u00a72.1 and \u00a72 associated with p can be directly reused as the first lambdas of the k lambda's associated with p . <ref type=\"bibr\" target=\"#b11\">(Csisz\u00e1r, 1996)</ref> gives only a minimal proof sketch, but it is in"
        },
        {
            "pid": "558c071fe4b00c3c48dfd4f9",
            "content": "to a unique solution P (x). P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) <ref type=\"bibr\" target=\"#b16\">(Hinton, 2002;</ref><ref type=\"bibr\" target=\"#b26\">LeCun et al., 2006 re sample diversity is a requirement.</p><p>Energy Based Models for Text Energy-Based Models (EBMs) <ref type=\"bibr\" target=\"#b16\">(Hinton, 2002;</ref><ref type=\"bibr\" target=\"#b26\">LeCun et al., 2006"
        },
        {
            "pid": null,
            "content": "picuous in pretrained language models. <ref type=\"bibr\" target=\"#b51\">(Stanovsky et al., 2019;</ref><ref type=\"bibr\" target=\"#b40\">Prates et al., 2020;</ref><ref type=\"bibr\" target=\"#b47\">Sheng et al."
        },
        {
            "pid": "558c071fe4b00c3c48dfd4f9",
            "content": "to a unique solution P (x). P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) <ref type=\"bibr\" target=\"#b16\">(Hinton, 2002;</ref><ref type=\"bibr\" target=\"#b26\">LeCun et al., 2006 re sample diversity is a requirement.</p><p>Energy Based Models for Text Energy-Based Models (EBMs) <ref type=\"bibr\" target=\"#b16\">(Hinton, 2002;</ref><ref type=\"bibr\" target=\"#b26\">LeCun et al., 2006"
        },
        {
            "pid": "5e5e18ca93d709897ce315f0",
            "content": "tions to text generation include <ref type=\"bibr\" target=\"#b35\">Parshakova et al. (2019a)</ref> and <ref type=\"bibr\" target=\"#b14\">Deng et al. (2020)</ref>, who augment a standard autoregressive LM wi"
        },
        {
            "pid": null,
            "content": "ed distribution, aka an Energy-Based Model (EBM) <ref type=\"bibr\" target=\"#b16\">(Hinton, 2002;</ref><ref type=\"bibr\" target=\"#b26\">LeCun et al., 2006;</ref><ref type=\"bibr\" target=\"#b3\">Bakhtin et al. Based Models for Text Energy-Based Models (EBMs) <ref type=\"bibr\" target=\"#b16\">(Hinton, 2002;</ref><ref type=\"bibr\" target=\"#b26\">LeCun et al., 2006;</ref><ref type=\"bibr\" target=\"#b42\">Ranzato et al \" place=\"foot\" n=\"6\" xml:id=\"foot_5\">Let's clarify here that the class of Energy-Based Models (EBMs)<ref type=\"bibr\" target=\"#b26\">(LeCun et al., 2006)</ref> is much larger than the exponential family"
        },
        {
            "pid": null,
            "content": "ion distributions across biographies generated by a GPT-2 model fine-tuned on Wikipedia Biographies <ref type=\"bibr\" target=\"#b25\">(Lebret et al., 2016)</ref> (henceforth GPT-2 bio ) ( \u00a7G gives additi ne-tune GPT-2 small (117M params) to produce biographies on a dataset of 700K Wikipedia biographies <ref type=\"bibr\" target=\"#b25\">(Lebret et al., 2016)</ref> which we refer to as GPT-2 bio . To detec"
        },
        {
            "pid": null,
            "content": ""
        }
    ],
    "5f61e88391e011fae8fd6b13": [
        {
            "pid": "5e281dc13a55ac4d187e0b1e",
            "content": "y or with only very few labeled examples <ref type=\"bibr\" target=\"#b26\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze, 2020a)</ref>.</p><p>Very recently, <ref type=\"bib mited to a few hundred tokens.</p><p>An alternative to priming is pattern-exploiting training (PET) <ref type=\"bibr\" target=\"#b30\">(Schick and Sch\u00fctze, 2020a)</ref>, which combines the idea of reformu are understood well by LMs is difficult <ref type=\"bibr\" target=\"#b12\">(Jiang et al., 2019)</ref>, <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref> propose PET, a method that uses know =\"#b4\">(Dagan et al., 2006)</ref> are textual entailment tasks like MNLI, so we use PVPs similar to <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>. For a premise p and hypothesis h, w ts. 5 We run PET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>. For COPA, WSC and ReCoRD, we use ou ref>). Given 32 examples, PET clearly outperforms both baselines, which is in line with findings by <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>.</p><p>We next compare PET directly t segments and mark p in s with asterisks.</p><p>MultiRC Deviating from the hyperparameters used by <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2019)</ref>, we use a maximum sequence length of"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "mentation of PET by Schick and Sch\u00fctze (2020a) which, in turn, is based on the Transformers library <ref type=\"bibr\" target=\"#b39\">(Wolf et al., 2019)</ref> and PyTorch <ref type=\"bibr\" target=\"#b21\">"
        },
        {
            "pid": "5b1642a68fbcbf6e5a9b7dcb",
            "content": "es some modifications during training and inference that are discussed in Appendix A.</p><p>MultiRC <ref type=\"bibr\" target=\"#b13\">(Khashabi et al., 2018</ref>) is a QA task. Given a passage p, a ques"
        },
        {
            "pid": null,
            "content": "f type=\"bibr\" target=\"#b26\">Radford et al. (2019)</ref> and has been applied to text classification <ref type=\"bibr\" target=\"#b24\">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref t"
        },
        {
            "pid": null,
            "content": "ation <ref type=\"bibr\" target=\"#b24\">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019)</ref> and argumentative relation classification"
        },
        {
            "pid": "599c7978601a182cd2641b24",
            "content": "ning sets for RTE and CB are very small, we additionally select random unlabeled examples from MNLI <ref type=\"bibr\" target=\"#b38\">(Williams et al., 2018)</ref> for both tasks. We refer to the resulti"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5e09a9d7df1a9c0c416afc7f",
            "content": "has previously been investigated by <ref type=\"bibr\" target=\"#b29\">Salazar et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b10\">Ghazvininejad et al. (2019)</ref>.</p></div> <div xmlns=\"http://www.t"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "mprovements on a wide range of NLP tasks <ref type=\"bibr\" target=\"#b25\">(Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b7\">Devlin et al., 2019;</ref><ref type=\"bibr\">Liu et al., 2019;</ref><ref t no parameter updates are performed.</p><p>Our modified version of PET uses masked language models <ref type=\"bibr\" target=\"#b7\">(Devlin et al., 2019)</ref> to assign probabilities to sequences of te"
        },
        {
            "pid": null,
            "content": "probing the knowledge contained within LMs <ref type=\"bibr\" target=\"#b33\">(Trinh and Le, 2018;</ref><ref type=\"bibr\" target=\"#b22\">Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Talmor et a"
        }
    ],
    "5f0277e911dc830562231dab": [
        {
            "pid": "5b67b45517c44aac1c8607e7",
            "content": "f stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm<ref type=\"bibr\" target=\"#b12\">[13]</ref>, a teacher network and a student network. The DGBR algorit ner is another promising direction.</p><p>Feature-Based Module. The current stable feature approach <ref type=\"bibr\" target=\"#b12\">[13]</ref> needs much time and computing resources. For implementing"
        },
        {
            "pid": "58d82fcbd649053542fd658e",
            "content": "><ref type=\"bibr\" target=\"#b5\">6]</ref>, previous model bias <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and position bias < niform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. In this paper, we f a recommender system, and that explicitly handling of the biases may help improve the performance <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta f>.</p><p>A recent work has shown that a uniform data can alleviate the previous model bias problem <ref type=\"bibr\" target=\"#b15\">[16]</ref>. But the uniform data is always few and expensive to colle us on how to solve the bias problems in a recommender system with a uniform data. Along the line of <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we conduct empirical studies on a real advertising system /p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">MOTIVATION</head><p>In a recent work <ref type=\"bibr\" target=\"#b15\">[16]</ref>, it is shown that a uniform (i.e., unbiased) data can alle"
        },
        {
            "pid": "5b67b45517c44aac1c860823",
            "content": "knowledge distillation and recommender systems has also attracted the attention of the researchers <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5736960c6e3b12023e51ffa7",
            "content": "25]</ref> and heuristic-based approaches. The former mainly uses the inverse propensity score (IPS) <ref type=\"bibr\" target=\"#b23\">[24]</ref> and the counterfactual risk minimization (CRM) principle < such as imputation model learning <ref type=\"bibr\" target=\"#b32\">[33]</ref>, propensity computation <ref type=\"bibr\" target=\"#b23\">[24]</ref> and modeling with uniform data directly <ref type=\"bibr\" t esentative counterfactual-based recommendation method as the second low-rank baseline, i.e., IPS-MF <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Note that we estimate the propensity scores via the na\u00efve arget=\"#b27\">28]</ref>. IPS is one of the most popular counterfactual approaches for recommendation <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, where each sample or learning an imputation model and its variants. Sample-based distillation includes the IPS method <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> and other approach ick and rate items as they wish. This can be considered as a stochastic logging policy by following <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, and thus the user"
        },
        {
            "pid": null,
            "content": "t works are no longer limited to model structure, but considers sample-based knowledge distillation <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. In this paper, we tudy rather than the past knowledge distillation approaches such as considering the level of sample <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> and model structur"
        },
        {
            "pid": "56d90058dabfae2eeed34113",
            "content": "adaptive, collective and integrative) in transfer learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>In addition, we must keep in mind that the different"
        },
        {
            "pid": "5dbebb7447c8f766462c22d2",
            "content": ". Moreover, a uniform data is useful for counterfactual learning, such as imputation model learning <ref type=\"bibr\" target=\"#b32\">[33]</ref>, propensity computation <ref type=\"bibr\" target=\"#b23\">[24 methods, such as the doubly robust method <ref type=\"bibr\" target=\"#b6\">[7]</ref> and its variants <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Moreover, they are also related to the types of knowledge st set (\ud835\udc46 \ud835\udc61\ud835\udc52 ). Following the settings of the previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we employ two evaluation metrics that are widely used in i"
        },
        {
            "pid": "56d90058dabfae2eeed34113",
            "content": "adaptive, collective and integrative) in transfer learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>In addition, we must keep in mind that the different"
        },
        {
            "pid": "599c7960601a182cd2636717",
            "content": "nsity computation <ref type=\"bibr\" target=\"#b23\">[24]</ref> and modeling with uniform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ idation set (\ud835\udc46 \ud835\udc63\ud835\udc4e ), and the rest as test set (\ud835\udc46 \ud835\udc61\ud835\udc52 ). Following the settings of the previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we employ two evalu istillation can be realized.</p><p>\u2022 Causal Embedding Strategy (CausE). The causal embedding method <ref type=\"bibr\" target=\"#b4\">[5]</ref> first considers the scenario of training \ud835\udc40 \ud835\udc50 and \ud835\udc40 \ud835\udc61 simulta"
        },
        {
            "pid": "56d90058dabfae2eeed34113",
            "content": "adaptive, collective and integrative) in transfer learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>In addition, we must keep in mind that the different"
        },
        {
            "pid": "5dbebb7447c8f766462c22d2",
            "content": ". Moreover, a uniform data is useful for counterfactual learning, such as imputation model learning <ref type=\"bibr\" target=\"#b32\">[33]</ref>, propensity computation <ref type=\"bibr\" target=\"#b23\">[24 methods, such as the doubly robust method <ref type=\"bibr\" target=\"#b6\">[7]</ref> and its variants <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Moreover, they are also related to the types of knowledge st set (\ud835\udc46 \ud835\udc61\ud835\udc52 ). Following the settings of the previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we employ two evaluation metrics that are widely used in i"
        },
        {
            "pid": "599c7960601a182cd2636717",
            "content": "nsity computation <ref type=\"bibr\" target=\"#b23\">[24]</ref> and modeling with uniform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ idation set (\ud835\udc46 \ud835\udc63\ud835\udc4e ), and the rest as test set (\ud835\udc46 \ud835\udc61\ud835\udc52 ). Following the settings of the previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we employ two evalu istillation can be realized.</p><p>\u2022 Causal Embedding Strategy (CausE). The causal embedding method <ref type=\"bibr\" target=\"#b4\">[5]</ref> first considers the scenario of training \ud835\udc40 \ud835\udc50 and \ud835\udc40 \ud835\udc61 simulta"
        },
        {
            "pid": "5a9cb60d17c44a376ffb3c63",
            "content": "f><ref type=\"bibr\" target=\"#b16\">17]</ref> and position bias <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Previous studies have shown that models and evaluation met position bias estimation methods for ranking are proposed in <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. IPS is one of the most popular counterfactual approaches f ly handling of the biases may help improve the performance <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Most of the previo"
        },
        {
            "pid": "5a9cb60d17c44a376ffb3c63",
            "content": "f><ref type=\"bibr\" target=\"#b16\">17]</ref> and position bias <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Previous studies have shown that models and evaluation met position bias estimation methods for ranking are proposed in <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. IPS is one of the most popular counterfactual approaches f ly handling of the biases may help improve the performance <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Most of the previo"
        },
        {
            "pid": null,
            "content": "<ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, previous model bias <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "5dbebb7447c8f766462c22d2",
            "content": ". Moreover, a uniform data is useful for counterfactual learning, such as imputation model learning <ref type=\"bibr\" target=\"#b32\">[33]</ref>, propensity computation <ref type=\"bibr\" target=\"#b23\">[24 methods, such as the doubly robust method <ref type=\"bibr\" target=\"#b6\">[7]</ref> and its variants <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Moreover, they are also related to the types of knowledge st set (\ud835\udc46 \ud835\udc61\ud835\udc52 ). Following the settings of the previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we employ two evaluation metrics that are widely used in i"
        },
        {
            "pid": "59a02623b161e8ad1a7b64f0",
            "content": "mmender Systems as a feedback loop system may suffer from the bias problems such as popularity bias <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, previous model bias <"
        },
        {
            "pid": "5550404045ce0a409eb3326d",
            "content": "ntroduce the samples from \ud835\udc46 \ud835\udc61 to help \ud835\udc40 \ud835\udc50 ? Inspired by modeling of heterogeneous implicit feedback <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we add a confidence parameter to each sample of \ud835\udc46 \ud835\udc50 and \ud835\udc46"
        },
        {
            "pid": null,
            "content": "of the objective function, the training of student networks is guided to achieve knowledge transfer <ref type=\"bibr\" target=\"#b17\">[18]</ref>. A series of followup works develop different distillation ance, feature and model) and strategies (adaptive, collective and integrative) in transfer learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>In addition"
        },
        {
            "pid": null,
            "content": "et=\"#b23\">[24]</ref> and modeling with uniform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5b67b46f17c44aac1c8632a4",
            "content": "em may suffer from the bias problems such as popularity bias <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, previous model bias <ref type=\"bibr\" target=\"#b8\">[9,</ref><"
        },
        {
            "pid": null,
            "content": "rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. In this paper, we would like to study methods for better u"
        }
    ],
    "5fef22c691e0113b265a0289": [
        {
            "pid": "5e5644103a55ac122e36c54b",
            "content": "div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We generalize deep self-attention distillation in MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> by only using self-attention relation disti den size the same, layer-wisely transferring hidden states and self-attention distributions. MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> proposes deep self-attention distillation, teacher.</p><p>In this work, we generalize and simplify deep self-attention distillation of MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> by using self-attention relation distillati the student has the same number of layers as its teacher to perform layer-wise distillation. MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> transfers selfattention knowledge of teache \"bibr\" target=\"#b14\">Jiao et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Sun et al., 2019b;</ref><ref type=\"bibr\" target=\"#b40\">Wang et al., 2020)</ref>. The student models are distilled from large \"bibr\" target=\"#b14\">Jiao et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Sun et al., 2019a;</ref><ref type=\"bibr\" target=\"#b40\">Wang et al., 2020)</ref>  MobileBERT compresses a specially designed esults of Distil-BERT, TinyBERT 2 , BERT SMALL , Truncated BERT BASE and 6\u00d7768 MINILM are taken from<ref type=\"bibr\" target=\"#b40\">Wang et al. (2020)</ref>. BERT SMALL</figDesc><table><row><cell>Model"
        },
        {
            "pid": null,
            "content": "rget=\"#b10\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\">Bentivogli et al., 2009)</ref> and WNLI <ref type=\"bibr\" target=\"#b18\">(Levesque et al., 2012)</ref>). Following BERT <ref type=\"bibr\" targe"
        },
        {
            "pid": null,
            "content": "lliams et al., 2018)</ref>, QNLI <ref type=\"bibr\" target=\"#b27\">(Rajpurkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b6\">(Dagan et al., 2006;</ref><ref type=\"bibr\" target=\"#b2\">Bar-Haim et al"
        },
        {
            "pid": "56d81308dabfae2eee5eff4b",
            "content": "target=\"#b39\">(Wang et al., 2019)</ref> consists of two single-sentence classification tasks (SST-2 <ref type=\"bibr\" target=\"#b31\">(Socher et al., 2013)</ref> and CoLA <ref type=\"bibr\" target=\"#b41\">("
        },
        {
            "pid": "56d81308dabfae2eee5eff4b",
            "content": "target=\"#b39\">(Wang et al., 2019)</ref> consists of two single-sentence classification tasks (SST-2 <ref type=\"bibr\" target=\"#b31\">(Socher et al., 2013)</ref> and CoLA <ref type=\"bibr\" target=\"#b41\">("
        },
        {
            "pid": null,
            "content": "for 400, 000 steps. We use linear warmup over the first 4, 000 steps and linear decay. We use Adam <ref type=\"bibr\" target=\"#b16\">(Kingma and Ba, 2015)</ref> with \u03b2 1 = 0.9, \u03b2 2 = 0.999.  <ref type=\""
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "/www.tei-c.org/ns/1.0\"><head n=\"2.1\">Backbone Network: Transformer</head><p>Multi-layer Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017)</ref> has been widely adopted in pretrained mo"
        },
        {
            "pid": null,
            "content": "\" target=\"#b23\">(Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b7\">Devlin et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Dong et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Yang et al., 201 \"bibr\" target=\"#b7\">Devlin et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Dong et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Yang et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Joshi et al., 2 to pretrain a deep bidirectional Transformer using masked language modeling (MLM) objective. UNILM <ref type=\"bibr\" target=\"#b9\">(Dong et al., 2019)</ref> is jointly pretrained on three types languag"
        },
        {
            "pid": "5b1643998fbcbf6e5a9bc3b6",
            "content": "xtractive question answering.</p><p>GLUE General Language Understanding Evaluation (GLUE) benchmark <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> consists of two single-sentence classificat p>The summary of datasets used for the General Language Understanding Evaluation (GLUE) benchmark 4 <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> is presented in Table <ref type=\"table\" tar"
        },
        {
            "pid": "53e9a6dfb7602d97030135a5",
            "content": "=\"#b27\">(Rajpurkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b6\">(Dagan et al., 2006;</ref><ref type=\"bibr\" target=\"#b2\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b10\">Giampiccolo"
        },
        {
            "pid": "56d81308dabfae2eee5eff4b",
            "content": "target=\"#b39\">(Wang et al., 2019)</ref> consists of two single-sentence classification tasks (SST-2 <ref type=\"bibr\" target=\"#b31\">(Socher et al., 2013)</ref> and CoLA <ref type=\"bibr\" target=\"#b41\">("
        }
    ],
    "5f3f917891e011d38f9242d9": [
        {
            "pid": "53e9b0e6b7602d9703b60e78",
            "content": "imilarity measure. A is the action space, f is the reward function, and T is the terminal condition <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Given an initial p (l ) r , the neighbor selector choose"
        },
        {
            "pid": "5db9292e47c8f766461eff27",
            "content": "GNN sampling methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> also filter the neighbors. While these works only consider"
        },
        {
            "pid": "53e9b08ab7602d9703af72fc",
            "content": "model designing (Section 4.5).  <ref type=\"bibr\" target=\"#b28\">[29]</ref> and Amazon review dataset <ref type=\"bibr\" target=\"#b25\">[26]</ref> to study the fraudster camouflage and GNNbased fraud detec"
        },
        {
            "pid": "5a9cb66717c44a376ffb8667",
            "content": "mized during training GNN which retains the end-to-end learning fashion.</p><p>GNN sampling methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "5a9cb66717c44a376ffb8a99",
            "content": "f><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, financial fraud <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" ta et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> or a transaction in the trading system <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. The node has a la egate the neighbor information from different relations. Previous methods adopt attention mechanism <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta AGE <ref type=\"bibr\" target=\"#b11\">[12]</ref> to represent general GNN models. We choose Ge-niePath <ref type=\"bibr\" target=\"#b22\">[23]</ref>, Player2Vec <ref type=\"bibr\" target=\"#b47\">[48]</ref>, Sem -graph based on multiple relations and employ GNNs to aggregate neighborhood information. GeniePath <ref type=\"bibr\" target=\"#b22\">[23]</ref> learns convolutional layers and neighbor weights using LST"
        },
        {
            "pid": null,
            "content": "oblems and the relation camouflage of fraudsters. According to the cost-sensitive learning research <ref type=\"bibr\" target=\"#b29\">[30]</ref>, misclassifying a fraudster has a much higher cost to defe"
        },
        {
            "pid": "5e9ef9b69fced0a24b1b64cb",
            "content": "dustrial communities <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. Graphbased methods connect entities with different relatio"
        },
        {
            "pid": null,
            "content": ">, financial fraud <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, mobile fraud <ref type=\"bibr\" target=\"#b40\">[41]</ref>, an et=\"#b28\">29]</ref> or a transaction in the trading system <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. The node has a label y v \u2208 {0, 1} \u2208 Y where 0 represents b rent relations. Previous methods adopt attention mechanism <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> or devise weighting type=\"bibr\" target=\"#b22\">[23]</ref>, Player2Vec <ref type=\"bibr\" target=\"#b47\">[48]</ref>, SemiGNN <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and GraphConsis <ref type=\"bibr\" target=\"#b24\">[25]</ref> m <ref type=\"bibr\" target=\"#b33\">[34]</ref>. GEM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SemiGNN <ref type=\"bibr\" target=\"#b36\">[37]</ref>, ASA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and Player"
        },
        {
            "pid": "5a9cb66717c44a376ffb8a99",
            "content": "f><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, financial fraud <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" ta et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> or a transaction in the trading system <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. The node has a la egate the neighbor information from different relations. Previous methods adopt attention mechanism <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta AGE <ref type=\"bibr\" target=\"#b11\">[12]</ref> to represent general GNN models. We choose Ge-niePath <ref type=\"bibr\" target=\"#b22\">[23]</ref>, Player2Vec <ref type=\"bibr\" target=\"#b47\">[48]</ref>, Sem -graph based on multiple relations and employ GNNs to aggregate neighborhood information. GeniePath <ref type=\"bibr\" target=\"#b22\">[23]</ref> learns convolutional layers and neighbor weights using LST"
        },
        {
            "pid": "599c797d601a182cd2643e8a",
            "content": "GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GAT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, RGCN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and GraphSAGE <ref type=\"bibr\" target=\"#b11\">[12]</ref> t"
        },
        {
            "pid": "5ef96b048806af6ef2772095",
            "content": ">. Fraudsters disguise as regular users to bypass the anti-fraud system and disperse disinformation <ref type=\"bibr\" target=\"#b43\">[44]</ref> or reap end-users' privacy <ref type=\"bibr\" target=\"#b31\"> <ref type=\"bibr\" target=\"#b42\">[43]</ref> and adjust their behavior to alleviate the suspiciousness <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Specifically, these crafty fraudsters camouflage themselv amouflage, fraudsters may connect to different amounts of benign entities under different relations <ref type=\"bibr\" target=\"#b43\">[44]</ref>. However, since data annotation is costly for real-world f"
        }
    ],
    "5f3e44b791e011c0de1c29bc": [
        {
            "pid": "5e2d653a3a55acc837436820",
            "content": "rmance, most of them ignore the interactions' timestamp values. While recent works such as TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> successfully incorporated time information, their usage of ransformer <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cloze-task based training method. TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> enhanced SASRec by merging timestamp information into self st, they don't utilize timestamp values which hold important contextual information. While TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> successfully addressed this issue, they also used a simple pe=\"bibr\" target=\"#b31\">[32]</ref>,</p><p>SASRec <ref type=\"bibr\" target=\"#b9\">[10]</ref>, TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and BERT4Rec <ref type=\"bibr\" target=\"#b18\">[19]</ref>. T essing procedure from <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. We convert each da g the custom practice <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, we discard users a We use the rest for training. We follow the common practice <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> of letting the mode"
        },
        {
            "pid": "5d1eb9d4da562961f0b0e960",
            "content": "target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. BERT4Rec <ref type=\"bibr\" target=\"#b18\">[19]</ref> improve"
        },
        {
            "pid": null,
            "content": "e strong order constraint of RNN models, CNN-based methods <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> were proposed. Some"
        },
        {
            "pid": "573696c56e3b12023e5c58be",
            "content": "\"bibr\" target=\"#b2\">[3]</ref>, MovieLens 20M <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Amazon Beauty <ref type=\"bibr\" target=\"#b15\">[16]</ref> and Amazon Game <ref type=\"bibr\" target=\"#b15\">[16]</ref>. br\" target=\"#b2\">[3]</ref>, Amazon Beauty <ref type=\"bibr\" target=\"#b15\">[16]</ref> and Amazon Game <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We follow the common data preprocessing procedure from <r"
        },
        {
            "pid": "599c794a601a182cd262c93d",
            "content": "roposed to better understand the sequential history of users <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target nce the first suggestion by GRU4Rec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, many RNN-based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "599c7958601a182cd26331f6",
            "content": "=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Despite their excellent performance, most of them ignore t"
        },
        {
            "pid": "57d063b4ac4436735428dc1c",
            "content": "get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5c04967517c44a2c74708f06",
            "content": "ere proposed. Some works adopted graph neural network (GNN) to understand user's session as a graph <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>As for attention mechanisms, NARM <ref type=\"bibr\""
        },
        {
            "pid": null,
            "content": "orms of time gates to better model the time intervals in user's interaction sequence. Recently, CTA <ref type=\"bibr\" target=\"#b24\">[25]</ref> used multiple parametrized kernel functions on temporal in"
        },
        {
            "pid": "5c5c55bfe1cd8e03e71689a9",
            "content": "CNN-based methods <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> were proposed. Some works adopted graph neural network (GNN"
        },
        {
            "pid": "599c7987601a182cd2648373",
            "content": "e success in NLP areas <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. BERT4Rec <ref type ]</ref>. BERT4Rec <ref type=\"bibr\" target=\"#b18\">[19]</ref> improved SASRec by adopting Transformer <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cloze-task based training method. TiSASRec <ref type=\" ><head n=\"3.3.1\">Attention</head><p>Architecture. Multi-head self-attention proposed in Transformer <ref type=\"bibr\" target=\"#b21\">[22]</ref> shows excellent performance in recommendation tasks <ref t s in previous works <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, we apply a residual connection for each sublayer to facili )<label>(4)</label></formula><p>Note that while some works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> apply LayerNorm at the very end, we empirically chose to ap"
        }
    ],
    "5f03f3b611dc830562231f99": [
        {
            "pid": "5dcbd5da3a55ac789b0dbdc8",
            "content": "tworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap type=\"bibr\" target=\"#b21\">[23]</ref> uses a non-parametric model for the graph generative model and <ref type=\"bibr\" target=\"#b22\">[24]</ref> proposes a node copying model to achieve flexibility in th rnative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type=\"bibr\" target=\"#b22\">[24]</ref>. We demonstrate in the following sections that this model setting.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Node Copying</head><p>In <ref type=\"bibr\" target=\"#b22\">[24]</ref>, Pal et al. introduce the node copying model for \ud835\udc5d (G). Sa etworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap ode classification when there are very few training labels <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar node classification when there are very few training labels<ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar s. These limitations were addressed in the follow-up works <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24]</ref>, where <ref type=\"bibr\" target=\"#b21\">[23]</ref> uses a non"
        },
        {
            "pid": "53e9bc74b7602d97048f4169",
            "content": "S <ref type=\"bibr\" target=\"#b13\">[15]</ref>, MF <ref type=\"bibr\" target=\"#b16\">[18]</ref> and SVD++ <ref type=\"bibr\" target=\"#b15\">[17]</ref>) learn user and item embeddings by reconstructing the hist t=\"#b13\">[15]</ref>), Matrix Factorization (MF <ref type=\"bibr\" target=\"#b16\">[18]</ref>) and SVD++ <ref type=\"bibr\" target=\"#b15\">[17]</ref>. They learn user and item embeddings by reconstructing the"
        },
        {
            "pid": "5b67b45517c44aac1c860876",
            "content": "endation relevance <ref type=\"bibr\" target=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b36\">38]</ref>. The proposed systems exploit user-item interaction graphs interaction graphs <ref type=\"bibr\" target=\"#b29\">[31,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b36\">38]</ref>, user-user and (or) item-item co-occurrence graphs <ref typ mmendation systems <ref type=\"bibr\" target=\"#b29\">[31,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b36\">38]</ref>. Graph Convolutional Matrix Completion (GCMC) <ref type=\"bi a graph convolution autoencoder to learn user and item embeddings. Pinterest have proposed Pin-Sage <ref type=\"bibr\" target=\"#b36\">[38]</ref>, a large-scale GNN-based recommendation model to learn the 2) Graph neural network-based CF methods: GC-MC <ref type=\"bibr\" target=\"#b29\">[31]</ref>, PinSAGE <ref type=\"bibr\" target=\"#b36\">[38]</ref>, PinSAGE-LSTM, and NGCF <ref type=\"bibr\" target=\"#b32\">[34 p://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_4\"><head></head><label></label><figDesc>\u2022 PinSAGE<ref type=\"bibr\" target=\"#b36\">[38]</ref>: PinSAGE is a recent industry application of graph represe the input node features with mean aggregator. A hit rate improvement of more than 20% is reported in<ref type=\"bibr\" target=\"#b36\">[38]</ref>.\u2022 PinSAGE-LSTM: Same overall architecture as PinSAGE but w"
        },
        {
            "pid": "5d3ed25a275ded87f97deaab",
            "content": "ype=\"bibr\" target=\"#b17\">[19]</ref> and heterogeneous graphs <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> coming from heterogeneous interaction types (search, guide, c s between users and items in large-scale e-commerce networks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. This problem setting is not in the scope of this paper since"
        },
        {
            "pid": null,
            "content": "the generative model and improve computational efficiency.</p><p>In the context of recommendation, <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">14,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "53e99be3b7602d970248aebe",
            "content": "=\"#b16\">[18]</ref> models achieved the best performance in Netflix contest. MF models (such as pLAS <ref type=\"bibr\" target=\"#b13\">[15]</ref>, MF <ref type=\"bibr\" target=\"#b16\">[18]</ref> and SVD++ <r raction data. Latent factor models are common, such as probabilistic Latent Semantic Analysis (pLAS <ref type=\"bibr\" target=\"#b13\">[15]</ref>), Matrix Factorization (MF <ref type=\"bibr\" target=\"#b16\">"
        },
        {
            "pid": null,
            "content": "n the entities. Graph Convolutional (Neural) Networks (GCNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b14\">16]</ref> have proven to be a s. With the success of graph (convolutional) neural networks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b14\">16]</ref> on a wide range of la_15\">BPR\u2212OPT G \ud835\udc5c\ud835\udc4f\ud835\udc60 := (\ud835\udc62,\ud835\udc56,\ud835\udc57) \u2208\ud835\udc37 \ud835\udc46 ln \ud835\udf0e x\ud835\udc62\ud835\udc56 \ud835\udc57 (\u0398, G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \u2212 \ud835\udf06 \u0398 ||\u0398|| 2 (13)</formula><p>Equation <ref type=\"bibr\" target=\"#b10\">(11)</ref>, in conjunction with <ref type=\"bibr\" target=\"#b6\">(7)</re cent industry application of graph representation learning for recommendation. It deploys Graph-Sage<ref type=\"bibr\" target=\"#b10\">[11]</ref> on an item-item graph with both image and text information verall architecture as PinSAGE but we replaced the mean aggregator with LSTM aggregation proposed in<ref type=\"bibr\" target=\"#b10\">[11]</ref>. Although LSTM is an undesirable aggregator because it is"
        },
        {
            "pid": null,
            "content": "rificing the other. This trade-off has been studied in both the natural language generation setting <ref type=\"bibr\" target=\"#b1\">[2]</ref>, where Caccia et al. evaluate diversity vs. quality and show"
        },
        {
            "pid": "5b67b45517c44aac1c860876",
            "content": "endation relevance <ref type=\"bibr\" target=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b36\">38]</ref>. The proposed systems exploit user-item interaction graphs interaction graphs <ref type=\"bibr\" target=\"#b29\">[31,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b36\">38]</ref>, user-user and (or) item-item co-occurrence graphs <ref typ mmendation systems <ref type=\"bibr\" target=\"#b29\">[31,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b36\">38]</ref>. Graph Convolutional Matrix Completion (GCMC) <ref type=\"bi a graph convolution autoencoder to learn user and item embeddings. Pinterest have proposed Pin-Sage <ref type=\"bibr\" target=\"#b36\">[38]</ref>, a large-scale GNN-based recommendation model to learn the 2) Graph neural network-based CF methods: GC-MC <ref type=\"bibr\" target=\"#b29\">[31]</ref>, PinSAGE <ref type=\"bibr\" target=\"#b36\">[38]</ref>, PinSAGE-LSTM, and NGCF <ref type=\"bibr\" target=\"#b32\">[34 p://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_4\"><head></head><label></label><figDesc>\u2022 PinSAGE<ref type=\"bibr\" target=\"#b36\">[38]</ref>: PinSAGE is a recent industry application of graph represe the input node features with mean aggregator. A hit rate improvement of more than 20% is reported in<ref type=\"bibr\" target=\"#b36\">[38]</ref>.\u2022 PinSAGE-LSTM: Same overall architecture as PinSAGE but w"
        },
        {
            "pid": "5a260c8117c44a4ba8a30f54",
            "content": "uce some spurious edges while attempting to find potential links. The graph attention network (GAT) <ref type=\"bibr\" target=\"#b31\">[33]</ref> applies a self attention strategy operating on groups of s"
        },
        {
            "pid": "53e9aed1b7602d97038f7d8e",
            "content": "datasets, comparing the effect on our model and on NGCF. The novelty of recommendations is computed <ref type=\"bibr\" target=\"#b30\">[32]</ref> as Nov@k = 1  From Figure <ref type=\"figure\">3</ref> we ca"
        }
    ],
    "5f0d85c69fced0a24be4f028": [
        {
            "pid": "556fb0872401b4b38c23789b",
            "content": "\"bibr\" target=\"#b55\">[56]</ref>, CoLT <ref type=\"bibr\" target=\"#b45\">[46]</ref>, and Clustered TLBs <ref type=\"bibr\" target=\"#b44\">[45]</ref> combine near virtual-to-physical page translations into si"
        },
        {
            "pid": null,
            "content": "ts in similar processors. Alternative skewed-associative <ref type=\"bibr\" target=\"#b43\">[44]</ref>, <ref type=\"bibr\" target=\"#b52\">[53]</ref> TLB designs are possible.</p><p>In the newly added any-pag"
        },
        {
            "pid": "53e9ab2bb7602d97034b3a5d",
            "content": ".</p><p>Address translation overhead can be lowered by reducing the TLB miss rate. Synergistic TLBs <ref type=\"bibr\" target=\"#b53\">[54]</ref> and shared last-level TLBs <ref type=\"bibr\" target=\"#b9\">["
        },
        {
            "pid": "53e9a758b7602d9703094a46",
            "content": "application page tables.</p><p>To demonstrate the importance of hitting in the L1 TLB, we use ZSim <ref type=\"bibr\" target=\"#b50\">[51]</ref>, a cycle-based simulator of an x86 superscalar out-of-orde"
        },
        {
            "pid": "53e9b9cdb7602d97045c695c",
            "content": "us financial support of the HPS Research Group. Prior work <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib rformance in applications suffering from limited TLB reach <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib ce the number of page walks and improve TLB reach. Prior work has proposed hardware PTE prefetchers <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bib"
        },
        {
            "pid": "53e9bb01b7602d9704736de6",
            "content": "e size. This approach limits benefit despite the many page sizes offered.</p><p>Romer et al.'s work <ref type=\"bibr\" target=\"#b48\">[49]</ref> in superpages considered adding more variability to availa"
        },
        {
            "pid": "5d04eeba8607575390f83f61",
            "content": "Existing OS proposals like Ingens and Translation Ranger <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref> already address the issues of maximizing memory contiguity"
        },
        {
            "pid": null,
            "content": "to keep entire working sets in memory to minimize latency <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Apple iOS also does not swap to secondary storage <ref ty"
        },
        {
            "pid": "53e9a034b7602d97029191a7",
            "content": "<p>Prior work in virtual caches reduces translation overhead by translating only after a cache miss <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref>. However, for poo"
        },
        {
            "pid": null,
            "content": "ibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref> and previously proposed in <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>. Reserved frame or superpages <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref> are already utilized in current processors. The prevalent"
        },
        {
            "pid": "5c20b1fcda5629702063afe8",
            "content": "LB miss rate. Synergistic TLBs <ref type=\"bibr\" target=\"#b53\">[54]</ref> and shared last-level TLBs <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr"
        }
    ],
    "5f0d85c69fced0a24be4f052": [
        {
            "pid": "573695f96e3b12023e50d00a",
            "content": "he past, IBM z13 and many other systems have used FPGA based PCIe attached compression accelerators <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>. However, the FPGA >. However, the FPGA cost and limited number of PCIe slots restrict their usage to high-end servers <ref type=\"bibr\" target=\"#b1\">[2]</ref> and specialized applications such as storage controllers.</p hput <ref type=\"bibr\" target=\"#b5\">[6]</ref>. We would need 68 PCIe based compression cards such as <ref type=\"bibr\" target=\"#b1\">[2]</ref>, with 4GB/s peak throughput to match the NXU performance. Be ficient LZ77 encoding hardware that improves state of the art in Section IV, where the related work <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t nd z15 to achieve highest possible compression ratio, as described in Section V, where related work <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t oftware extensively in its products. IBM z13 and POWER systems used PCIe based Deflate accelerators <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t jectives which is not suitable for an on-chip design. In <ref type=\"bibr\" target=\"#b3\">[4]</ref> In <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, the 32KB sliding nt a static table with an assumed LZ symbol distribution which typically degrades compression ratio <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t here one of 15 different static Huffman tables yielding the smallest output is selected at run time <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We implemented in contrast, a true \"Dynamic Huffman\" mode f"
        },
        {
            "pid": "599c7c82601a182cd27b6ffb",
            "content": "e page fault handler to provide low latency access, described in Section VII. Related works include <ref type=\"bibr\" target=\"#b28\">[29]</ref>- <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>IBM has w latency access.</p><p>The value of user-mode access and page fault handling is also recognized in <ref type=\"bibr\" target=\"#b28\">[29]</ref> for network adapters. User-mode page fault handlers have b"
        },
        {
            "pid": null,
            "content": "3\">[44]</ref> and for applications to optimize access (e.g. caching, prefetching) to backing stores <ref type=\"bibr\" target=\"#b44\">[45]</ref>. Our method is for managing page faults caused by a physic"
        },
        {
            "pid": null,
            "content": "th prefix-free codes produced by the Huffman algorithm to compress an LZ77 stream by another 10-20% <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Frequent symbols are encoded with fewer bits. For example #b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bib er of symbols in the table; n = 286 for the literal and length symbols and n = 30 for the distances <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The algorithm is sequential and a basic hardware implemen orithm builds a binary tree with LZ symbols at the leaves arranged according to their probabilities <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The path from the tree root is the binary code of the LZ"
        },
        {
            "pid": null,
            "content": "TCC was implemented in low level firmware called millicode running on the CISC type processor cores <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Millicode interacts with the NXU on the same chip and per"
        },
        {
            "pid": "5550488345ce0a409eb6f03a",
            "content": "arge scale big data applications for improving storage space efficiency and application performance <ref type=\"bibr\" target=\"#b51\">[52]</ref>.</p><p>In this section, we first describe the integration ad>A. TPC-DS Benchmark on Apache Spark</head><p>TPC-DS is a de-facto standard benchmark in academia <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>- <ref type=\"bib"
        },
        {
            "pid": "53e9b5dab7602d9704128476",
            "content": "access, described in Section VII. Related works include <ref type=\"bibr\" target=\"#b28\">[29]</ref>- <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>IBM has been employing compression hardware and sof"
        },
        {
            "pid": "599c7c82601a182cd27b6ffb",
            "content": "e page fault handler to provide low latency access, described in Section VII. Related works include <ref type=\"bibr\" target=\"#b28\">[29]</ref>- <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>IBM has w latency access.</p><p>The value of user-mode access and page fault handling is also recognized in <ref type=\"bibr\" target=\"#b28\">[29]</ref> for network adapters. User-mode page fault handlers have b"
        },
        {
            "pid": null,
            "content": "loud environments.</p><p>Deflate uses the LZ77 variant of the Lempel-Ziv (LZ) compression algorithm <ref type=\"bibr\" target=\"#b8\">[9]</ref>, followed by an entropy coding algorithm <ref type=\"bibr\" ta es duplicate strings with references to earlier copies in the most recent 32KB of the source stream <ref type=\"bibr\" target=\"#b8\">[9]</ref>, called the sliding window or dynamic dictionary (called \"Hi"
        },
        {
            "pid": "599c7efe601a182cd28e059e",
            "content": "contexts. The Userfaultfd and CRIU mechanisms were proposed for live migration of virtual machines <ref type=\"bibr\" target=\"#b43\">[44]</ref> and for applications to optimize access (e.g. caching, pre"
        },
        {
            "pid": null,
            "content": "in O(n) time when symbols are pre-sorted by their count <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>. We implemented the 2dimensional parallel Shear-Sort algor e were found to be complex for a hardware implementation <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>. Our algorithm boosts small counter values to the effect o"
        }
    ],
    "5f8ebbb99fced0a24b4e1966": [
        {
            "pid": "599c7987601a182cd2648373",
            "content": "sequence. Recently, the models fully based on self-attention mechanism (denoted as self-attention  <ref type=\"bibr\" target=\"#b15\">[16]</ref> in the Neural Machine Translation (NMT) task, have achieve"
        },
        {
            "pid": "59a02ff0b161e8ad1a7b6eb9",
            "content": "=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. To generate diversified results, these methods either exp ef type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>Those existing approaches used greedy document sequ strategy may not lead to global optimal rankings. Based on the reinforced learning approach MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Feng <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed t rmula><p>Here \ud835\udc98 \ud835\udc5f is a learnable parameter. We use the same relevance features as the previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref> for \ud835\udc99 \ud835\udc5e and \ud835\udc99 \ud835\udc5e \ud835\udc56 , including BM25, TF-IDF, language model bers of incoming links and outgoing links, et al. More details about these features can be found in <ref type=\"bibr\" target=\"#b13\">[14]</ref> and we omit the details due to space limitation. In the fu used are actually the document embeddings. We use the subtopic embeddings released by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> based on doc2vec. The subtopic embeddings is produced from t diversification task is limited, we inherit the list-pairwise sampling approach from Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> in order to get enough training samples. We are using pair ance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> in the repository on GitHub<ref type=\"foot\" target=\"#foot_ =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, all those metrics are computed on top 20 results of a doc \"#b11\">[12]</ref> and PAMM-NTN <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Inspired by previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we use the metric of \ud835\udefc \u2212 \ud835\udc5bDCG@20 to tune the parameters. 100-dimensional vectors generated by the LDA <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>. We train the DSSA model with the code and data released b lt is denoted as DSSA (doc2vec).</p><p>Since the deep reinforced learning based models e.g. MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref> and M2DIV <ref type=\"bibr\" target=\"#b14\">[15]</ref> are ta target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> (i.e., explicit approaches), or directly reduce result redu"
        },
        {
            "pid": "5550441845ce0a409eb4b330",
            "content": "order to learn an optimized ranking function automatically <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" t the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" t e also proposed supervised methods, such as SVM-DIV <ref type=\"bibr\" target=\"#b9\">[10]</ref>, R-LTR <ref type=\"bibr\" target=\"#b10\">[11]</ref>), PAMM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and PAMM ype=\"bibr\" target=\"#b30\">[31]</ref> (denoted as S-rec). Inheriting the spirit of the previous works <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" t t=\"#b9\">[10]</ref> is used to learn a prior relevance function with no diversification.</p><p>R-LTR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, PAMM <ref type=\"bibr\" target=\"#b11\">[12]</ref> and PAMM-N"
        },
        {
            "pid": "53e99e38b7602d97026fa228",
            "content": "target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. While in recent year verage of the results <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe supervised explicit approaches are proposed e.g. xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSS lts of Lemur as initial ranking sequences.</p><p>xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Th"
        },
        {
            "pid": "5bdc31b417c44a1f58a0b8c2",
            "content": "archers have used self-attention networks, e.g. GPT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ERNIE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, as al extraction and document representation e.g. K-NRM <ref type=\"bibr\" target=\"#b26\">[27]</ref> or BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>"
        },
        {
            "pid": "573698486e3b12023e7110e7",
            "content": "target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. While in recent years, more and more researchers tried to u target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> (i.e., explicit appr D <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>Th ber of the queries is 10, and the average subtopic number is about 9.48. As those previous works do <ref type=\"bibr\" target=\"#b8\">[9]</ref> we treat all those subtopics with uniform weights.</p><p>For \" target=\"#foot_2\">3</ref> as the non-diversified baseline. These results are released by Hu et at. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and can be found on the website <ref type=\"foot\" target=\"#fo ef type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>. These are the unsupervised explicit baseline approaches for"
        },
        {
            "pid": null,
            "content": "eatures and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe ods either explicitly model subtopic coverage of the results <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe as possible. Nowadays both unsupervised and supervised explicit approaches are proposed e.g. xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <r in our experiments are using the search results of Lemur as initial ranking sequences.</p><p>xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM"
        },
        {
            "pid": "53e99ebdb7602d9702786088",
            "content": "ad><p>The evaluation metrics. The official diversity evaluation metrics of Web Track include ERR-IA <ref type=\"bibr\" target=\"#b27\">[28]</ref>, \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b28\">[29]</ref> and NRBP"
        },
        {
            "pid": "57d063b4ac4436735428dbc4",
            "content": "t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. To generate dive t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref> (i.e., implicit approaches).</p><p>To simplify the problem type=\"bibr\" target=\"#b10\">[11]</ref>), PAMM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and PAMM-NTN <ref type=\"bibr\" target=\"#b12\">[13]</ref>), for learning a better document similarity function autom he previous works <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, all those metric f type=\"bibr\" target=\"#b10\">[11]</ref>, PAMM <ref type=\"bibr\" target=\"#b11\">[12]</ref> and PAMM-NTN <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Inspired by previous work <ref type=\"bibr\" target=\"#b13\">"
        },
        {
            "pid": null,
            "content": "hods in search result diversification in order to learn an optimized ranking function automatically <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta e. Inheriting the spirit of MMR, researchers have also proposed supervised methods, such as SVM-DIV <ref type=\"bibr\" target=\"#b9\">[10]</ref>, R-LTR <ref type=\"bibr\" target=\"#b10\">[11]</ref>), PAMM <re hts of the hierarchical subtopic layers. The parameters are tuned with cross validation and ListMLE <ref type=\"bibr\" target=\"#b9\">[10]</ref> is used to learn a prior relevance function with no diversi"
        },
        {
            "pid": "5e8d8e6d9fced0a24b5d65ba",
            "content": "<ref type=\"bibr\" target=\"#b16\">[17]</ref>, BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ERNIE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. Howev"
        },
        {
            "pid": "53e99e38b7602d97026fa228",
            "content": "target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. While in recent year verage of the results <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe supervised explicit approaches are proposed e.g. xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSS lts of Lemur as initial ranking sequences.</p><p>xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Th"
        },
        {
            "pid": null,
            "content": "ty evaluation metrics of Web Track include ERR-IA <ref type=\"bibr\" target=\"#b27\">[28]</ref>, \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b28\">[29]</ref> and NRBP <ref type=\"bibr\" target=\"#b29\">[30]</ref>, which"
        },
        {
            "pid": null,
            "content": "ch result diversification are unsupervised and they are based on handcrafted features and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type=\"bibr\" target=\"#b4\">[5]</ref> model:</p><formula xml:id=\"formula_0\">Score MMR = \ud835\udf06score(\ud835\udc51 \ud835\udc56 reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": "53e99ebdb7602d9702786088",
            "content": "ad><p>The evaluation metrics. The official diversity evaluation metrics of Web Track include ERR-IA <ref type=\"bibr\" target=\"#b27\">[28]</ref>, \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b28\">[29]</ref> and NRBP"
        },
        {
            "pid": null,
            "content": "eatures and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe ods either explicitly model subtopic coverage of the results <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe as possible. Nowadays both unsupervised and supervised explicit approaches are proposed e.g. xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <r in our experiments are using the search results of Lemur as initial ranking sequences.</p><p>xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM"
        },
        {
            "pid": "573698486e3b12023e7110e7",
            "content": "target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. While in recent years, more and more researchers tried to u target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> (i.e., explicit appr D <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>Th ber of the queries is 10, and the average subtopic number is about 9.48. As those previous works do <ref type=\"bibr\" target=\"#b8\">[9]</ref> we treat all those subtopics with uniform weights.</p><p>For \" target=\"#foot_2\">3</ref> as the non-diversified baseline. These results are released by Hu et at. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and can be found on the website <ref type=\"foot\" target=\"#fo ef type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>. These are the unsupervised explicit baseline approaches for"
        },
        {
            "pid": "599c7968601a182cd263a485",
            "content": "veral deeplearning based technologies for feature extraction and document representation e.g. K-NRM <ref type=\"bibr\" target=\"#b26\">[27]</ref> or BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p></di"
        },
        {
            "pid": null,
            "content": "ics of Precision-IA <ref type=\"bibr\" target=\"#b5\">[6]</ref> (denoted as Pre-IA) and Subtopic Recall <ref type=\"bibr\" target=\"#b30\">[31]</ref> (denoted as S-rec). Inheriting the spirit of the previous"
        },
        {
            "pid": "53e99ebdb7602d9702786088",
            "content": "ad><p>The evaluation metrics. The official diversity evaluation metrics of Web Track include ERR-IA <ref type=\"bibr\" target=\"#b27\">[28]</ref>, \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b28\">[29]</ref> and NRBP"
        },
        {
            "pid": null,
            "content": "ref type=\"bibr\" target=\"#b27\">[28]</ref>, \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b28\">[29]</ref> and NRBP <ref type=\"bibr\" target=\"#b29\">[30]</ref>, which are used in our experiments. Besides the metrics ab"
        },
        {
            "pid": "53e9b7e0b7602d970438e88c",
            "content": "s/1.0\"><head n=\"1\">INTRODUCTION</head><p>Research shows that most queries issued by users are short <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe"
        }
    ],
    "5f69d1e09fced0a24bc32bb1": [
        {
            "pid": "599c7cda601a182cd27e0d74",
            "content": "unctions. To exploit the sparsity of DNNs to improve energy efficiency, many architectures, such as <ref type=\"bibr\" target=\"#b24\">[25]</ref>, are proposed to detect and skip the multiplications assoc"
        },
        {
            "pid": null,
            "content": "on information: <ref type=\"bibr\">DOI</ref>   TNAM <ref type=\"bibr\" target=\"#b30\">[31]</ref> Twin-8T <ref type=\"bibr\" target=\"#b31\">[32]</ref> Sandwich-RAM <ref type=\"bibr\" target=\"#b7\">[8]</ref> Wang"
        },
        {
            "pid": null,
            "content": "systemlevel in-memory computing prototype, capable of speech recognition, was recently reported in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. It utilized multiple PIM macros. The precision issue was flow optimization to reduce unnecessary data movement. Only output-stationary designs were reported <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Because analog signals' replication, storage, and accumul"
        },
        {
            "pid": null,
            "content": "method solving such an issue is known as tensorization or dataflows, whose details are discussed in <ref type=\"bibr\" target=\"#b27\">[28]</ref>. It is discovered that local storage and movement of parti . <ref type=\"bibr\" target=\"#b32\">[33]</ref> Thinker <ref type=\"bibr\" target=\"#b13\">[14]</ref> iFPNA <ref type=\"bibr\" target=\"#b27\">[28]</ref> This Work  power consuming part is the clock tree. The clo"
        },
        {
            "pid": "5e3166bfdf1a9c0c41e96630",
            "content": "memory access latency. Reconfigurable <ref type=\"bibr\" target=\"#b13\">[14]</ref> and sparsity-aware <ref type=\"bibr\" target=\"#b14\">[15]</ref> architectures were addressed in digital DNN accelerators. ces, an additional set of vectors are needed to store the indices or guard bits of nonzero elements <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, for the ultra-low-bitwidth PIM architectures wit"
        },
        {
            "pid": "573696026e3b12023e516748",
            "content": "lowbitwidth quantization. Binary/ternary quantization and binary weight quantization appeared first <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. However, the accu"
        },
        {
            "pid": null,
            "content": "on information: <ref type=\"bibr\">DOI</ref>   TNAM <ref type=\"bibr\" target=\"#b30\">[31]</ref> Twin-8T <ref type=\"bibr\" target=\"#b31\">[32]</ref> Sandwich-RAM <ref type=\"bibr\" target=\"#b7\">[8]</ref> Wang"
        },
        {
            "pid": null,
            "content": "method solving such an issue is known as tensorization or dataflows, whose details are discussed in <ref type=\"bibr\" target=\"#b27\">[28]</ref>. It is discovered that local storage and movement of parti . <ref type=\"bibr\" target=\"#b32\">[33]</ref> Thinker <ref type=\"bibr\" target=\"#b13\">[14]</ref> iFPNA <ref type=\"bibr\" target=\"#b27\">[28]</ref> This Work  power consuming part is the clock tree. The clo"
        },
        {
            "pid": "57d063e0ac443673542947b2",
            "content": "</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. Furthermore, processing-in-memory (PIM) architectures <ref type=\"bibr\" target=\"#b3\">[4]</ref> and silicon prototypes <ref type=\"bibr\" target=\"#b4\">[5]</re"
        },
        {
            "pid": null,
            "content": "awned processing-near-memory systems that fabricated processors in DRAM chips, dating back to 1990s <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. Furthermore, processi"
        },
        {
            "pid": null,
            "content": "awned processing-near-memory systems that fabricated processors in DRAM chips, dating back to 1990s <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. Furthermore, processi"
        }
    ],
    "5f0277e911dc830562231dea": [
        {
            "pid": "5b67b46f17c44aac1c86329e",
            "content": "rations are used to model the sequential document selection process in search result diversi cation <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and multi-page sea"
        },
        {
            "pid": "5550441845ce0a409eb4b2f5",
            "content": "of partially observed Markov decision process (POMDP) <ref type=\"bibr\" target=\"#b21\">[22]</ref>. In <ref type=\"bibr\" target=\"#b41\">[42]</ref>, a log-based document re-ranking algorithm is proposed, al"
        },
        {
            "pid": "5aed14d617c44a44381595ee",
            "content": ""
        },
        {
            "pid": null,
            "content": "versi cation, including the heuristic methods of MMR <ref type=\"bibr\" target=\"#b4\">[5]</ref>, xQuAD <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and PM-2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>; and the"
        },
        {
            "pid": null,
            "content": "ayed a vital role in the eld of information retrieval (IR) <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. In recent years, reinforcement learning (RL) <ref type=\"bi"
        },
        {
            "pid": null,
            "content": "ELATED WORK</head><p>Existing learning to rank studies can be categorized into pointwise approaches <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, pairwise approaches"
        },
        {
            "pid": null,
            "content": "<ref type=\"bibr\" target=\"#b4\">[5]</ref>, xQuAD <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and PM-2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>; and the learning methods of SVM-DIV <ref type=\"bibr\" target"
        },
        {
            "pid": "5aed148b17c44a4438155007",
            "content": ""
        },
        {
            "pid": null,
            "content": "versi cation, including the heuristic methods of MMR <ref type=\"bibr\" target=\"#b4\">[5]</ref>, xQuAD <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and PM-2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>; and the"
        },
        {
            "pid": null,
            "content": ".</p><p>In recent years, reinforcement learning has been applied in the IR ranking.Radlinski et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> proposed two online learning bandit algorithms to learn a"
        },
        {
            "pid": null,
            "content": "on retrieval systems as a dueling bandit problem, called Dueling Bandit Gradient Descent (DBGD). In <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DBGD was further improved so that the click data can be u"
        }
    ],
    "5f00587b9fced0a24b1fbbf1": [
        {
            "pid": "5e16fa4bdf1a9c0c41713de9",
            "content": "quence Models for Peptide Binding Prediction</head><p>The approach builds on the UDSMProt-framework <ref type=\"bibr\" target=\"#b11\">[12]</ref> and related work in natural language processing <ref type= with a concat pooling layer and two fully connected layers. The setup closely follows that used in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, where protein properties were predicted. The smaller data of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the training procedure included 1-cycle learning rate sch e potential of unlabeled peptide data in order to observe similar improvements as seen for proteins <ref type=\"bibr\" target=\"#b11\">[12]</ref> in particular for small datasets.  Turning to MHC Class II uarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This effect is a direct consequence of the considerably s"
        },
        {
            "pid": "53e9be0fb7602d9704ac683e",
            "content": "ffinity dataset from the IEDB site<ref type=\"foot\" target=\"#foot_1\">2</ref> based on the dataset by <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We used it to train our prediction tools.</p><p>e) IEDB16"
        },
        {
            "pid": null,
            "content": "alleles, which is the default evaluation metric for related tasks such as remote homology detection <ref type=\"bibr\" target=\"#b19\">[20]</ref> or transcription factor binding site prediction <ref type="
        },
        {
            "pid": "5cefb2913a55ac87f7e19785",
            "content": "ch makes it difficult to realistically assess its prediction performance. The very recent MHCSeqNet <ref type=\"bibr\" target=\"#b10\">[11]</ref> also uses a recurrent architecture, again with pretrained"
        },
        {
            "pid": null,
            "content": "10; 11]</ref> and we discuss in more detail how USMPep stands out from these approaches. MHCnuggets <ref type=\"bibr\" target=\"#b9\">[10]</ref> is rather similar to the proposed approach (apart from the"
        },
        {
            "pid": null,
            "content": "10; 11]</ref> and we discuss in more detail how USMPep stands out from these approaches. MHCnuggets <ref type=\"bibr\" target=\"#b9\">[10]</ref> is rather similar to the proposed approach (apart from the"
        },
        {
            "pid": "55503fe545ce0a409eb30ddc",
            "content": "embedding dimensions, were set based on selected alleles of a particular MHC class I dataset (Kim14 <ref type=\"bibr\" target=\"#b15\">[16]</ref>, see the detailed description below) by using the score on finities to specific MHC alleles. a) Kim14: is a commonly used binding affinity dataset compiled by <ref type=\"bibr\" target=\"#b15\">[16]</ref>, available on the Immune Epitope Database (IEDB)<ref type= ich is peculiar in the sense that 172 of the 176 test set samples fall into a single Hobohl cluster <ref type=\"bibr\" target=\"#b15\">[16]</ref> of sequences with more than 80% sequence similarity, i.e."
        },
        {
            "pid": null,
            "content": "discussion about micro vs. macro averages for the evaluation of multi-class classification problems <ref type=\"bibr\" target=\"#b21\">[22]</ref>. In particular, there are two fundamental differences betw"
        },
        {
            "pid": null,
            "content": "framework <ref type=\"bibr\" target=\"#b11\">[12]</ref> and related work in natural language processing <ref type=\"bibr\" target=\"#b12\">[13]</ref>. We distinguish two variants of our approach, either train earning rate scheduling <ref type=\"bibr\" target=\"#b14\">[15]</ref> and discriminative learning rates <ref type=\"bibr\" target=\"#b12\">[13]</ref> during finetuning. Target variables for the regression mod"
        },
        {
            "pid": null,
            "content": "10; 11]</ref> and we discuss in more detail how USMPep stands out from these approaches. MHCnuggets <ref type=\"bibr\" target=\"#b9\">[10]</ref> is rather similar to the proposed approach (apart from the"
        },
        {
            "pid": null,
            "content": "-transformed half-maximal inhibitory concentration (IC 50 )-values and a modified MSE loss function <ref type=\"bibr\" target=\"#b7\">[8]</ref> that allows to incorporate qualitative data.</p><p>Dropout r reported in the literature vary in size and compilation. We trained our models on data provided by <ref type=\"bibr\" target=\"#b7\">[8]</ref> and refer to this dataset as MHCFlurry18. It is assembled fr aining data set, we trained and tested our model on the Kim14 BD2009 and Blind data. The authors of <ref type=\"bibr\" target=\"#b7\">[8]</ref> kindly provided us with the Blind predictions of their tool tool. Corresponding training routines are by now also available in the code repository accompanying <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>First, we compare the prediction success measured by"
        }
    ],
    "5f58a1b491e011e46ee73247": [
        {
            "pid": "5b67b47917c44aac1c863824",
            "content": "al networks has concentrated on node classification task <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib ake a specific node (e.g., a person in social networks) misclassified. In this scenario, Dai et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> study the adversarial attack on graph structure data and p olutional Network (SGC) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and gradient-based attack methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we propose a n cient way to generate destructive adversarial examples. Focusing on the targeted attack, Dai et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose GradArgmax, which extracts gradients of the surrog Network (GCN)</head><p>Since a number of existing works <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bib f-the-art targeted attack methods: Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head ly nodes belonging to the same class/different classes will be disconnected/connected. \u2022 GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Since the attack budget \u2206 is defined as the degrees of ta"
        },
        {
            "pid": "599c798a601a182cd2649842",
            "content": "aw distribution <ref type=\"bibr\" target=\"#b36\">[37]</ref>, i.e., <ref type=\"bibr\">3.</ref> Refer to <ref type=\"bibr\" target=\"#b35\">[36]</ref>, the time complexity of GCN is O(|E|d 2 ), and the computa"
        },
        {
            "pid": "5cf48a3eda56291d582a1174",
            "content": "pe=\"bibr\" target=\"#b40\">[41]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b39\">[40]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b41\">[42]</ref>. For each target model, our method is compared with other ors with a fixed size, instead of using a full-neighborhood set during training.</p><p>\u2022 ClusterGCN <ref type=\"bibr\" target=\"#b41\">[42]</ref>. This is the state-of-the-art mini-batch GCN framework. It"
        },
        {
            "pid": "5bdc316717c44a1f58a06f5b",
            "content": "=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Undoubtedly, graph plays a crucial role in many high impact"
        },
        {
            "pid": "599c797a601a182cd2641eda",
            "content": "matrix A (sub) is computed for each edge e = (u, v) \u2208 E s .</p><p>However, as stated by Guo et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>, most of the modern neural networks are poorly calibrated,"
        },
        {
            "pid": "5a260c8117c44a4ba8a30f54",
            "content": "ks: GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, SGC <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b39\">[40]</ref>, Clus h achieves competitive results and even significantly improves the training efficiency.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>. GAT enhances GCN by leveraging a masked self-attention me"
        },
        {
            "pid": null,
            "content": "ies focus primarily on the intrinsic properties of graph <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, but measuring the impact of graph adversarial attacks is >. Unlike previous works, Newman et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Foster et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> focus on another important network feature, i.e., assortat ure the attack impacts, degree assortativity coefficient <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, it measures the tendency of nodes connected to each other"
        },
        {
            "pid": null,
            "content": "ies focus primarily on the intrinsic properties of graph <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, but measuring the impact of graph adversarial attacks is >. Unlike previous works, Newman et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Foster et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> focus on another important network feature, i.e., assortat ure the attack impacts, degree assortativity coefficient <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, it measures the tendency of nodes connected to each other"
        },
        {
            "pid": "5d06e488da562926acc4bd09",
            "content": "underlying the challenge of poisoning attacks (a.k.a, training-time attacks). Similarly, Xu et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> propose PGD structure attack that conducts gradient attack \">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> use vanilla GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> as"
        },
        {
            "pid": null,
            "content": "\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. In order to fool a classifier and misclassify specific no rget=\"#b16\">[17]</ref> and gradient-based attack methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we propose a novel Simplified Gradient-based Attack (SGA) attack that is undifferentiated and global <ref type=\"bibr\" target=\"#b22\">[23]</ref>, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> utilize the metagradients to solve the bi-level problem un xisting works <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> use vanilla GCN of the surrogate loss, we will sequentially compute the gradients after adding or removing an edge <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Let Z (i) t and Z (subi) t denote the prediction of targe"
        },
        {
            "pid": "5cf48a3eda56291d582a1174",
            "content": "pe=\"bibr\" target=\"#b40\">[41]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b39\">[40]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b41\">[42]</ref>. For each target model, our method is compared with other ors with a fixed size, instead of using a full-neighborhood set during training.</p><p>\u2022 ClusterGCN <ref type=\"bibr\" target=\"#b41\">[42]</ref>. This is the state-of-the-art mini-batch GCN framework. It"
        }
    ],
    "5f7fdd328de39f0828397e7f": [
        {
            "pid": "5550412e45ce0a409eb39182",
            "content": "high memory footprint, both of which grow quadratically with respect to the image height (or width) <ref type=\"bibr\" target=\"#b37\">[38]</ref>. In real-world applications like content-based image searc ployed for higher efficiency. This differentiates our method from early recurrent attention methods <ref type=\"bibr\" target=\"#b37\">[38]</ref> which adopt pure recurrent models. In addition, we focus o \">8]</ref>.</p><p>One similar work to our GFNet is the recurrent visual attention model proposed in <ref type=\"bibr\" target=\"#b37\">[38]</ref>. However, our method differs from it in two important aspe ults with only a few class-discriminative patches, such as the head of a dog or the wings of a bird <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "5a260c0217c44a4ba8a1c9b6",
            "content": "patches, such as the head of a dog or the wings of a bird <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. These regions are ty rget=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>One similar wo"
        },
        {
            "pid": "5a260c8617c44a4ba8a32308",
            "content": "s focus on pruning <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> or quantizing the w"
        },
        {
            "pid": "5736960a6e3b12023e51d64d",
            "content": "denotes the entropy bonus to ensure sufficient exploration <ref type=\"bibr\" target=\"#b57\">[58,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, and L VF t is a sq"
        },
        {
            "pid": null,
            "content": "ascading <ref type=\"bibr\" target=\"#b3\">[4]</ref> or mixing <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> paradigm. Some other works propose to dynamically skip unne"
        },
        {
            "pid": "53e9a0d2b7602d97029bca07",
            "content": "e attention mechanism is typically exploited to extract information from some task-relevant regions <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "unnecessary layers <ref type=\"bibr\" target=\"#b51\">[52,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> or channels <ref type=\"bibr\" target=\"#b32\">[33]</ref>.</p><"
        },
        {
            "pid": "573695fe6e3b12023e511ba3",
            "content": "get=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b64\">65]</ref>. In the context of image recognition, the attention mechani"
        },
        {
            "pid": "5736986c6e3b12023e73037b",
            "content": "ask-relevant regions <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "555045d945ce0a409eb5a0f5",
            "content": "ef type=\"bibr\" target=\"#b37\">[38]</ref>. In real-world applications like content-based image search <ref type=\"bibr\" target=\"#b53\">[54]</ref> or autonomous vehicles <ref type=\"bibr\" target=\"#b2\">[3]</"
        },
        {
            "pid": null,
            "content": "dancy. Recent research has revealed that considerable spatial redundancy occurs when inferring CNNs <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref>. Several approaches spatial resolution by using low-frequency features. The Spatially Adaptive Computation Time (SACT) <ref type=\"bibr\" target=\"#b9\">[10]</ref> dynamically adjusts the number of executed layers for diffe AS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SkipNet <ref type=\"bibr\" target=\"#b54\">[55]</ref>, SACT <ref type=\"bibr\" target=\"#b9\">[10]</ref>, GoogLeNet <ref type=\"bibr\" target=\"#b47\">[48]</ref> and MS"
        }
    ],
    "5fa909a591e011e83f7406b0": [
        {
            "pid": "5dd3bf443a55ac1bdd46d73e",
            "content": "e challenging. A number of GPM frameworks have been proposed to reduce the burden on the programmer <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta lgorithms. Low-level systems such as RStream <ref type=\"bibr\" target=\"#b55\">[56]</ref> and Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref> provide low-level API functions for the user to control th form the state-of-the-art GPM systems, AutoMine <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Peregrine <ref type=\"bibr\" target=\"#b28\">[29]</ref> b e an example in Appendix B.6). This technique is called Memoization of Embedding Connectivity (MEC) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. \u2022 For edge-induced extension, a set of edges instead of v tern Classification (CP): FP and CP are low-level optimizations enabled in a prior system, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, so we describe them in Appendices B.4 and B.5. To support type=\"foot\" target=\"#foot_0\">3</ref> : AutoMine <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Peregrine <ref type=\"bibr\" target=\"#b28\">[29]</ref>. \"#b10\">[11]</ref> is a distributed GPM system which incorporates task-parallel processing. Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a shared-memory GPM system targeting both CPU and GPU. ferent patterns. For small implicit patterns, Sandslash uses customized pattern classification (CP) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For example, in FSM, the labeled wedge patterns can be di"
        },
        {
            "pid": "5def6ca63a55ac6095fe0607",
            "content": "22,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, and GPUs <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": "58437722ac44360f1082f1db",
            "content": "rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> can use algorithmic insight to aggressively prune the searc \">[3]</ref> counts 3 and 4-motifs by leveraging proven formulas to reduce enumeration space. Escape <ref type=\"bibr\" target=\"#b44\">[45]</ref> extends this approach to 5-motifs. Subgraph listing <ref t"
        },
        {
            "pid": "58d82fd2d649053542fd7486",
            "content": "ibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. One example is motif counting <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\">23,</ref><ref type=\"bibr\" target=\"#b40\">41]<"
        },
        {
            "pid": "56d84e5fdabfae2eeeefa996",
            "content": "tended in Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> because it is automorphic to the subgraph <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b1\">2)</ref>. High-level Sandslash"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "57d06412ac4436735429bb02",
            "content": "t high-level GPM system. It includes efficient matching strategies from well-established techniques <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]<"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "ef>. One example is motif counting <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\">23,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which counts the number of occurrences of certain structur"
        },
        {
            "pid": null,
            "content": "ef>. One example is motif counting <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\">23,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which counts the number of occurrences of certain structur"
        },
        {
            "pid": "56d84e5fdabfae2eeeefa996",
            "content": "tended in Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> because it is automorphic to the subgraph <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b1\">2)</ref>. High-level Sandslash"
        }
    ],
    "5f7fdd328de39f08283980ba": [
        {
            "pid": "5736971f6e3b12023e612bee",
            "content": "tured data. In particular, the Information Bottleneck (IB) <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> provides a critical principle for representation learning:"
        },
        {
            "pid": "5bdc31b417c44a1f58a0ba6c",
            "content": "ve performance, by learning to fuse information from both the node features and the graph structure <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Recently, many works have been focusing on developing get=\"#b7\">[8]</ref>.</p><p>Recently, many works have been focusing on developing more powerful GNNs <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe s an equivalent form: max P(Z|D):I(D;Z)\u2264Ic</p><p>Intuitively, Eq. ( <ref type=\"formula\">7</ref>) or <ref type=\"bibr\" target=\"#b7\">(8)</ref> encourages the representation Z to maximally capture the inf straight line with slope \u03b2 to sweep out the Pareto frontier of I(Y ; Z) vs. I(X; Z) as given by Eq. <ref type=\"bibr\" target=\"#b7\">(8)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>\ud835\udc9f"
        },
        {
            "pid": "5a260c8117c44a4ba8a30f54",
            "content": "Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe target.</p><p>We demonstrate the GIB principle by applying it to the Graph Attention Networks (GAT) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, where we leverage the attention weights of GAT to sample th ning. In the next subsection, we will introduce two instantiations of GIB, which is inspired by GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= can be applied to many GNN models. As an example, we apply it to the Graph Attention Network model <ref type=\"bibr\" target=\"#b4\">[5]</ref> and present GIB-Cat and GIB-Bern. Algorithm 1 illustrates th resentations will be sampled. Note that we may also use a mechanism similar to multi-head attention <ref type=\"bibr\" target=\"#b4\">[5]</ref>: We split Z(l\u22121)</p><p>X into different channels w.r.t. its e GIB-Cat and GIB-Bern with baselines including GCN <ref type=\"bibr\" target=\"#b2\">[3]</ref> and GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the most relevant baseline as GIB-Cat and GIB-Bern are to i he standard transductive node classification setting and standard trainvalidation-test split as GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The summary statistics of the datasets and their splitting and GIB-Bern follows Alg. 1 (and Alg. 2 and 3 for the respective neighbor-sampling). We follow GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>'s default architecture, in which we use 8 attention heads, n rporate the attention mechanism to adaptively learn the correlation between a node and its neighbor <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Recent literature s"
        },
        {
            "pid": null,
            "content": "8 CPU @ 2.40GH CPUs. We use PyTorch <ref type=\"bibr\" target=\"#b49\">[50]</ref> and PyTorch Geometric <ref type=\"bibr\" target=\"#b50\">[51]</ref> for constructing the GNNs and evaluation. Project website"
        },
        {
            "pid": "5c8a11324895d9cbc6121c34",
            "content": "l\u22121) X , A). Generally speaking, any node-pair representations, such as messages over edges in MPNN <ref type=\"bibr\" target=\"#b28\">[29]</ref>, can be leveraged to sample structures. Applying the GIB p egation from neighbors <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" t"
        },
        {
            "pid": null,
            "content": "a lower bound of I(Y ; Z (L) X ), which is reproduced from <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, and an upper bound of I(D; Z </p><formula xml:id=\"formula_ f. We use the Nguyen, Wainright &amp; Jordan's bound I NWJ <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>: Lemma B.1. <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref t </ref><ref type=\"bibr\" target=\"#b22\">23]</ref>: Lemma B.1. <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> For any two random variables X 1 , X 2 and any function g :"
        },
        {
            "pid": "5d0b006f8607575390fc5a3c",
            "content": "resentations <ref type=\"bibr\" target=\"#b32\">[33]</ref>, removing suspicious and uninformative edges <ref type=\"bibr\" target=\"#b33\">[34]</ref>, low-rank approximation of the adjacency matrix <ref type= state-of-the-art graph defense models specifically designed against adversarial attacks: GCNJaccard <ref type=\"bibr\" target=\"#b33\">[34]</ref> that pre-processes the graph by deleting the edges between fferent class with very different features, which exactly matches the assumption used by GCNJaccard <ref type=\"bibr\" target=\"#b33\">[34]</ref>. GCNJaccard proceeds to delete edges with dissimilar node cks by connecting nodes with different classes. This exactly satisfies the assumption of GCNJaccard <ref type=\"bibr\" target=\"#b33\">[34]</ref>. GCNJaccard proceeds by deleting edges with low feature si"
        },
        {
            "pid": "5c8a11324895d9cbc6121c34",
            "content": "l\u22121) X , A). Generally speaking, any node-pair representations, such as messages over edges in MPNN <ref type=\"bibr\" target=\"#b28\">[29]</ref>, can be leveraged to sample structures. Applying the GIB p egation from neighbors <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" t"
        },
        {
            "pid": "5c75711af56def9798751adb",
            "content": "representations. Other methods apply IB to various domains <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. The difference is that we develop information-theoretic mo and <ref type=\"bibr\" target=\"#b5\">(6)</ref>. To allow more flexibility (in similar spirit as \u03b2-VAE <ref type=\"bibr\" target=\"#b40\">[41]</ref>), we allow the coefficient before AIB and XIB to be differ"
        },
        {
            "pid": null,
            "content": "target=\"#b36\">[37]</ref>, between representations of sub-structures and the hidden feature vectors <ref type=\"bibr\" target=\"#b37\">[38]</ref>, between representations of graphs and their sub-structure graph-structured data. Furthermore, several works on GNNs <ref type=\"bibr\" target=\"#b36\">[37]</ref><ref type=\"bibr\" target=\"#b37\">[38]</ref><ref type=\"bibr\" target=\"#b38\">[39]</ref> leverage informat"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "tions of graph-structured data for downstream tasks such as node classification and link prediction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. Graph representation e=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ead><p>GNNs learn node-level representations through message passing and aggregation from neighbors <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target"
        }
    ],
    "5f8cf5159e795ea21aee7f07": [
        {
            "pid": "5b8c9f4a17c44af36f8b71a9",
            "content": "ature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controll rning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consi d have similar performances for \u03c6 and \u2212\u03c6. Due to space limitation we refer the interested reader to <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref> for a review of all formal theoretical philic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>. The results show that, asymptotically, ate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p de et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>). Assume that n, f \u2192 \u221e, n f \u2192 \u03be and d \u2192"
        },
        {
            "pid": "57a4e91aac44365e35c97c6e",
            "content": "puts of different GCN layers to arrive at the final output. On the other hand, the GCN-Cheby method <ref type=\"bibr\" target=\"#b9\">(Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Kipf &am kovic et al., 2018)</ref>, JK-Net <ref type=\"bibr\" target=\"#b37\">(Xu et al., 2018)</ref>, GCN-Cheby <ref type=\"bibr\" target=\"#b9\">(Defferrard et al., 2016)</ref>, APPNP <ref type=\"bibr\" target=\"#b19\">"
        },
        {
            "pid": null,
            "content": "t performance improvements over <ref type=\"bibr\">Personalized PageRank (Kloumann et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019)</ref>. The operational principles of GPRs can be suc f type=\"bibr\" target=\"#b7\">(Chung, 2007)</ref>, are associated with specific choices of GPR weights <ref type=\"bibr\" target=\"#b22\">(Li et al., 2019)</ref>. For an excellent in-depth discussion of Page (Cora) and heterophilic (Texas) dataset. The learnt GPR weights from Cora match the behavior of IPR <ref type=\"bibr\" target=\"#b22\">(Li et al., 2019)</ref>, which verifies that large-step propagation i known to be suboptimal compared to the IPR framework when applied to homophilic node classification <ref type=\"bibr\" target=\"#b22\">(Li et al., 2019)</ref>.</p><p>Fixing the GPR weights makes the model e interested reader is referred to <ref type=\"bibr\" target=\"#b13\">(Gleich, 2015)</ref>. The work in <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> recently introduced and theoretically analyzed ng which is significantly different from the PPR weights. This result matches the recent finding in <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> and behave similar to IPR proposed by the auth"
        },
        {
            "pid": "573696c56e3b12023e5c58be",
            "content": "\"bibr\" target=\"#b38\">Yang et al., 2016)</ref> and the Amazon co-purchase graphs Computers and Photo <ref type=\"bibr\" target=\"#b25\">(McAuley et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Shchur et"
        },
        {
            "pid": "5d9edc8347c8f76646042a37",
            "content": "ignals. In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Klicpera et al. heart GNNs that is related to our GPR-GNN approach. It can be easily seen that APPNP as well as SGC <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019)</ref> are special cases of our model since APPNP fi errard et al., 2016)</ref>, APPNP <ref type=\"bibr\" target=\"#b19\">(Klicpera et al., 2018)</ref>, SGC <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019)</ref>, SAGE <ref type=\"bibr\" target=\"#b14\">(Hamilto"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "5d9edc8347c8f76646042a37",
            "content": "ignals. In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Klicpera et al. heart GNNs that is related to our GPR-GNN approach. It can be easily seen that APPNP as well as SGC <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019)</ref> are special cases of our model since APPNP fi errard et al., 2016)</ref>, APPNP <ref type=\"bibr\" target=\"#b19\">(Klicpera et al., 2018)</ref>, SGC <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019)</ref>, SAGE <ref type=\"bibr\" target=\"#b14\">(Hamilto"
        },
        {
            "pid": "5d9edc8347c8f76646042a37",
            "content": "ignals. In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Klicpera et al. heart GNNs that is related to our GPR-GNN approach. It can be easily seen that APPNP as well as SGC <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019)</ref> are special cases of our model since APPNP fi errard et al., 2016)</ref>, APPNP <ref type=\"bibr\" target=\"#b19\">(Klicpera et al., 2018)</ref>, SGC <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019)</ref>, SAGE <ref type=\"bibr\" target=\"#b14\">(Hamilto"
        },
        {
            "pid": null,
            "content": "of them seem to be tailor-made to work on homophilic (associative) graphs. The homophily principle <ref type=\"bibr\" target=\"#b26\">(McPherson et al., 2001)</ref> in the context of node classification"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "tention layers (GAT) <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref> and many others <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\">Wijesinghe &amp; Wang, 19\">(Klicpera et al., 2018)</ref>, SGC <ref type=\"bibr\" target=\"#b36\">(Wu et al., 2019)</ref>, SAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and Geom-GCN <ref type=\"bibr\" target=\"#"
        },
        {
            "pid": "55503f7f45ce0a409eb2e79c",
            "content": ""
        }
    ],
    "5fd8964891e0119b22c1f219": [
        {
            "pid": "57a4e91dac44365e35c9844d",
            "content": "r/item embeddings by mimicking the meta-learning setting via episode based training, as proposed in <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Specifically, we pick the users/items with sufficient int rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, which consists of metric-based recommendation <ref type=\"b"
        },
        {
            "pid": "5d3ed25a275ded87f97deb93",
            "content": "<ref type=\"bibr\" target=\"#b43\">44]</ref>, social trust path <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5cf48a47da56291d582ab19d",
            "content": "target=\"#b43\">44]</ref> or external knowledge graphs (KGs) <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to compensate the low-quality embeddings caused by sparse i f type=\"bibr\" target=\"#b41\">42]</ref> and knowledge graphs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to enhance the representations of the cold-start users/item"
        },
        {
            "pid": "5b1642388fbcbf6e5a9b54be",
            "content": "ta learner, the meta aggregator and the neighbor sampler together (Line 4). Same as the settings of <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, to have a stable up"
        },
        {
            "pid": "5ce2d0b2ced107d4c63a8bff",
            "content": "or sampler as a hierarchical Markov Decision Process (MDP) <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Specifically, we formulate the neighbor sampler as \ud835\udc3f \u2212 1 M neighbor sampler together (Line 4). Same as the settings of <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, to have a stable update during joint training, each parame ublic datasets including MovieLens-1M (Ml-1M) 3  <ref type=\"bibr\" target=\"#b11\">[12]</ref>, MOOCs 4 <ref type=\"bibr\" target=\"#b46\">[47]</ref> and Last.fm 5 . Table <ref type=\"table\">1</ref> illustrate"
        },
        {
            "pid": "57a4e91aac44365e35c97c6e",
            "content": "9 N x c 2 W x s o + P R l L R e X k o = \" &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type=\"bibr\" target=\"#b5\">6</ref>  </p><formula xml:id=\"formula_2\">O U B z E d K R E J R t F K f 9 N x c 2 W x s o + P R l L R e X k o = \" &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type=\"bibr\" target=\"#b5\">6</ref>  </p><formula xml:id=\"formula_3\">O U B z E d K R E J R t F K f 9 N x c 2 W x s o + P R l L R e X k o = \" &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h <ref type=\"bibr\" target=\"#b5\">6</ref>  </p><formula xml:id=\"formula_4\">O U B z E d K R E J R t F K f"
        },
        {
            "pid": "5d3ed25a275ded87f97deb93",
            "content": "<ref type=\"bibr\" target=\"#b43\">44]</ref>, social trust path <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5cfa5b985ced2477cb3c5175",
            "content": "entities.</p><p>On another line, inspired by the recent development of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ"
        },
        {
            "pid": null,
            "content": "ce between the predicted target embedding h \ud835\udc3f \ud835\udc62 and the ground-truth embedding h \ud835\udc62 , as proposed by <ref type=\"bibr\" target=\"#b15\">[16]</ref>, due to its popularity as an indicator for the semantic si models and the GNN models are initialized by the NCF embedding results. We use Spearman correlation <ref type=\"bibr\" target=\"#b15\">[16]</ref> to measure the agreement between the ground truth embeddin"
        },
        {
            "pid": "599c7988601a182cd2648a09",
            "content": "ed by the recent development of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, NGCF <ref type=\"bi stics of neighbors during the graph convolution process. Although some GNN models such as GrageSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> or FastGCN <ref type=\"bibr\" target=\"#b3\">[4]</ref> filter esults in at most \ud835\udc3e \ud835\udc59 (1 \u2264 \ud835\udc59 \u2264 \ud835\udc3f) \ud835\udc59-order neighbors for each target user/item. Similar to GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>, we sample high-order neighbors to improve the computation i-layer Perceptron and matrix factorization to learn the embeddings of users and items. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>: is a general GNN model which samples neighbors randomly a"
        },
        {
            "pid": "5cf48a48da56291d582ab75a",
            "content": "> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>Recommendation systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> have been extensiv s matrix factorization <ref type=\"bibr\" target=\"#b20\">[21]</ref> and neural collaborative filtering <ref type=\"bibr\" target=\"#b13\">[14]</ref>, is to learn embeddings, i.e. the preferences for users an is learned upon the observed abundant interactions by NCF<ref type=\"foot\" target=\"#foot_0\">1</ref>  <ref type=\"bibr\" target=\"#b13\">[14]</ref>. To mimic the cold-start users/items, in each training epi split \ud835\udc37 \ud835\udc47 into the training set \ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \ud835\udc47 and the test set \ud835\udc47 \ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc47 with a ratio of 7:3. We train NCF <ref type=\"bibr\" target=\"#b13\">[14]</ref> to get the ground-truth embeddings for the target users/it torization model, the general GNN models and the special GNN models for recommendation:</p><p>\u2022 NCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>: is a neural matrix factorization model which combines Mul"
        }
    ],
    "5f058d15dfae54570ec57ea1": [
        {
            "pid": "5cede0fcda562983788db9a8",
            "content": "ces a channel-attention mechanism by adaptively recalibrating the channel feature responses. SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref> brings the feature-map attention across two network branch /ref>: Comparing our ResNeSt block with SE-Net <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref>. A detailed view of Split-Attention unit is shown in Figur -Net operates on top of the entire block regardless of multiple groups. Previous models like SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref> introduced feature attention between two network branches, type=\"bibr\" target=\"#b28\">[29]</ref>, ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref> and SKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Remarkably, our ResNeSt-50 achieves 80.64 top-1 accuracy, ach group is Split Attention in Cardinal Groups. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, a combined representation for each cardinal group can be o obal average pooling across spatial dimensions s k \u2208 R C/K <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Here the c-th component is calculated as:</p><formula xml: Our method generalizes prior work on feature-map attention <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> within a cardinal group setting <ref type=\"bibr\" target=\"#b"
        },
        {
            "pid": "5a260c8117c44a4ba8a30771",
            "content": "the previous methods, our Fig. <ref type=\"figure\">1</ref>: Comparing our ResNeSt block with SE-Net <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref>. A de then the intermediate representation of each group is Split Attention in Cardinal Groups. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, a combined repres ead of the 1 \u00d7 1 layer to better preserve such information <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Convolutional layers require handling featuremap boundarie"
        },
        {
            "pid": "58d82fcbd649053542fd67d7",
            "content": "\"#b51\">[52]</ref>, in which each network block consists of different convolutional kernels. ResNeXt <ref type=\"bibr\" target=\"#b60\">[61]</ref> adopts group convolution <ref type=\"bibr\" target=\"#b33\">[3 ight) depicts an overview of a Split-Attention Block.</p><p>Feature-map Group. As in ResNeXt blocks <ref type=\"bibr\" target=\"#b60\">[61]</ref>, the feature can be divided into several groups, and the n"
        },
        {
            "pid": "5550401245ce0a409eb3205c",
            "content": "not during inference) to form an implicit network ensemble <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>. A dropout layer wi"
        },
        {
            "pid": "599c7949601a182cd262c13a",
            "content": ""
        },
        {
            "pid": "5736960e6e3b12023e520c34",
            "content": "/ref>.</p><p>Label Smoothing Label smoothing was first used to improve the training of Inception-V2 <ref type=\"bibr\" target=\"#b52\">[53]</ref>. Recall the cross entropy loss incurred by our network's p ception-Net family<ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref> uses a training crop size of 299. Recently, the EfficientN ed to its optimal value \u221e, and this can induce overfitting <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. Rather than assigning hard labels as targets, label smooth"
        },
        {
            "pid": "5736960e6e3b12023e520c34",
            "content": "/ref>.</p><p>Label Smoothing Label smoothing was first used to improve the training of Inception-V2 <ref type=\"bibr\" target=\"#b52\">[53]</ref>. Recall the cross entropy loss incurred by our network's p ception-Net family<ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref> uses a training crop size of 299. Recently, the EfficientN ed to its optimal value \u221e, and this can induce overfitting <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. Rather than assigning hard labels as targets, label smooth"
        },
        {
            "pid": "5c04967517c44a2c74708a53",
            "content": "CNN operators.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Method</head><p>Deformable <ref type=\"bibr\" target=\"#b71\">[72]</ref>   Beyond the paper contributions, we empirically find seve ade-Mask-RCNN on COCO val set. The ResNeSt-101 is applied with and without deformable convolution v2<ref type=\"bibr\" target=\"#b71\">[72]</ref>. It shows that our split-attention module is compatible wi nce segmentation, shown in  We also evaluate our ResNeSt with and without deformable convolution v2 <ref type=\"bibr\" target=\"#b71\">[72]</ref>. With its help, we are able to obtain a higher performance"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "bibr\" target=\"#b54\">[55]</ref> as shown in Table <ref type=\"table\">1</ref>. Our single Cascade-RCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref> model using a ResNeSt-101 backbone achieves 48.3% box mAP an"
        },
        {
            "pid": "5a260bfb17c44a4ba8a1c682",
            "content": "n mAP on MS-COCO <ref type=\"bibr\" target=\"#b41\">[42]</ref> and semantic segmentation mIoU on ADE20K <ref type=\"bibr\" target=\"#b70\">[71]</ref>.</p><p>not even trainable on a GPU with an appropriate per"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "h-wise convolution <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>. Despite their supe classification performance, such as: Amoe-baNet <ref type=\"bibr\" target=\"#b44\">[45]</ref>, MNASNet <ref type=\"bibr\" target=\"#b53\">[54]</ref>, and EfficientNet <ref type=\"bibr\" target=\"#b54\">[55]</ref"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": "p><p>Multi-path and Feature-map Attention. Multi-path representation has shown success in GoogleNet <ref type=\"bibr\" target=\"#b51\">[52]</ref>, in which each network block consists of different convolu d training crop size of 224, while the Inception-Net family<ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref> uses a training c"
        },
        {
            "pid": "5a4aef9e17c44a2190f7a8e2",
            "content": "target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref> or introduce long-range connections <ref type=\"bibr\" target=\"#b55\">[56]</ref> or use cross-channel feature-map attention <ref type=\"bibr sks at the same time? Cross-channel information has demonstrated success in downstream applications <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": "bibr\" target=\"#b54\">[55]</ref> as shown in Table <ref type=\"table\">1</ref>. Our single Cascade-RCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref> model using a ResNeSt-101 backbone achieves 48.3% box mAP an"
        },
        {
            "pid": "5c2c7a9217c44a4e7cf3136a",
            "content": "rided convolution at the 3 \u00d7 3 layer instead of the 1 \u00d7 1 layer to better preserve such information <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Convolutional lay vers (64 GPUs in total) in parallel. Our learning rates are adjusted according to a cosine schedule <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. We follow the com small for j = c, while z c is being pushed to its optimal value \u221e, and this can induce overfitting <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. Rather than assig ><p>Tweaks from ResNet-D. We also adopt two simple yet effective ResNet modifications introduced by <ref type=\"bibr\" target=\"#b25\">[26]</ref>: (1) The first 7 \u00d7 7 convolutional layer is replaced with including bias units, \u03b3 and \u03b2 in the batch normalization layers.</p><p>#P GFLOPs acc(%) ResNetD-50 <ref type=\"bibr\" target=\"#b25\">[26]</ref>  For example 2s2x40d denotes radix=2, cardinality=2 and wi ixup training, we simply mix each sample from the current mini-batch with its reversed order sample <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Batch Normalization <ref type=\"bibr\" target=\"#b31\">[32]</ /www.tei-c.org/ns/1.0\"><head n=\"5.2\">Ablation Study</head><p>ResNeSt is based on the ResNet-D model <ref type=\"bibr\" target=\"#b25\">[26]</ref>  ResNeSt-fast setting, the effective average downsampling inality, and d the network width (0s represents the use of a standard residual block as in ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref>). We empirically find that increasing the radix from 0 to ref type=\"bibr\" target=\"#b59\">[60]</ref>, SENet <ref type=\"bibr\" target=\"#b28\">[29]</ref>, ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref> and SKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Remar to the weights of convolutional and fully connected layers <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. We do not subject any of the other network parameters to w ng on images that share the same crop size. ResNet variants<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar"
        },
        {
            "pid": "5c04967517c44a2c74708a53",
            "content": "CNN operators.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Method</head><p>Deformable <ref type=\"bibr\" target=\"#b71\">[72]</ref>   Beyond the paper contributions, we empirically find seve ade-Mask-RCNN on COCO val set. The ResNeSt-101 is applied with and without deformable convolution v2<ref type=\"bibr\" target=\"#b71\">[72]</ref>. It shows that our split-attention module is compatible wi nce segmentation, shown in  We also evaluate our ResNeSt with and without deformable convolution v2 <ref type=\"bibr\" target=\"#b71\">[72]</ref>. With its help, we are able to obtain a higher performance"
        },
        {
            "pid": "5550401245ce0a409eb3205c",
            "content": "not during inference) to form an implicit network ensemble <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>. A dropout layer wi"
        },
        {
            "pid": "5da2f8aa3a55ac3402d8bfef",
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": "599c7949601a182cd262c13a",
            "content": ""
        },
        {
            "pid": "5a9cb65d17c44a376ffb820b",
            "content": "ficantly boosted image classification accuracy through large scale neural architecture search (NAS) <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>. Despite their sta ed CNN architectures that achieved state-of-the-art classification performance, such as: Amoe-baNet <ref type=\"bibr\" target=\"#b44\">[45]</ref>, MNASNet <ref type=\"bibr\" target=\"#b53\">[54]</ref>, and Ef"
        },
        {
            "pid": null,
            "content": "type=\"bibr\" target=\"#b43\">[44]</ref>. Network weights are initialized using Kaiming Initialization <ref type=\"bibr\" target=\"#b23\">[24]</ref>. A drop layer is inserted before the final classification"
        },
        {
            "pid": "58d82fced649053542fd71f3",
            "content": "ugmented example. Regularization. Very deep neural networks tend to overfit even for large datasets <ref type=\"bibr\" target=\"#b67\">[68]</ref>. To prevent this, dropout regularization randomly masks ou t network ensemble <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>. A dropout layer with the dropout probability of 0.2 is app"
        },
        {
            "pid": null,
            "content": "type=\"bibr\" target=\"#b43\">[44]</ref>. Network weights are initialized using Kaiming Initialization <ref type=\"bibr\" target=\"#b23\">[24]</ref>. A drop layer is inserted before the final classification"
        },
        {
            "pid": null,
            "content": "f type=\"bibr\" target=\"#b45\">46]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref> and pose estimation"
        },
        {
            "pid": "53e9a508b7602d9702e2bcf5",
            "content": "type=\"bibr\" target=\"#b31\">[32]</ref> is used after each convolutional layer before ReLU activation <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Network weights are initialized using Kaiming Initializat"
        },
        {
            "pid": "5a4aef9e17c44a2190f7a8e2",
            "content": "target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref> or introduce long-range connections <ref type=\"bibr\" target=\"#b55\">[56]</ref> or use cross-channel feature-map attention <ref type=\"bibr sks at the same time? Cross-channel information has demonstrated success in downstream applications <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" ta"
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        },
        {
            "pid": null,
            "content": ""
        }
    ]
}